{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "80bcf751",
      "metadata": {
        "id": "80bcf751"
      },
      "source": [
        "# The Deep Learning Homework 3 - Question No. 10 Part C\n",
        "Bahar Mahdavi - SN: 40152521337"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a4e790f",
      "metadata": {
        "id": "6a4e790f"
      },
      "source": [
        "## C. Train the network of part A with batchsize=32 and following loss function:\n",
        "\n",
        "1. nn.MultiLabelSoftMarginLoss()\n",
        "2. nn.MultiMarginLoss()\n",
        "2. nn.PoissonNLLLoss()\n",
        "4. nn.GaussianNLLLoss()\n",
        "5. nn.MultiLabelMarginLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up the Environment"
      ],
      "metadata": {
        "id": "65YXRYCJbd2w"
      },
      "id": "65YXRYCJbd2w"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "x=torch.tensor([1]).to('cuda')\n",
        "print(torch.cuda.is_available())\n",
        "# moves your model to train on your gpu if available else it uses your cpu\n",
        "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqfynTaPbpZb",
        "outputId": "1f7146c6-e84b-47cd-ec6f-c81ef45be573"
      },
      "id": "xqfynTaPbpZb",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load MNIST Dataset from TorchVision"
      ],
      "metadata": {
        "id": "XfwQRcG0VGvU"
      },
      "id": "XfwQRcG0VGvU"
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Define transform to normalize data\n",
        "transform = transforms.Compose([\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.1307,),(0.3081,))\n",
        "                                ])\n",
        "\n",
        "# Download and load the training data\n",
        "train_set = datasets.MNIST('DATA_MNIST/', download=True, train=True, transform=transforms.ToTensor())\n",
        "trainLoader = torch.utils.data.DataLoader(train_set, batch_size=32, shuffle=True)\n",
        "\n",
        "test_set = datasets.MNIST('DATA_MNIST/', download=True, train=False, transform=transforms.ToTensor())\n",
        "testLoader = torch.utils.data.DataLoader(test_set, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "o0zKwO-LVFXD"
      },
      "id": "o0zKwO-LVFXD",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data = enumerate(trainLoader)\n",
        "batch_idx, (images, labels) = next(training_data)\n",
        "print(type(images)) # Checking the datatype\n",
        "print(images.shape) # the size of the image\n",
        "print(labels.shape) # the size of the labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKlev2fcaL6I",
        "outputId": "581f62b5-6b16-4c57-943c-852ea8446ec6"
      },
      "id": "YKlev2fcaL6I",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'torch.Tensor'>\n",
            "torch.Size([32, 1, 28, 28])\n",
            "torch.Size([32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building the Network"
      ],
      "metadata": {
        "id": "H7CBGEkzbTR2"
      },
      "id": "H7CBGEkzbTR2"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MnistModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(MnistModel, self).__init__()\n",
        "        self.convolutaional_neural_network_layers = nn.Sequential(\n",
        "                nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1),\n",
        "                nn.ReLU(),\n",
        "                nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "                nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
        "                nn.ReLU(),\n",
        "                nn.MaxPool2d(kernel_size=2)\n",
        "        )\n",
        "        self.linear_layers = nn.Sequential(\n",
        "                nn.Linear(in_features=1568, out_features=1024),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(p=0.2),\n",
        "                nn.Linear(in_features=1024, out_features=10),\n",
        "                nn.Softmax()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.convolutaional_neural_network_layers(x)\n",
        "        x = x.view(-1, 1568)\n",
        "        x = self.linear_layers(x)\n",
        "        return"
      ],
      "metadata": {
        "id": "Kcm972HjPIMc"
      },
      "id": "Kcm972HjPIMc",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MnistModel()\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZD81A2L6Zab",
        "outputId": "c41390bc-5e69-4d9e-e398-b38d465af2df"
      },
      "id": "NZD81A2L6Zab",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MnistModel(\n",
              "  (convolutaional_neural_network_layers): Sequential(\n",
              "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (4): ReLU()\n",
              "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (linear_layers): Sequential(\n",
              "    (0): Linear(in_features=1568, out_features=1024, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Dropout(p=0.2, inplace=False)\n",
              "    (3): Linear(in_features=1024, out_features=10, bias=True)\n",
              "    (4): Softmax(dim=None)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. MultiLabelSoftMarginLoss"
      ],
      "metadata": {
        "id": "T1qMuqBhKoyC"
      },
      "id": "T1qMuqBhKoyC"
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import optim\n",
        "from torchsummary import summary\n",
        "\n",
        "criterion = nn.MultiLabelSoftMarginLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "summary(model, (1,28,28))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBjLoxSy6pu9",
        "outputId": "bc5cb845-f1b9-4384-98bb-8b498464c42f"
      },
      "id": "GBjLoxSy6pu9",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 16, 28, 28]             160\n",
            "              ReLU-2           [-1, 16, 28, 28]               0\n",
            "         MaxPool2d-3           [-1, 16, 14, 14]               0\n",
            "            Conv2d-4           [-1, 32, 14, 14]           4,640\n",
            "              ReLU-5           [-1, 32, 14, 14]               0\n",
            "         MaxPool2d-6             [-1, 32, 7, 7]               0\n",
            "            Linear-7                 [-1, 1024]       1,606,656\n",
            "              ReLU-8                 [-1, 1024]               0\n",
            "           Dropout-9                 [-1, 1024]               0\n",
            "           Linear-10                   [-1, 10]          10,250\n",
            "          Softmax-11                   [-1, 10]               0\n",
            "================================================================\n",
            "Total params: 1,621,706\n",
            "Trainable params: 1,621,706\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.35\n",
            "Params size (MB): 6.19\n",
            "Estimated Total Size (MB): 6.54\n",
            "----------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training and Testing the Model with nn.MultiLabelSoftMarginLoss()"
      ],
      "metadata": {
        "id": "iw8xKXQIdHlr"
      },
      "id": "iw8xKXQIdHlr"
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 15\n",
        "train_loss, val_loss = [], []\n",
        "accuracy_total_train, accuracy_total_val = [], []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    total_train_loss = 0\n",
        "    total_val_loss = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    total = 0\n",
        "    # training our model\n",
        "    for idx, (image, label) in enumerate(trainLoader):\n",
        "\n",
        "        image, label = image.to(device), label.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        pred = model(image)\n",
        "\n",
        "        loss = criterion(pred, label)\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        loss.backward()    # calc gradients\n",
        "        optimizer.step()   # update gradients\n",
        "\n",
        "        #pred = torch.nn.functional.softmax(pred, dim=1)\n",
        "        for i, p in enumerate(pred):\n",
        "            if label[i] == torch.max(p.data, 0)[1]:\n",
        "                total = total + 1\n",
        "\n",
        "    accuracy_train = total / len(train_set)\n",
        "    accuracy_total_train.append(accuracy_train)\n",
        "\n",
        "    total_train_loss = total_train_loss / (idx + 1)\n",
        "    train_loss.append(total_train_loss)\n",
        "\n",
        "    # validating our model\n",
        "    model.eval()\n",
        "    total = 0\n",
        "    for idx, (image, label) in enumerate(testLoader):\n",
        "        image, label = image.cuda(), label.cuda()\n",
        "        pred = model(image)\n",
        "        loss = criterion(pred, label)\n",
        "        total_val_loss += loss.item()\n",
        "\n",
        "        #pred = torch.nn.functional.softmax(pred, dim=1)\n",
        "        for i, p in enumerate(pred):\n",
        "            if label[i] == torch.max(p.data, 0)[1]:\n",
        "                total = total + 1\n",
        "\n",
        "    accuracy_val = total / len(test_set)\n",
        "    accuracy_total_val.append(accuracy_val)\n",
        "\n",
        "    total_val_loss = total_val_loss / (idx + 1)\n",
        "    val_loss.append(total_val_loss)\n",
        "\n",
        "    if epoch % 5 == 0:\n",
        "      print(\"Epoch: {}/{}  \".format(epoch, epochs),\n",
        "            \"Training loss: {:.4f}  \".format(total_train_loss),\n",
        "            \"Testing loss: {:.4f}  \".format(total_val_loss),\n",
        "            \"Train accuracy: {:.4f}  \".format(accuracy_train),\n",
        "            \"Test accuracy: {:.4f}  \".format(accuracy_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "GdaH9LTPBZWf",
        "outputId": "525b08ec-bd7a-4bf7-b701-8ef2047e1d69"
      },
      "id": "GdaH9LTPBZWf",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-fb097d4f9843>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mtotal_train_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1223\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultilabel_soft_margin_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmultilabel_soft_margin_loss\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3441\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3443\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlogsigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlogsigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3445\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: log_sigmoid(): argument 'input' (position 1) must be Tensor, not NoneType"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(train_loss, label='Training loss')\n",
        "plt.plot(val_loss, label='Test loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(accuracy_total_train, label='Training Accuracy')\n",
        "plt.plot(accuracy_total_val, label='Test Accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "\n",
        "plt.suptitle(f'Model with 32 batch_size and MultiLabelSoftMarginLoss Loss')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IeHRh_9pDxFf"
      },
      "id": "IeHRh_9pDxFf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluating the Network Model with nn.MultiLabelSoftMarginLoss()"
      ],
      "metadata": {
        "id": "ytNv2_rgEhpJ"
      },
      "id": "ytNv2_rgEhpJ"
    },
    {
      "cell_type": "code",
      "source": [
        "img = images[0]\n",
        "img = img.to(device)\n",
        "img = img.view(-1, 1, 28, 28)\n",
        "print(img.shape)\n",
        "\n",
        "# Since we want to use the already pretrained weights to make some prediction\n",
        "# we are turning off the gradients\n",
        "with torch.no_grad():\n",
        "    logits = model.forward(img)"
      ],
      "metadata": {
        "id": "magI_xLvEkGu"
      },
      "id": "magI_xLvEkGu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We take the softmax for probabilites since our outputs are logits\n",
        "probabilities = F.softmax(logits, dim=1).detach().cpu().numpy().squeeze()\n",
        "\n",
        "print(probabilities)\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(figsize=(6,8), ncols=2)\n",
        "ax1.imshow(img.view(1, 28, 28).detach().cpu().numpy().squeeze(), cmap='inferno')\n",
        "ax1.axis('off')\n",
        "ax2.barh(np.arange(10), probabilities, color='r' )\n",
        "ax2.set_aspect(0.1)\n",
        "ax2.set_yticks(np.arange(10))\n",
        "ax2.set_yticklabels(np.arange(10))\n",
        "ax2.set_title('Class Probability')\n",
        "ax2.set_xlim(0, 1.1)\n",
        "\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "CGhBboJyEkCT"
      },
      "id": "CGhBboJyEkCT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. MultiLabelSoftMarginLoss:"
      ],
      "metadata": {
        "id": "079QNSe-lIE5"
      },
      "id": "079QNSe-lIE5"
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import optim\n",
        "from torchsummary import summary\n",
        "\n",
        "criterion = nn.MultiMarginLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "summary(model, (1,28,28))"
      ],
      "metadata": {
        "id": "5p6ZQ6d_lpIV"
      },
      "id": "5p6ZQ6d_lpIV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training and Testing the Model with nn.MultiMarginLoss()"
      ],
      "metadata": {
        "id": "tmNoZwHDPCfG"
      },
      "id": "tmNoZwHDPCfG"
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 15\n",
        "train_loss, val_loss = [], []\n",
        "accuracy_total_train, accuracy_total_val = [], []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    total_train_loss = 0\n",
        "    total_val_loss = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    total = 0\n",
        "    # training our model\n",
        "    for idx, (image, label) in enumerate(trainLoader):\n",
        "\n",
        "        image, label = image.to(device), label.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        pred = model(image)\n",
        "\n",
        "        loss = criterion(pred, label)\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        loss.backward()    # calc gradients\n",
        "        optimizer.step()   # update gradients\n",
        "\n",
        "        #pred = torch.nn.functional.softmax(pred, dim=1)\n",
        "        for i, p in enumerate(pred):\n",
        "            if label[i] == torch.max(p.data, 0)[1]:\n",
        "                total = total + 1\n",
        "\n",
        "    accuracy_train = total / len(train_set)\n",
        "    accuracy_total_train.append(accuracy_train)\n",
        "\n",
        "    total_train_loss = total_train_loss / (idx + 1)\n",
        "    train_loss.append(total_train_loss)\n",
        "\n",
        "    # validating our model\n",
        "    model.eval()\n",
        "    total = 0\n",
        "    for idx, (image, label) in enumerate(testLoader):\n",
        "        image, label = image.cuda(), label.cuda()\n",
        "        pred = model(image)\n",
        "        loss = criterion(pred, label)\n",
        "        total_val_loss += loss.item()\n",
        "\n",
        "        #pred = torch.nn.functional.softmax(pred, dim=1)\n",
        "        for i, p in enumerate(pred):\n",
        "            if label[i] == torch.max(p.data, 0)[1]:\n",
        "                total = total + 1\n",
        "\n",
        "    accuracy_val = total / len(test_set)\n",
        "    accuracy_total_val.append(accuracy_val)\n",
        "\n",
        "    total_val_loss = total_val_loss / (idx + 1)\n",
        "    val_loss.append(total_val_loss)\n",
        "\n",
        "    if epoch % 5 == 0:\n",
        "      print(\"Epoch: {}/{}  \".format(epoch, epochs),\n",
        "            \"Training loss: {:.4f}  \".format(total_train_loss),\n",
        "            \"Testing loss: {:.4f}  \".format(total_val_loss),\n",
        "            \"Train accuracy: {:.4f}  \".format(accuracy_train),\n",
        "            \"Test accuracy: {:.4f}  \".format(accuracy_val))"
      ],
      "metadata": {
        "id": "1kv0uXa4Ej2U"
      },
      "id": "1kv0uXa4Ej2U",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.subplot(1,2,1)\n",
        "plt.plot(train_loss, label='Training loss')\n",
        "plt.plot(val_loss, label='Test loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(accuracy_total_train, label='Training Accuracy')\n",
        "plt.plot(accuracy_total_val, label='Test Accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "\n",
        "plt.suptitle(f'Model with 32 batch_size and MultiMarginLoss')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vAC_TuP8Ow6N"
      },
      "id": "vAC_TuP8Ow6N",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluating the Network Model with nn.MultiMarginLoss()"
      ],
      "metadata": {
        "id": "0fuGjnODPr04"
      },
      "id": "0fuGjnODPr04"
    },
    {
      "cell_type": "code",
      "source": [
        "img = images[0]\n",
        "img = img.to(device)\n",
        "img = img.view(-1, 1, 28, 28)\n",
        "print(img.shape)\n",
        "\n",
        "# Since we want to use the already pretrained weights to make some prediction\n",
        "# we are turning off the gradients\n",
        "with torch.no_grad():\n",
        "    logits = model.forward(img)"
      ],
      "metadata": {
        "id": "qLIM2FBZO4tS"
      },
      "id": "qLIM2FBZO4tS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We take the softmax for probabilites since our outputs are logits\n",
        "probabilities = F.softmax(logits, dim=1).detach().cpu().numpy().squeeze()\n",
        "\n",
        "print(probabilities)\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(figsize=(6,8), ncols=2)\n",
        "ax1.imshow(img.view(1, 28, 28).detach().cpu().numpy().squeeze(), cmap='inferno')\n",
        "ax1.axis('off')\n",
        "ax2.barh(np.arange(10), probabilities, color='r' )\n",
        "ax2.set_aspect(0.1)\n",
        "ax2.set_yticks(np.arange(10))\n",
        "ax2.set_yticklabels(np.arange(10))\n",
        "ax2.set_title('Class Probability')\n",
        "ax2.set_xlim(0, 1.1)\n",
        "\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "FD11h-eNO6Ow"
      },
      "id": "FD11h-eNO6Ow",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. PoissonNLLLoss:"
      ],
      "metadata": {
        "id": "aei0qsanpi9H"
      },
      "id": "aei0qsanpi9H"
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import optim\n",
        "from torchsummary import summary\n",
        "\n",
        "criterion = nn.PoissonNLLLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "summary(model, (1,28,28))"
      ],
      "metadata": {
        "id": "aXdqOo-ZmhBl"
      },
      "id": "aXdqOo-ZmhBl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training and Testing the Model with nn.PoissonNLLLoss()"
      ],
      "metadata": {
        "id": "-5rvnVNrPV3Y"
      },
      "id": "-5rvnVNrPV3Y"
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 15\n",
        "train_loss, val_loss = [], []\n",
        "accuracy_total_train, accuracy_total_val = [], []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    total_train_loss = 0\n",
        "    total_val_loss = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    total = 0\n",
        "    # training our model\n",
        "    for idx, (image, label) in enumerate(trainLoader):\n",
        "\n",
        "        image, label = image.to(device), label.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        pred = model(image)\n",
        "\n",
        "        loss = criterion(pred, label)\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        loss.backward()    # calc gradients\n",
        "        optimizer.step()   # update gradients\n",
        "\n",
        "        #pred = torch.nn.functional.softmax(pred, dim=1)\n",
        "        for i, p in enumerate(pred):\n",
        "            if label[i] == torch.max(p.data, 0)[1]:\n",
        "                total = total + 1\n",
        "\n",
        "    accuracy_train = total / len(train_set)\n",
        "    accuracy_total_train.append(accuracy_train)\n",
        "\n",
        "    total_train_loss = total_train_loss / (idx + 1)\n",
        "    train_loss.append(total_train_loss)\n",
        "\n",
        "    # validating our model\n",
        "    model.eval()\n",
        "    total = 0\n",
        "    for idx, (image, label) in enumerate(testLoader):\n",
        "        image, label = image.cuda(), label.cuda()\n",
        "        pred = model(image)\n",
        "        loss = criterion(pred, label)\n",
        "        total_val_loss += loss.item()\n",
        "\n",
        "        #pred = torch.nn.functional.softmax(pred, dim=1)\n",
        "        for i, p in enumerate(pred):\n",
        "            if label[i] == torch.max(p.data, 0)[1]:\n",
        "                total = total + 1\n",
        "\n",
        "    accuracy_val = total / len(test_set)\n",
        "    accuracy_total_val.append(accuracy_val)\n",
        "\n",
        "    total_val_loss = total_val_loss / (idx + 1)\n",
        "    val_loss.append(total_val_loss)\n",
        "\n",
        "    if epoch % 5 == 0:\n",
        "      print(\"Epoch: {}/{}  \".format(epoch, epochs),\n",
        "            \"Training loss: {:.4f}  \".format(total_train_loss),\n",
        "            \"Testing loss: {:.4f}  \".format(total_val_loss),\n",
        "            \"Train accuracy: {:.4f}  \".format(accuracy_train),\n",
        "            \"Test accuracy: {:.4f}  \".format(accuracy_val))"
      ],
      "metadata": {
        "id": "upA1I5GBQKko"
      },
      "id": "upA1I5GBQKko",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.subplot(1,2,1)\n",
        "plt.plot(train_loss, label='Training loss')\n",
        "plt.plot(val_loss, label='Test loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(accuracy_total_train, label='Training Accuracy')\n",
        "plt.plot(accuracy_total_val, label='Test Accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "\n",
        "plt.suptitle(f'Model with 32 batch_size and PoissonNLLLoss')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fL8B5lgfQqUJ"
      },
      "id": "fL8B5lgfQqUJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluating the Network Model with nn.PoissonNLLLoss()"
      ],
      "metadata": {
        "id": "H-KWp3kmPxVD"
      },
      "id": "H-KWp3kmPxVD"
    },
    {
      "cell_type": "code",
      "source": [
        "img = images[0]\n",
        "img = img.to(device)\n",
        "img = img.view(-1, 1, 28, 28)\n",
        "print(img.shape)\n",
        "\n",
        "# Since we want to use the already pretrained weights to make some prediction\n",
        "# we are turning off the gradients\n",
        "with torch.no_grad():\n",
        "    logits = model.forward(img)"
      ],
      "metadata": {
        "id": "KL8KroeTQrxD"
      },
      "id": "KL8KroeTQrxD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We take the softmax for probabilites since our outputs are logits\n",
        "probabilities = F.softmax(logits, dim=1).detach().cpu().numpy().squeeze()\n",
        "\n",
        "print(probabilities)\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(figsize=(6,8), ncols=2)\n",
        "ax1.imshow(img.view(1, 28, 28).detach().cpu().numpy().squeeze(), cmap='inferno')\n",
        "ax1.axis('off')\n",
        "ax2.barh(np.arange(10), probabilities, color='r' )\n",
        "ax2.set_aspect(0.1)\n",
        "ax2.set_yticks(np.arange(10))\n",
        "ax2.set_yticklabels(np.arange(10))\n",
        "ax2.set_title('Class Probability')\n",
        "ax2.set_xlim(0, 1.1)\n",
        "\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "Q8By0boKQrcu"
      },
      "id": "Q8By0boKQrcu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. GaussianNLLLoss"
      ],
      "metadata": {
        "id": "Yl5vk0KyoIAb"
      },
      "id": "Yl5vk0KyoIAb"
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import optim\n",
        "from torchsummary import summary\n",
        "\n",
        "criterion = nn.GaussianNLLLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "summary(model, (1,28,28))"
      ],
      "metadata": {
        "id": "YsDgmyJlmj76"
      },
      "id": "YsDgmyJlmj76",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training and Testing the Model with nn.GaussianNLLLoss()"
      ],
      "metadata": {
        "id": "Hfv5AlYtPdqO"
      },
      "id": "Hfv5AlYtPdqO"
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 15\n",
        "train_loss, val_loss = [], []\n",
        "accuracy_total_train, accuracy_total_val = [], []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    total_train_loss = 0\n",
        "    total_val_loss = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    total = 0\n",
        "    # training our model\n",
        "    for idx, (image, label) in enumerate(trainLoader):\n",
        "\n",
        "        image, label = image.to(device), label.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        pred = model(image)\n",
        "\n",
        "        loss = criterion(pred, label)\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        loss.backward()    # calc gradients\n",
        "        optimizer.step()   # update gradients\n",
        "\n",
        "        #pred = torch.nn.functional.softmax(pred, dim=1)\n",
        "        for i, p in enumerate(pred):\n",
        "            if label[i] == torch.max(p.data, 0)[1]:\n",
        "                total = total + 1\n",
        "\n",
        "    accuracy_train = total / len(train_set)\n",
        "    accuracy_total_train.append(accuracy_train)\n",
        "\n",
        "    total_train_loss = total_train_loss / (idx + 1)\n",
        "    train_loss.append(total_train_loss)\n",
        "\n",
        "    # validating our model\n",
        "    model.eval()\n",
        "    total = 0\n",
        "    for idx, (image, label) in enumerate(testLoader):\n",
        "        image, label = image.cuda(), label.cuda()\n",
        "        pred = model(image)\n",
        "        loss = criterion(pred, label)\n",
        "        total_val_loss += loss.item()\n",
        "\n",
        "        #pred = torch.nn.functional.softmax(pred, dim=1)\n",
        "        for i, p in enumerate(pred):\n",
        "            if label[i] == torch.max(p.data, 0)[1]:\n",
        "                total = total + 1\n",
        "\n",
        "    accuracy_val = total / len(test_set)\n",
        "    accuracy_total_val.append(accuracy_val)\n",
        "\n",
        "    total_val_loss = total_val_loss / (idx + 1)\n",
        "    val_loss.append(total_val_loss)\n",
        "\n",
        "    if epoch % 5 == 0:\n",
        "      print(\"Epoch: {}/{}  \".format(epoch, epochs),\n",
        "            \"Training loss: {:.4f}  \".format(total_train_loss),\n",
        "            \"Testing loss: {:.4f}  \".format(total_val_loss),\n",
        "            \"Train accuracy: {:.4f}  \".format(accuracy_train),\n",
        "            \"Test accuracy: {:.4f}  \".format(accuracy_val))"
      ],
      "metadata": {
        "id": "mEvbxp0gQO9b"
      },
      "id": "mEvbxp0gQO9b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.subplot(1,2,1)\n",
        "plt.plot(train_loss, label='Training loss')\n",
        "plt.plot(val_loss, label='Test loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(accuracy_total_train, label='Training Accuracy')\n",
        "plt.plot(accuracy_total_val, label='Test Accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "\n",
        "plt.suptitle(f'Model with 32 batch_size and GaussianNLLLoss')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Gf4f6qYmQswh"
      },
      "id": "Gf4f6qYmQswh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluating the Network Model with nn.GaussianNLLLoss()"
      ],
      "metadata": {
        "id": "TFXLUHIwPzsJ"
      },
      "id": "TFXLUHIwPzsJ"
    },
    {
      "cell_type": "code",
      "source": [
        "img = images[0]\n",
        "img = img.to(device)\n",
        "img = img.view(-1, 1, 28, 28)\n",
        "print(img.shape)\n",
        "\n",
        "# Since we want to use the already pretrained weights to make some prediction\n",
        "# we are turning off the gradients\n",
        "with torch.no_grad():\n",
        "    logits = model.forward(img)"
      ],
      "metadata": {
        "id": "RPt8gpoYQtcL"
      },
      "id": "RPt8gpoYQtcL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We take the softmax for probabilites since our outputs are logits\n",
        "probabilities = F.softmax(logits, dim=1).detach().cpu().numpy().squeeze()\n",
        "\n",
        "print(probabilities)\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(figsize=(6,8), ncols=2)\n",
        "ax1.imshow(img.view(1, 28, 28).detach().cpu().numpy().squeeze(), cmap='inferno')\n",
        "ax1.axis('off')\n",
        "ax2.barh(np.arange(10), probabilities, color='r' )\n",
        "ax2.set_aspect(0.1)\n",
        "ax2.set_yticks(np.arange(10))\n",
        "ax2.set_yticklabels(np.arange(10))\n",
        "ax2.set_title('Class Probability')\n",
        "ax2.set_xlim(0, 1.1)\n",
        "\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "7coc_MAQQtXF"
      },
      "id": "7coc_MAQQtXF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. MultiLabelMarginLoss:"
      ],
      "metadata": {
        "id": "n_mzMW0Inm-Q"
      },
      "id": "n_mzMW0Inm-Q"
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import optim\n",
        "from torchsummary import summary\n",
        "\n",
        "criterion = nn.MultiLabelMarginLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "summary(model, (1,28,28))"
      ],
      "metadata": {
        "id": "xFsGZu35mnBT"
      },
      "id": "xFsGZu35mnBT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training and Testing the Model with nn.MultiLabelMarginLoss()"
      ],
      "metadata": {
        "id": "xlx7A673m5Ub"
      },
      "id": "xlx7A673m5Ub"
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 15\n",
        "train_loss, val_loss = [], []\n",
        "accuracy_total_train, accuracy_total_val = [], []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    total_train_loss = 0\n",
        "    total_val_loss = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    total = 0\n",
        "    # training our model\n",
        "    for idx, (image, label) in enumerate(trainLoader):\n",
        "\n",
        "        image, label = image.to(device), label.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        pred = model(image)\n",
        "\n",
        "        loss = criterion(pred, label)\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        loss.backward()    # calc gradients\n",
        "        optimizer.step()   # update gradients\n",
        "\n",
        "        #pred = torch.nn.functional.softmax(pred, dim=1)\n",
        "        for i, p in enumerate(pred):\n",
        "            if label[i] == torch.max(p.data, 0)[1]:\n",
        "                total = total + 1\n",
        "\n",
        "    accuracy_train = total / len(train_set)\n",
        "    accuracy_total_train.append(accuracy_train)\n",
        "\n",
        "    total_train_loss = total_train_loss / (idx + 1)\n",
        "    train_loss.append(total_train_loss)\n",
        "\n",
        "    # validating our model\n",
        "    model.eval()\n",
        "    total = 0\n",
        "    for idx, (image, label) in enumerate(testLoader):\n",
        "        image, label = image.cuda(), label.cuda()\n",
        "        pred = model(image)\n",
        "        loss = criterion(pred, label)\n",
        "        total_val_loss += loss.item()\n",
        "\n",
        "        #pred = torch.nn.functional.softmax(pred, dim=1)\n",
        "        for i, p in enumerate(pred):\n",
        "            if label[i] == torch.max(p.data, 0)[1]:\n",
        "                total = total + 1\n",
        "\n",
        "    accuracy_val = total / len(test_set)\n",
        "    accuracy_total_val.append(accuracy_val)\n",
        "\n",
        "    total_val_loss = total_val_loss / (idx + 1)\n",
        "    val_loss.append(total_val_loss)\n",
        "\n",
        "    if epoch % 5 == 0:\n",
        "      print(\"Epoch: {}/{}  \".format(epoch, epochs),\n",
        "            \"Training loss: {:.4f}  \".format(total_train_loss),\n",
        "            \"Testing loss: {:.4f}  \".format(total_val_loss),\n",
        "            \"Train accuracy: {:.4f}  \".format(accuracy_train),\n",
        "            \"Test accuracy: {:.4f}  \".format(accuracy_val))"
      ],
      "metadata": {
        "id": "9u3aEXEJm6ZP"
      },
      "id": "9u3aEXEJm6ZP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.subplot(1,2,1)\n",
        "plt.plot(train_loss, label='Training loss')\n",
        "plt.plot(val_loss, label='Test loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(accuracy_total_train, label='Training Accuracy')\n",
        "plt.plot(accuracy_total_val, label='Test Accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "\n",
        "plt.suptitle(f'Model with 32 batch_size and MultiLabelMarginLoss')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o2vK22q1m7Ou"
      },
      "id": "o2vK22q1m7Ou",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Evaluating the Network Model with nn.MultiLabelMarginLoss()"
      ],
      "metadata": {
        "id": "msTSXVwdnIXO"
      },
      "id": "msTSXVwdnIXO"
    },
    {
      "cell_type": "code",
      "source": [
        "img = images[0]\n",
        "img = img.to(device)\n",
        "img = img.view(-1, 1, 28, 28)\n",
        "print(img.shape)\n",
        "\n",
        "# Since we want to use the already pretrained weights to make some prediction\n",
        "# we are turning off the gradients\n",
        "with torch.no_grad():\n",
        "    logits = model.forward(img)"
      ],
      "metadata": {
        "id": "-qPvV0Vqm7Ka"
      },
      "id": "-qPvV0Vqm7Ka",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We take the softmax for probabilites since our outputs are logits\n",
        "probabilities = F.softmax(logits, dim=1).detach().cpu().numpy().squeeze()\n",
        "\n",
        "print(probabilities)\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(figsize=(6,8), ncols=2)\n",
        "ax1.imshow(img.view(1, 28, 28).detach().cpu().numpy().squeeze(), cmap='inferno')\n",
        "ax1.axis('off')\n",
        "ax2.barh(np.arange(10), probabilities, color='r' )\n",
        "ax2.set_aspect(0.1)\n",
        "ax2.set_yticks(np.arange(10))\n",
        "ax2.set_yticklabels(np.arange(10))\n",
        "ax2.set_title('Class Probability')\n",
        "ax2.set_xlim(0, 1.1)\n",
        "\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "UWYVCPa8m7Gz"
      },
      "id": "UWYVCPa8m7Gz",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}