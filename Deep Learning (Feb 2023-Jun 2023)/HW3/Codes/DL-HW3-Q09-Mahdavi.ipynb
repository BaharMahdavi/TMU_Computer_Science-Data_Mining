{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# question 9\n",
        "## This question about Breast Cancer and How to develop a Fully Neural Network for it.\n",
        "# this question (9) can be divided to 4 part\n",
        "\n",
        "\n",
        "\n",
        "Dataset: Breast Cancer (Use scikit-learn)\n",
        "\n",
        "Train/test Split: ratio of (number of test samples / number of train samples) = 0.1\n",
        "\n",
        "Optimizer: Adam, Learning rate=0.001\n",
        "\n",
        "Loss : Cross-Entropy\n",
        "\n",
        "Number of epochs: 50\n",
        "\n",
        "Number of nodes in each hidden layer: 5\n",
        "\n",
        "Active functions:\n",
        "\n",
        "for model A: Sigmoid\n",
        "\n",
        "for model B: ReLU"
      ],
      "metadata": {
        "id": "hidGT0EhurtL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import torch.utils.data as utils\n",
        "import torch.utils.data as td"
      ],
      "metadata": {
        "id": "hIcD2mHrxAPq"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "9MJNW0SwMHn9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ModelA(Input_size,Output_size,Hiden_layers,Node_Number):\n",
        "  layers =[nn.Linear(Input_size, Node_Number),nn.Sigmoid()]\n",
        "  for i in range (Hiden_layers):\n",
        "    layers.append(nn.Linear(Node_Number, Node_Number))\n",
        "    layers.append(nn.Sigmoid())\n",
        "\n",
        "  #for last layer\n",
        "  layers.append(nn.Linear(Node_Number, Output_size))\n",
        "  layers.append(nn.Sigmoid())\n",
        "  return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model_A_3 = ModelA(30,2,3,5)\n",
        "model_A_3\n",
        "\n"
      ],
      "metadata": {
        "id": "g1RYiy5MwTD0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01cfd850-db92-49a1-e76d-0689e549e2f6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=30, out_features=5, bias=True)\n",
              "  (1): Sigmoid()\n",
              "  (2): Linear(in_features=5, out_features=5, bias=True)\n",
              "  (3): Sigmoid()\n",
              "  (4): Linear(in_features=5, out_features=5, bias=True)\n",
              "  (5): Sigmoid()\n",
              "  (6): Linear(in_features=5, out_features=5, bias=True)\n",
              "  (7): Sigmoid()\n",
              "  (8): Linear(in_features=5, out_features=2, bias=True)\n",
              "  (9): Sigmoid()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q9 Model A\n",
        "\n",
        "Create Model A using active function Sigmoid"
      ],
      "metadata": {
        "id": "s_HVi8gZMSkj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HiddenLayer=[3,9,13,17,19]\n",
        "\n",
        "model_A_3 =  ModelA(30,2,HiddenLayer[0],5)\n",
        "model_A_9 =  ModelA(30,2,HiddenLayer[1],5)\n",
        "model_A_13 = ModelA(30,2,HiddenLayer[2],5)\n",
        "model_A_17 = ModelA(30,2,HiddenLayer[3],5)\n",
        "model_A_19 = ModelA(30,2,HiddenLayer[4],5)\n",
        "MODELS= [model_A_3,model_A_9,model_A_13,model_A_17,model_A_19]\n",
        "model_A_3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AiNmx5aBJY6h",
        "outputId": "3cfeb419-4ce8-4800-aa0d-e6feb0c51595"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=30, out_features=5, bias=True)\n",
              "  (1): Sigmoid()\n",
              "  (2): Linear(in_features=5, out_features=5, bias=True)\n",
              "  (3): Sigmoid()\n",
              "  (4): Linear(in_features=5, out_features=5, bias=True)\n",
              "  (5): Sigmoid()\n",
              "  (6): Linear(in_features=5, out_features=5, bias=True)\n",
              "  (7): Sigmoid()\n",
              "  (8): Linear(in_features=5, out_features=2, bias=True)\n",
              "  (9): Sigmoid()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "breast = load_breast_cancer()\n",
        "\n",
        "x,y=breast.data,breast.target\n",
        "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=1) # splitting for train and validation set\n",
        "\n",
        "x_test,x_val,y_test,y_val=train_test_split(x_test,y_test,test_size=0.5,random_state=1) #further validation set is splitted for test set\n",
        "\n",
        "batch_size=10,\n",
        "\n",
        "# Create a dataset and loader for the training data and labels\n",
        "train_x = torch.Tensor(x_train).float()\n",
        "train_y = torch.Tensor(y_train).long()\n",
        "train_ds = utils.TensorDataset(train_x,train_y)\n",
        "train_loader = td.DataLoader(train_ds, batch_size=10,shuffle=True)\n",
        "\n",
        "# Create a dataset and loader for the test data and labels\n",
        "test_x = torch.Tensor(x_test).float()\n",
        "test_y = torch.Tensor(y_test).long()\n",
        "test_ds = utils.TensorDataset(test_x,test_y)\n",
        "test_loader = td.DataLoader(test_ds, batch_size=10, shuffle=True)\n",
        "\n",
        "# Create a dataset and loader for the test data and labels\n",
        "val_x = torch.Tensor(x_val).float()\n",
        "val_y = torch.Tensor(y_val).long()\n",
        "val_ds = utils.TensorDataset(val_x,val_y)\n",
        "val_loader = td.DataLoader(val_ds, batch_size=10, shuffle=True)"
      ],
      "metadata": {
        "id": "7Vv3Gd3KNMaq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, data_loader, data_loader1, criterion,optimizer):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    valid_loss = 0\n",
        "\n",
        "    for batch, tensor in enumerate(data_loader):\n",
        "        data, target = tensor\n",
        "        #feedforward\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data)\n",
        "        loss = criterion(out, target)\n",
        "        train_loss += loss.item()\n",
        "        # backpropagate\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        #train_loss += loss.item()\n",
        "## evaluation part\n",
        "    model.eval()\n",
        "    for batch1, tensor1 in enumerate(data_loader1):\n",
        "        data1, target1 = tensor1\n",
        "        output = model(data1)\n",
        "        loss1 = criterion(output, target1)\n",
        "        valid_loss += loss1.item()\n",
        "\n",
        "\n",
        "    #Return loss\n",
        "    avg_loss = train_loss / len(data_loader.dataset)\n",
        "    avg_loss1 = valid_loss/len(data_loader1.dataset)\n",
        "    return avg_loss, avg_loss1\n",
        "\n",
        "def test(model, data_loader,criterion):\n",
        "    # Switch the model to evaluation mode (so we don't backpropagate)\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch, tensor in enumerate(data_loader):\n",
        "            data, target = tensor\n",
        "            # Get the predictions\n",
        "            out = model(data)\n",
        "\n",
        "            # calculate the loss\n",
        "            test_loss += criterion(out, target).item()\n",
        "\n",
        "            # Calculate the accuracy\n",
        "            _, predicted = torch.max(out.data, 1)\n",
        "            correct += torch.sum(target==predicted).item()\n",
        "\n",
        "    # return validation loss and prediction accuracy for the epoch\n",
        "    avg_accuracy = correct / len(data_loader.dataset)\n",
        "    avg_loss = test_loss / len(data_loader.dataset)\n",
        "\n",
        "    return avg_loss, avg_accuracy"
      ],
      "metadata": {
        "id": "9LeZCo3KQg_r"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OPTIMIZERS=[]\n",
        "for m in MODELS:\n",
        "  optimizer = torch.optim.Adam(m.parameters(), lr=0.0001)\n",
        "  OPTIMIZERS.append(optimizer)\n",
        "\n",
        "\n",
        "OPTIMIZERS"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0S1VEZIrOmPR",
        "outputId": "a7c403ac-4f1a-45c7-f885-0e5cd8c5d25a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Adam (\n",
              " Parameter Group 0\n",
              "     amsgrad: False\n",
              "     betas: (0.9, 0.999)\n",
              "     capturable: False\n",
              "     differentiable: False\n",
              "     eps: 1e-08\n",
              "     foreach: None\n",
              "     fused: None\n",
              "     lr: 0.0001\n",
              "     maximize: False\n",
              "     weight_decay: 0\n",
              " ),\n",
              " Adam (\n",
              " Parameter Group 0\n",
              "     amsgrad: False\n",
              "     betas: (0.9, 0.999)\n",
              "     capturable: False\n",
              "     differentiable: False\n",
              "     eps: 1e-08\n",
              "     foreach: None\n",
              "     fused: None\n",
              "     lr: 0.0001\n",
              "     maximize: False\n",
              "     weight_decay: 0\n",
              " ),\n",
              " Adam (\n",
              " Parameter Group 0\n",
              "     amsgrad: False\n",
              "     betas: (0.9, 0.999)\n",
              "     capturable: False\n",
              "     differentiable: False\n",
              "     eps: 1e-08\n",
              "     foreach: None\n",
              "     fused: None\n",
              "     lr: 0.0001\n",
              "     maximize: False\n",
              "     weight_decay: 0\n",
              " ),\n",
              " Adam (\n",
              " Parameter Group 0\n",
              "     amsgrad: False\n",
              "     betas: (0.9, 0.999)\n",
              "     capturable: False\n",
              "     differentiable: False\n",
              "     eps: 1e-08\n",
              "     foreach: None\n",
              "     fused: None\n",
              "     lr: 0.0001\n",
              "     maximize: False\n",
              "     weight_decay: 0\n",
              " ),\n",
              " Adam (\n",
              " Parameter Group 0\n",
              "     amsgrad: False\n",
              "     betas: (0.9, 0.999)\n",
              "     capturable: False\n",
              "     differentiable: False\n",
              "     eps: 1e-08\n",
              "     foreach: None\n",
              "     fused: None\n",
              "     lr: 0.0001\n",
              "     maximize: False\n",
              "     weight_decay: 0\n",
              " )]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epoch_nums = []\n",
        "epochs=500\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "training_loss=[]\n",
        "validation_loss=[]\n",
        "acc_avg = []\n",
        "loss_avg = []\n",
        "for i in range (5):\n",
        "  tr_loss=[]\n",
        "  val_loss=[]\n",
        "  acc=[]\n",
        "  ave_loss = []\n",
        "  training_loss.append(tr_loss)\n",
        "  validation_loss.append(val_loss)\n",
        "  acc_avg.append(acc_avg)\n",
        "  loss_avg.append(ave_loss)\n",
        "\n",
        "\n",
        "\n",
        "for epoch in range(1, epochs):\n",
        "  epoch_nums.append(epoch)\n",
        "  for i in range(5):\n",
        "    # Feed the training data into the model to optimize the weights\n",
        "    train_loss,valid_loss = train(MODELS[i], train_loader,val_loader, criterion, OPTIMIZERS[i])\n",
        "    avg_loss, avg_accuracy = test (MODELS[i], val_loader,criterion)\n",
        "\n",
        "\n",
        "    # Log the metrcs for this epoch and models\n",
        "\n",
        "    training_loss[i].append(train_loss)\n",
        "    validation_loss[i].append(valid_loss)\n",
        "    acc_avg[i].append(avg_accuracy)\n",
        "    loss_avg[i].append(avg_loss)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Print stats for every 10th epoch so we can see training progress\n",
        "    if (epoch) % 1 == 0:\n",
        "        print('Epoch {:d} : for modelA {:} ==> Training loss= {:.4f} , Validation loss= {:.4f} , Average loss= {:.4f} , Accuracy = {:.4f}'.format(epoch,i, train_loss, valid_loss ,avg_loss, avg_accuracy))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rk6bMX2HLPDa",
        "outputId": "670dc127-d1de-4e1e-84a9-05ac85ac9eb1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 : for modelA 0 ==> Training loss= 0.0716 , Validation loss= 0.0755 , Average loss= 0.0754 , Accuracy = 0.2807\n",
            "Epoch 1 : for modelA 1 ==> Training loss= 0.0700 , Validation loss= 0.0727 , Average loss= 0.0727 , Accuracy = 0.7193\n",
            "Epoch 1 : for modelA 2 ==> Training loss= 0.0701 , Validation loss= 0.0728 , Average loss= 0.0729 , Accuracy = 0.7193\n",
            "Epoch 1 : for modelA 3 ==> Training loss= 0.0686 , Validation loss= 0.0699 , Average loss= 0.0702 , Accuracy = 0.7193\n",
            "Epoch 1 : for modelA 4 ==> Training loss= 0.0701 , Validation loss= 0.0728 , Average loss= 0.0728 , Accuracy = 0.7193\n",
            "Epoch 2 : for modelA 0 ==> Training loss= 0.0714 , Validation loss= 0.0754 , Average loss= 0.0752 , Accuracy = 0.2807\n",
            "Epoch 2 : for modelA 1 ==> Training loss= 0.0699 , Validation loss= 0.0726 , Average loss= 0.0726 , Accuracy = 0.7193\n",
            "Epoch 2 : for modelA 2 ==> Training loss= 0.0700 , Validation loss= 0.0727 , Average loss= 0.0727 , Accuracy = 0.7193\n",
            "Epoch 2 : for modelA 3 ==> Training loss= 0.0686 , Validation loss= 0.0699 , Average loss= 0.0701 , Accuracy = 0.7193\n",
            "Epoch 2 : for modelA 4 ==> Training loss= 0.0700 , Validation loss= 0.0727 , Average loss= 0.0727 , Accuracy = 0.7193\n",
            "Epoch 3 : for modelA 0 ==> Training loss= 0.0714 , Validation loss= 0.0750 , Average loss= 0.0752 , Accuracy = 0.2807\n",
            "Epoch 3 : for modelA 1 ==> Training loss= 0.0698 , Validation loss= 0.0724 , Average loss= 0.0724 , Accuracy = 0.7193\n",
            "Epoch 3 : for modelA 2 ==> Training loss= 0.0699 , Validation loss= 0.0726 , Average loss= 0.0726 , Accuracy = 0.7193\n",
            "Epoch 3 : for modelA 3 ==> Training loss= 0.0686 , Validation loss= 0.0699 , Average loss= 0.0700 , Accuracy = 0.7193\n",
            "Epoch 3 : for modelA 4 ==> Training loss= 0.0699 , Validation loss= 0.0725 , Average loss= 0.0725 , Accuracy = 0.7193\n",
            "Epoch 4 : for modelA 0 ==> Training loss= 0.0713 , Validation loss= 0.0749 , Average loss= 0.0749 , Accuracy = 0.2807\n",
            "Epoch 4 : for modelA 1 ==> Training loss= 0.0698 , Validation loss= 0.0723 , Average loss= 0.0723 , Accuracy = 0.7193\n",
            "Epoch 4 : for modelA 2 ==> Training loss= 0.0698 , Validation loss= 0.0725 , Average loss= 0.0725 , Accuracy = 0.7193\n",
            "Epoch 4 : for modelA 3 ==> Training loss= 0.0685 , Validation loss= 0.0697 , Average loss= 0.0698 , Accuracy = 0.7193\n",
            "Epoch 4 : for modelA 4 ==> Training loss= 0.0698 , Validation loss= 0.0724 , Average loss= 0.0724 , Accuracy = 0.7193\n",
            "Epoch 5 : for modelA 0 ==> Training loss= 0.0712 , Validation loss= 0.0749 , Average loss= 0.0749 , Accuracy = 0.2807\n",
            "Epoch 5 : for modelA 1 ==> Training loss= 0.0697 , Validation loss= 0.0722 , Average loss= 0.0722 , Accuracy = 0.7193\n",
            "Epoch 5 : for modelA 2 ==> Training loss= 0.0698 , Validation loss= 0.0724 , Average loss= 0.0723 , Accuracy = 0.7193\n",
            "Epoch 5 : for modelA 3 ==> Training loss= 0.0685 , Validation loss= 0.0697 , Average loss= 0.0701 , Accuracy = 0.7193\n",
            "Epoch 5 : for modelA 4 ==> Training loss= 0.0697 , Validation loss= 0.0723 , Average loss= 0.0722 , Accuracy = 0.7193\n",
            "Epoch 6 : for modelA 0 ==> Training loss= 0.0711 , Validation loss= 0.0748 , Average loss= 0.0748 , Accuracy = 0.2807\n",
            "Epoch 6 : for modelA 1 ==> Training loss= 0.0696 , Validation loss= 0.0721 , Average loss= 0.0720 , Accuracy = 0.7193\n",
            "Epoch 6 : for modelA 2 ==> Training loss= 0.0697 , Validation loss= 0.0722 , Average loss= 0.0722 , Accuracy = 0.7193\n",
            "Epoch 6 : for modelA 3 ==> Training loss= 0.0685 , Validation loss= 0.0697 , Average loss= 0.0695 , Accuracy = 0.7193\n",
            "Epoch 6 : for modelA 4 ==> Training loss= 0.0697 , Validation loss= 0.0721 , Average loss= 0.0721 , Accuracy = 0.7193\n",
            "Epoch 7 : for modelA 0 ==> Training loss= 0.0710 , Validation loss= 0.0745 , Average loss= 0.0746 , Accuracy = 0.2807\n",
            "Epoch 7 : for modelA 1 ==> Training loss= 0.0695 , Validation loss= 0.0720 , Average loss= 0.0719 , Accuracy = 0.7193\n",
            "Epoch 7 : for modelA 2 ==> Training loss= 0.0696 , Validation loss= 0.0721 , Average loss= 0.0721 , Accuracy = 0.7193\n",
            "Epoch 7 : for modelA 3 ==> Training loss= 0.0684 , Validation loss= 0.0698 , Average loss= 0.0697 , Accuracy = 0.7193\n",
            "Epoch 7 : for modelA 4 ==> Training loss= 0.0696 , Validation loss= 0.0720 , Average loss= 0.0719 , Accuracy = 0.7193\n",
            "Epoch 8 : for modelA 0 ==> Training loss= 0.0710 , Validation loss= 0.0744 , Average loss= 0.0743 , Accuracy = 0.2807\n",
            "Epoch 8 : for modelA 1 ==> Training loss= 0.0695 , Validation loss= 0.0718 , Average loss= 0.0717 , Accuracy = 0.7193\n",
            "Epoch 8 : for modelA 2 ==> Training loss= 0.0696 , Validation loss= 0.0719 , Average loss= 0.0720 , Accuracy = 0.7193\n",
            "Epoch 8 : for modelA 3 ==> Training loss= 0.0684 , Validation loss= 0.0693 , Average loss= 0.0697 , Accuracy = 0.7193\n",
            "Epoch 8 : for modelA 4 ==> Training loss= 0.0695 , Validation loss= 0.0718 , Average loss= 0.0718 , Accuracy = 0.7193\n",
            "Epoch 9 : for modelA 0 ==> Training loss= 0.0708 , Validation loss= 0.0742 , Average loss= 0.0742 , Accuracy = 0.2807\n",
            "Epoch 9 : for modelA 1 ==> Training loss= 0.0694 , Validation loss= 0.0717 , Average loss= 0.0717 , Accuracy = 0.7193\n",
            "Epoch 9 : for modelA 2 ==> Training loss= 0.0695 , Validation loss= 0.0719 , Average loss= 0.0719 , Accuracy = 0.7193\n",
            "Epoch 9 : for modelA 3 ==> Training loss= 0.0683 , Validation loss= 0.0696 , Average loss= 0.0694 , Accuracy = 0.7193\n",
            "Epoch 9 : for modelA 4 ==> Training loss= 0.0694 , Validation loss= 0.0717 , Average loss= 0.0717 , Accuracy = 0.7193\n",
            "Epoch 10 : for modelA 0 ==> Training loss= 0.0708 , Validation loss= 0.0742 , Average loss= 0.0742 , Accuracy = 0.2807\n",
            "Epoch 10 : for modelA 1 ==> Training loss= 0.0693 , Validation loss= 0.0716 , Average loss= 0.0716 , Accuracy = 0.7193\n",
            "Epoch 10 : for modelA 2 ==> Training loss= 0.0694 , Validation loss= 0.0717 , Average loss= 0.0717 , Accuracy = 0.7193\n",
            "Epoch 10 : for modelA 3 ==> Training loss= 0.0683 , Validation loss= 0.0697 , Average loss= 0.0693 , Accuracy = 0.7193\n",
            "Epoch 10 : for modelA 4 ==> Training loss= 0.0693 , Validation loss= 0.0715 , Average loss= 0.0715 , Accuracy = 0.7193\n",
            "Epoch 11 : for modelA 0 ==> Training loss= 0.0707 , Validation loss= 0.0740 , Average loss= 0.0741 , Accuracy = 0.2807\n",
            "Epoch 11 : for modelA 1 ==> Training loss= 0.0693 , Validation loss= 0.0714 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 11 : for modelA 2 ==> Training loss= 0.0694 , Validation loss= 0.0717 , Average loss= 0.0716 , Accuracy = 0.7193\n",
            "Epoch 11 : for modelA 3 ==> Training loss= 0.0682 , Validation loss= 0.0694 , Average loss= 0.0695 , Accuracy = 0.7193\n",
            "Epoch 11 : for modelA 4 ==> Training loss= 0.0693 , Validation loss= 0.0714 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 12 : for modelA 0 ==> Training loss= 0.0706 , Validation loss= 0.0739 , Average loss= 0.0739 , Accuracy = 0.2807\n",
            "Epoch 12 : for modelA 1 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 12 : for modelA 2 ==> Training loss= 0.0693 , Validation loss= 0.0715 , Average loss= 0.0715 , Accuracy = 0.7193\n",
            "Epoch 12 : for modelA 3 ==> Training loss= 0.0683 , Validation loss= 0.0692 , Average loss= 0.0692 , Accuracy = 0.7193\n",
            "Epoch 12 : for modelA 4 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0712 , Accuracy = 0.7193\n",
            "Epoch 13 : for modelA 0 ==> Training loss= 0.0706 , Validation loss= 0.0738 , Average loss= 0.0738 , Accuracy = 0.2807\n",
            "Epoch 13 : for modelA 1 ==> Training loss= 0.0692 , Validation loss= 0.0712 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 13 : for modelA 2 ==> Training loss= 0.0693 , Validation loss= 0.0714 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 13 : for modelA 3 ==> Training loss= 0.0681 , Validation loss= 0.0691 , Average loss= 0.0690 , Accuracy = 0.7193\n",
            "Epoch 13 : for modelA 4 ==> Training loss= 0.0691 , Validation loss= 0.0712 , Average loss= 0.0712 , Accuracy = 0.7193\n",
            "Epoch 14 : for modelA 0 ==> Training loss= 0.0705 , Validation loss= 0.0736 , Average loss= 0.0736 , Accuracy = 0.2807\n",
            "Epoch 14 : for modelA 1 ==> Training loss= 0.0691 , Validation loss= 0.0711 , Average loss= 0.0711 , Accuracy = 0.7193\n",
            "Epoch 14 : for modelA 2 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0712 , Accuracy = 0.7193\n",
            "Epoch 14 : for modelA 3 ==> Training loss= 0.0682 , Validation loss= 0.0691 , Average loss= 0.0691 , Accuracy = 0.7193\n",
            "Epoch 14 : for modelA 4 ==> Training loss= 0.0691 , Validation loss= 0.0710 , Average loss= 0.0710 , Accuracy = 0.7193\n",
            "Epoch 15 : for modelA 0 ==> Training loss= 0.0704 , Validation loss= 0.0735 , Average loss= 0.0735 , Accuracy = 0.2807\n",
            "Epoch 15 : for modelA 1 ==> Training loss= 0.0691 , Validation loss= 0.0710 , Average loss= 0.0709 , Accuracy = 0.7193\n",
            "Epoch 15 : for modelA 2 ==> Training loss= 0.0692 , Validation loss= 0.0712 , Average loss= 0.0711 , Accuracy = 0.7193\n",
            "Epoch 15 : for modelA 3 ==> Training loss= 0.0681 , Validation loss= 0.0689 , Average loss= 0.0690 , Accuracy = 0.7193\n",
            "Epoch 15 : for modelA 4 ==> Training loss= 0.0691 , Validation loss= 0.0709 , Average loss= 0.0710 , Accuracy = 0.7193\n",
            "Epoch 16 : for modelA 0 ==> Training loss= 0.0703 , Validation loss= 0.0734 , Average loss= 0.0734 , Accuracy = 0.2807\n",
            "Epoch 16 : for modelA 1 ==> Training loss= 0.0690 , Validation loss= 0.0709 , Average loss= 0.0708 , Accuracy = 0.7193\n",
            "Epoch 16 : for modelA 2 ==> Training loss= 0.0691 , Validation loss= 0.0710 , Average loss= 0.0711 , Accuracy = 0.7193\n",
            "Epoch 16 : for modelA 3 ==> Training loss= 0.0681 , Validation loss= 0.0691 , Average loss= 0.0691 , Accuracy = 0.7193\n",
            "Epoch 16 : for modelA 4 ==> Training loss= 0.0690 , Validation loss= 0.0708 , Average loss= 0.0707 , Accuracy = 0.7193\n",
            "Epoch 17 : for modelA 0 ==> Training loss= 0.0703 , Validation loss= 0.0733 , Average loss= 0.0733 , Accuracy = 0.2807\n",
            "Epoch 17 : for modelA 1 ==> Training loss= 0.0689 , Validation loss= 0.0708 , Average loss= 0.0708 , Accuracy = 0.7193\n",
            "Epoch 17 : for modelA 2 ==> Training loss= 0.0690 , Validation loss= 0.0710 , Average loss= 0.0709 , Accuracy = 0.7193\n",
            "Epoch 17 : for modelA 3 ==> Training loss= 0.0681 , Validation loss= 0.0692 , Average loss= 0.0692 , Accuracy = 0.7193\n",
            "Epoch 17 : for modelA 4 ==> Training loss= 0.0689 , Validation loss= 0.0706 , Average loss= 0.0707 , Accuracy = 0.7193\n",
            "Epoch 18 : for modelA 0 ==> Training loss= 0.0702 , Validation loss= 0.0731 , Average loss= 0.0731 , Accuracy = 0.2807\n",
            "Epoch 18 : for modelA 1 ==> Training loss= 0.0689 , Validation loss= 0.0707 , Average loss= 0.0707 , Accuracy = 0.7193\n",
            "Epoch 18 : for modelA 2 ==> Training loss= 0.0690 , Validation loss= 0.0709 , Average loss= 0.0708 , Accuracy = 0.7193\n",
            "Epoch 18 : for modelA 3 ==> Training loss= 0.0681 , Validation loss= 0.0690 , Average loss= 0.0688 , Accuracy = 0.7193\n",
            "Epoch 18 : for modelA 4 ==> Training loss= 0.0688 , Validation loss= 0.0708 , Average loss= 0.0706 , Accuracy = 0.7193\n",
            "Epoch 19 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 19 : for modelA 1 ==> Training loss= 0.0688 , Validation loss= 0.0704 , Average loss= 0.0706 , Accuracy = 0.7193\n",
            "Epoch 19 : for modelA 2 ==> Training loss= 0.0689 , Validation loss= 0.0707 , Average loss= 0.0708 , Accuracy = 0.7193\n",
            "Epoch 19 : for modelA 3 ==> Training loss= 0.0681 , Validation loss= 0.0689 , Average loss= 0.0692 , Accuracy = 0.7193\n",
            "Epoch 19 : for modelA 4 ==> Training loss= 0.0688 , Validation loss= 0.0706 , Average loss= 0.0705 , Accuracy = 0.7193\n",
            "Epoch 20 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0729 , Average loss= 0.0729 , Accuracy = 0.7193\n",
            "Epoch 20 : for modelA 1 ==> Training loss= 0.0688 , Validation loss= 0.0704 , Average loss= 0.0705 , Accuracy = 0.7193\n",
            "Epoch 20 : for modelA 2 ==> Training loss= 0.0689 , Validation loss= 0.0706 , Average loss= 0.0707 , Accuracy = 0.7193\n",
            "Epoch 20 : for modelA 3 ==> Training loss= 0.0681 , Validation loss= 0.0687 , Average loss= 0.0687 , Accuracy = 0.7193\n",
            "Epoch 20 : for modelA 4 ==> Training loss= 0.0687 , Validation loss= 0.0704 , Average loss= 0.0703 , Accuracy = 0.7193\n",
            "Epoch 21 : for modelA 0 ==> Training loss= 0.0700 , Validation loss= 0.0728 , Average loss= 0.0728 , Accuracy = 0.7193\n",
            "Epoch 21 : for modelA 1 ==> Training loss= 0.0687 , Validation loss= 0.0704 , Average loss= 0.0703 , Accuracy = 0.7193\n",
            "Epoch 21 : for modelA 2 ==> Training loss= 0.0688 , Validation loss= 0.0706 , Average loss= 0.0706 , Accuracy = 0.7193\n",
            "Epoch 21 : for modelA 3 ==> Training loss= 0.0680 , Validation loss= 0.0688 , Average loss= 0.0686 , Accuracy = 0.7193\n",
            "Epoch 21 : for modelA 4 ==> Training loss= 0.0687 , Validation loss= 0.0703 , Average loss= 0.0701 , Accuracy = 0.7193\n",
            "Epoch 22 : for modelA 0 ==> Training loss= 0.0699 , Validation loss= 0.0727 , Average loss= 0.0727 , Accuracy = 0.7193\n",
            "Epoch 22 : for modelA 1 ==> Training loss= 0.0687 , Validation loss= 0.0704 , Average loss= 0.0704 , Accuracy = 0.7193\n",
            "Epoch 22 : for modelA 2 ==> Training loss= 0.0688 , Validation loss= 0.0704 , Average loss= 0.0705 , Accuracy = 0.7193\n",
            "Epoch 22 : for modelA 3 ==> Training loss= 0.0680 , Validation loss= 0.0686 , Average loss= 0.0684 , Accuracy = 0.7193\n",
            "Epoch 22 : for modelA 4 ==> Training loss= 0.0686 , Validation loss= 0.0701 , Average loss= 0.0700 , Accuracy = 0.7193\n",
            "Epoch 23 : for modelA 0 ==> Training loss= 0.0699 , Validation loss= 0.0726 , Average loss= 0.0725 , Accuracy = 0.7193\n",
            "Epoch 23 : for modelA 1 ==> Training loss= 0.0686 , Validation loss= 0.0700 , Average loss= 0.0700 , Accuracy = 0.7193\n",
            "Epoch 23 : for modelA 2 ==> Training loss= 0.0688 , Validation loss= 0.0706 , Average loss= 0.0703 , Accuracy = 0.7193\n",
            "Epoch 23 : for modelA 3 ==> Training loss= 0.0680 , Validation loss= 0.0685 , Average loss= 0.0688 , Accuracy = 0.7193\n",
            "Epoch 23 : for modelA 4 ==> Training loss= 0.0686 , Validation loss= 0.0699 , Average loss= 0.0699 , Accuracy = 0.7193\n",
            "Epoch 24 : for modelA 0 ==> Training loss= 0.0698 , Validation loss= 0.0724 , Average loss= 0.0724 , Accuracy = 0.7193\n",
            "Epoch 24 : for modelA 1 ==> Training loss= 0.0686 , Validation loss= 0.0701 , Average loss= 0.0699 , Accuracy = 0.7193\n",
            "Epoch 24 : for modelA 2 ==> Training loss= 0.0687 , Validation loss= 0.0702 , Average loss= 0.0702 , Accuracy = 0.7193\n",
            "Epoch 24 : for modelA 3 ==> Training loss= 0.0679 , Validation loss= 0.0684 , Average loss= 0.0684 , Accuracy = 0.7193\n",
            "Epoch 24 : for modelA 4 ==> Training loss= 0.0685 , Validation loss= 0.0698 , Average loss= 0.0700 , Accuracy = 0.7193\n",
            "Epoch 25 : for modelA 0 ==> Training loss= 0.0697 , Validation loss= 0.0723 , Average loss= 0.0723 , Accuracy = 0.7193\n",
            "Epoch 25 : for modelA 1 ==> Training loss= 0.0685 , Validation loss= 0.0702 , Average loss= 0.0701 , Accuracy = 0.7193\n",
            "Epoch 25 : for modelA 2 ==> Training loss= 0.0687 , Validation loss= 0.0702 , Average loss= 0.0702 , Accuracy = 0.7193\n",
            "Epoch 25 : for modelA 3 ==> Training loss= 0.0680 , Validation loss= 0.0684 , Average loss= 0.0684 , Accuracy = 0.7193\n",
            "Epoch 25 : for modelA 4 ==> Training loss= 0.0685 , Validation loss= 0.0702 , Average loss= 0.0698 , Accuracy = 0.7193\n",
            "Epoch 26 : for modelA 0 ==> Training loss= 0.0697 , Validation loss= 0.0722 , Average loss= 0.0722 , Accuracy = 0.7193\n",
            "Epoch 26 : for modelA 1 ==> Training loss= 0.0684 , Validation loss= 0.0700 , Average loss= 0.0698 , Accuracy = 0.7193\n",
            "Epoch 26 : for modelA 2 ==> Training loss= 0.0687 , Validation loss= 0.0700 , Average loss= 0.0701 , Accuracy = 0.7193\n",
            "Epoch 26 : for modelA 3 ==> Training loss= 0.0678 , Validation loss= 0.0682 , Average loss= 0.0687 , Accuracy = 0.7193\n",
            "Epoch 26 : for modelA 4 ==> Training loss= 0.0684 , Validation loss= 0.0696 , Average loss= 0.0698 , Accuracy = 0.7193\n",
            "Epoch 27 : for modelA 0 ==> Training loss= 0.0696 , Validation loss= 0.0721 , Average loss= 0.0721 , Accuracy = 0.7193\n",
            "Epoch 27 : for modelA 1 ==> Training loss= 0.0685 , Validation loss= 0.0696 , Average loss= 0.0700 , Accuracy = 0.7193\n",
            "Epoch 27 : for modelA 2 ==> Training loss= 0.0685 , Validation loss= 0.0698 , Average loss= 0.0700 , Accuracy = 0.7193\n",
            "Epoch 27 : for modelA 3 ==> Training loss= 0.0679 , Validation loss= 0.0683 , Average loss= 0.0684 , Accuracy = 0.7193\n",
            "Epoch 27 : for modelA 4 ==> Training loss= 0.0685 , Validation loss= 0.0696 , Average loss= 0.0695 , Accuracy = 0.7193\n",
            "Epoch 28 : for modelA 0 ==> Training loss= 0.0696 , Validation loss= 0.0720 , Average loss= 0.0720 , Accuracy = 0.7193\n",
            "Epoch 28 : for modelA 1 ==> Training loss= 0.0684 , Validation loss= 0.0698 , Average loss= 0.0697 , Accuracy = 0.7193\n",
            "Epoch 28 : for modelA 2 ==> Training loss= 0.0685 , Validation loss= 0.0698 , Average loss= 0.0699 , Accuracy = 0.7193\n",
            "Epoch 28 : for modelA 3 ==> Training loss= 0.0678 , Validation loss= 0.0684 , Average loss= 0.0684 , Accuracy = 0.7193\n",
            "Epoch 28 : for modelA 4 ==> Training loss= 0.0683 , Validation loss= 0.0694 , Average loss= 0.0697 , Accuracy = 0.7193\n",
            "Epoch 29 : for modelA 0 ==> Training loss= 0.0695 , Validation loss= 0.0719 , Average loss= 0.0718 , Accuracy = 0.7193\n",
            "Epoch 29 : for modelA 1 ==> Training loss= 0.0684 , Validation loss= 0.0700 , Average loss= 0.0697 , Accuracy = 0.7193\n",
            "Epoch 29 : for modelA 2 ==> Training loss= 0.0685 , Validation loss= 0.0698 , Average loss= 0.0697 , Accuracy = 0.7193\n",
            "Epoch 29 : for modelA 3 ==> Training loss= 0.0678 , Validation loss= 0.0683 , Average loss= 0.0682 , Accuracy = 0.7193\n",
            "Epoch 29 : for modelA 4 ==> Training loss= 0.0683 , Validation loss= 0.0693 , Average loss= 0.0694 , Accuracy = 0.7193\n",
            "Epoch 30 : for modelA 0 ==> Training loss= 0.0695 , Validation loss= 0.0718 , Average loss= 0.0717 , Accuracy = 0.7193\n",
            "Epoch 30 : for modelA 1 ==> Training loss= 0.0683 , Validation loss= 0.0694 , Average loss= 0.0695 , Accuracy = 0.7193\n",
            "Epoch 30 : for modelA 2 ==> Training loss= 0.0684 , Validation loss= 0.0697 , Average loss= 0.0700 , Accuracy = 0.7193\n",
            "Epoch 30 : for modelA 3 ==> Training loss= 0.0677 , Validation loss= 0.0683 , Average loss= 0.0681 , Accuracy = 0.7193\n",
            "Epoch 30 : for modelA 4 ==> Training loss= 0.0683 , Validation loss= 0.0693 , Average loss= 0.0693 , Accuracy = 0.7193\n",
            "Epoch 31 : for modelA 0 ==> Training loss= 0.0694 , Validation loss= 0.0717 , Average loss= 0.0716 , Accuracy = 0.7193\n",
            "Epoch 31 : for modelA 1 ==> Training loss= 0.0683 , Validation loss= 0.0694 , Average loss= 0.0695 , Accuracy = 0.7193\n",
            "Epoch 31 : for modelA 2 ==> Training loss= 0.0684 , Validation loss= 0.0697 , Average loss= 0.0697 , Accuracy = 0.7193\n",
            "Epoch 31 : for modelA 3 ==> Training loss= 0.0678 , Validation loss= 0.0681 , Average loss= 0.0682 , Accuracy = 0.7193\n",
            "Epoch 31 : for modelA 4 ==> Training loss= 0.0682 , Validation loss= 0.0692 , Average loss= 0.0691 , Accuracy = 0.7193\n",
            "Epoch 32 : for modelA 0 ==> Training loss= 0.0694 , Validation loss= 0.0716 , Average loss= 0.0716 , Accuracy = 0.7193\n",
            "Epoch 32 : for modelA 1 ==> Training loss= 0.0683 , Validation loss= 0.0692 , Average loss= 0.0695 , Accuracy = 0.7193\n",
            "Epoch 32 : for modelA 2 ==> Training loss= 0.0683 , Validation loss= 0.0695 , Average loss= 0.0695 , Accuracy = 0.7193\n",
            "Epoch 32 : for modelA 3 ==> Training loss= 0.0678 , Validation loss= 0.0680 , Average loss= 0.0684 , Accuracy = 0.7193\n",
            "Epoch 32 : for modelA 4 ==> Training loss= 0.0681 , Validation loss= 0.0689 , Average loss= 0.0694 , Accuracy = 0.7193\n",
            "Epoch 33 : for modelA 0 ==> Training loss= 0.0693 , Validation loss= 0.0714 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 33 : for modelA 1 ==> Training loss= 0.0683 , Validation loss= 0.0692 , Average loss= 0.0692 , Accuracy = 0.7193\n",
            "Epoch 33 : for modelA 2 ==> Training loss= 0.0684 , Validation loss= 0.0695 , Average loss= 0.0693 , Accuracy = 0.7193\n",
            "Epoch 33 : for modelA 3 ==> Training loss= 0.0677 , Validation loss= 0.0683 , Average loss= 0.0681 , Accuracy = 0.7193\n",
            "Epoch 33 : for modelA 4 ==> Training loss= 0.0681 , Validation loss= 0.0689 , Average loss= 0.0689 , Accuracy = 0.7193\n",
            "Epoch 34 : for modelA 0 ==> Training loss= 0.0693 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 34 : for modelA 1 ==> Training loss= 0.0683 , Validation loss= 0.0690 , Average loss= 0.0689 , Accuracy = 0.7193\n",
            "Epoch 34 : for modelA 2 ==> Training loss= 0.0683 , Validation loss= 0.0693 , Average loss= 0.0692 , Accuracy = 0.7193\n",
            "Epoch 34 : for modelA 3 ==> Training loss= 0.0678 , Validation loss= 0.0684 , Average loss= 0.0681 , Accuracy = 0.7193\n",
            "Epoch 34 : for modelA 4 ==> Training loss= 0.0681 , Validation loss= 0.0690 , Average loss= 0.0690 , Accuracy = 0.7193\n",
            "Epoch 35 : for modelA 0 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 35 : for modelA 1 ==> Training loss= 0.0681 , Validation loss= 0.0694 , Average loss= 0.0691 , Accuracy = 0.7193\n",
            "Epoch 35 : for modelA 2 ==> Training loss= 0.0683 , Validation loss= 0.0695 , Average loss= 0.0692 , Accuracy = 0.7193\n",
            "Epoch 35 : for modelA 3 ==> Training loss= 0.0676 , Validation loss= 0.0682 , Average loss= 0.0680 , Accuracy = 0.7193\n",
            "Epoch 35 : for modelA 4 ==> Training loss= 0.0680 , Validation loss= 0.0687 , Average loss= 0.0689 , Accuracy = 0.7193\n",
            "Epoch 36 : for modelA 0 ==> Training loss= 0.0691 , Validation loss= 0.0711 , Average loss= 0.0712 , Accuracy = 0.7193\n",
            "Epoch 36 : for modelA 1 ==> Training loss= 0.0682 , Validation loss= 0.0690 , Average loss= 0.0689 , Accuracy = 0.7193\n",
            "Epoch 36 : for modelA 2 ==> Training loss= 0.0682 , Validation loss= 0.0692 , Average loss= 0.0693 , Accuracy = 0.7193\n",
            "Epoch 36 : for modelA 3 ==> Training loss= 0.0676 , Validation loss= 0.0678 , Average loss= 0.0680 , Accuracy = 0.7193\n",
            "Epoch 36 : for modelA 4 ==> Training loss= 0.0680 , Validation loss= 0.0688 , Average loss= 0.0689 , Accuracy = 0.7193\n",
            "Epoch 37 : for modelA 0 ==> Training loss= 0.0691 , Validation loss= 0.0711 , Average loss= 0.0711 , Accuracy = 0.7193\n",
            "Epoch 37 : for modelA 1 ==> Training loss= 0.0681 , Validation loss= 0.0689 , Average loss= 0.0689 , Accuracy = 0.7193\n",
            "Epoch 37 : for modelA 2 ==> Training loss= 0.0682 , Validation loss= 0.0691 , Average loss= 0.0693 , Accuracy = 0.7193\n",
            "Epoch 37 : for modelA 3 ==> Training loss= 0.0677 , Validation loss= 0.0679 , Average loss= 0.0677 , Accuracy = 0.7193\n",
            "Epoch 37 : for modelA 4 ==> Training loss= 0.0679 , Validation loss= 0.0689 , Average loss= 0.0685 , Accuracy = 0.7193\n",
            "Epoch 38 : for modelA 0 ==> Training loss= 0.0690 , Validation loss= 0.0709 , Average loss= 0.0711 , Accuracy = 0.7193\n",
            "Epoch 38 : for modelA 1 ==> Training loss= 0.0680 , Validation loss= 0.0687 , Average loss= 0.0690 , Accuracy = 0.7193\n",
            "Epoch 38 : for modelA 2 ==> Training loss= 0.0681 , Validation loss= 0.0693 , Average loss= 0.0690 , Accuracy = 0.7193\n",
            "Epoch 38 : for modelA 3 ==> Training loss= 0.0677 , Validation loss= 0.0675 , Average loss= 0.0679 , Accuracy = 0.7193\n",
            "Epoch 38 : for modelA 4 ==> Training loss= 0.0679 , Validation loss= 0.0686 , Average loss= 0.0686 , Accuracy = 0.7193\n",
            "Epoch 39 : for modelA 0 ==> Training loss= 0.0690 , Validation loss= 0.0710 , Average loss= 0.0709 , Accuracy = 0.7193\n",
            "Epoch 39 : for modelA 1 ==> Training loss= 0.0681 , Validation loss= 0.0688 , Average loss= 0.0688 , Accuracy = 0.7193\n",
            "Epoch 39 : for modelA 2 ==> Training loss= 0.0681 , Validation loss= 0.0689 , Average loss= 0.0694 , Accuracy = 0.7193\n",
            "Epoch 39 : for modelA 3 ==> Training loss= 0.0675 , Validation loss= 0.0680 , Average loss= 0.0676 , Accuracy = 0.7193\n",
            "Epoch 39 : for modelA 4 ==> Training loss= 0.0680 , Validation loss= 0.0684 , Average loss= 0.0687 , Accuracy = 0.7193\n",
            "Epoch 40 : for modelA 0 ==> Training loss= 0.0689 , Validation loss= 0.0707 , Average loss= 0.0708 , Accuracy = 0.7193\n",
            "Epoch 40 : for modelA 1 ==> Training loss= 0.0679 , Validation loss= 0.0686 , Average loss= 0.0687 , Accuracy = 0.7193\n",
            "Epoch 40 : for modelA 2 ==> Training loss= 0.0680 , Validation loss= 0.0690 , Average loss= 0.0693 , Accuracy = 0.7193\n",
            "Epoch 40 : for modelA 3 ==> Training loss= 0.0676 , Validation loss= 0.0682 , Average loss= 0.0676 , Accuracy = 0.7193\n",
            "Epoch 40 : for modelA 4 ==> Training loss= 0.0679 , Validation loss= 0.0684 , Average loss= 0.0684 , Accuracy = 0.7193\n",
            "Epoch 41 : for modelA 0 ==> Training loss= 0.0689 , Validation loss= 0.0706 , Average loss= 0.0708 , Accuracy = 0.7193\n",
            "Epoch 41 : for modelA 1 ==> Training loss= 0.0680 , Validation loss= 0.0686 , Average loss= 0.0686 , Accuracy = 0.7193\n",
            "Epoch 41 : for modelA 2 ==> Training loss= 0.0681 , Validation loss= 0.0689 , Average loss= 0.0694 , Accuracy = 0.7193\n",
            "Epoch 41 : for modelA 3 ==> Training loss= 0.0675 , Validation loss= 0.0677 , Average loss= 0.0679 , Accuracy = 0.7193\n",
            "Epoch 41 : for modelA 4 ==> Training loss= 0.0678 , Validation loss= 0.0684 , Average loss= 0.0682 , Accuracy = 0.7193\n",
            "Epoch 42 : for modelA 0 ==> Training loss= 0.0688 , Validation loss= 0.0706 , Average loss= 0.0706 , Accuracy = 0.7193\n",
            "Epoch 42 : for modelA 1 ==> Training loss= 0.0680 , Validation loss= 0.0682 , Average loss= 0.0682 , Accuracy = 0.7193\n",
            "Epoch 42 : for modelA 2 ==> Training loss= 0.0680 , Validation loss= 0.0689 , Average loss= 0.0690 , Accuracy = 0.7193\n",
            "Epoch 42 : for modelA 3 ==> Training loss= 0.0675 , Validation loss= 0.0679 , Average loss= 0.0677 , Accuracy = 0.7193\n",
            "Epoch 42 : for modelA 4 ==> Training loss= 0.0678 , Validation loss= 0.0681 , Average loss= 0.0686 , Accuracy = 0.7193\n",
            "Epoch 43 : for modelA 0 ==> Training loss= 0.0688 , Validation loss= 0.0705 , Average loss= 0.0706 , Accuracy = 0.7193\n",
            "Epoch 43 : for modelA 1 ==> Training loss= 0.0678 , Validation loss= 0.0683 , Average loss= 0.0683 , Accuracy = 0.7193\n",
            "Epoch 43 : for modelA 2 ==> Training loss= 0.0680 , Validation loss= 0.0691 , Average loss= 0.0691 , Accuracy = 0.7193\n",
            "Epoch 43 : for modelA 3 ==> Training loss= 0.0675 , Validation loss= 0.0674 , Average loss= 0.0674 , Accuracy = 0.7193\n",
            "Epoch 43 : for modelA 4 ==> Training loss= 0.0677 , Validation loss= 0.0682 , Average loss= 0.0688 , Accuracy = 0.7193\n",
            "Epoch 44 : for modelA 0 ==> Training loss= 0.0688 , Validation loss= 0.0704 , Average loss= 0.0705 , Accuracy = 0.7193\n",
            "Epoch 44 : for modelA 1 ==> Training loss= 0.0678 , Validation loss= 0.0683 , Average loss= 0.0683 , Accuracy = 0.7193\n",
            "Epoch 44 : for modelA 2 ==> Training loss= 0.0680 , Validation loss= 0.0690 , Average loss= 0.0687 , Accuracy = 0.7193\n",
            "Epoch 44 : for modelA 3 ==> Training loss= 0.0676 , Validation loss= 0.0680 , Average loss= 0.0678 , Accuracy = 0.7193\n",
            "Epoch 44 : for modelA 4 ==> Training loss= 0.0677 , Validation loss= 0.0681 , Average loss= 0.0681 , Accuracy = 0.7193\n",
            "Epoch 45 : for modelA 0 ==> Training loss= 0.0687 , Validation loss= 0.0705 , Average loss= 0.0704 , Accuracy = 0.7193\n",
            "Epoch 45 : for modelA 1 ==> Training loss= 0.0679 , Validation loss= 0.0684 , Average loss= 0.0684 , Accuracy = 0.7193\n",
            "Epoch 45 : for modelA 2 ==> Training loss= 0.0679 , Validation loss= 0.0688 , Average loss= 0.0685 , Accuracy = 0.7193\n",
            "Epoch 45 : for modelA 3 ==> Training loss= 0.0674 , Validation loss= 0.0674 , Average loss= 0.0674 , Accuracy = 0.7193\n",
            "Epoch 45 : for modelA 4 ==> Training loss= 0.0678 , Validation loss= 0.0677 , Average loss= 0.0681 , Accuracy = 0.7193\n",
            "Epoch 46 : for modelA 0 ==> Training loss= 0.0688 , Validation loss= 0.0703 , Average loss= 0.0703 , Accuracy = 0.7193\n",
            "Epoch 46 : for modelA 1 ==> Training loss= 0.0679 , Validation loss= 0.0683 , Average loss= 0.0685 , Accuracy = 0.7193\n",
            "Epoch 46 : for modelA 2 ==> Training loss= 0.0680 , Validation loss= 0.0686 , Average loss= 0.0683 , Accuracy = 0.7193\n",
            "Epoch 46 : for modelA 3 ==> Training loss= 0.0676 , Validation loss= 0.0677 , Average loss= 0.0675 , Accuracy = 0.7193\n",
            "Epoch 46 : for modelA 4 ==> Training loss= 0.0677 , Validation loss= 0.0678 , Average loss= 0.0678 , Accuracy = 0.7193\n",
            "Epoch 47 : for modelA 0 ==> Training loss= 0.0687 , Validation loss= 0.0701 , Average loss= 0.0702 , Accuracy = 0.7193\n",
            "Epoch 47 : for modelA 1 ==> Training loss= 0.0678 , Validation loss= 0.0684 , Average loss= 0.0684 , Accuracy = 0.7193\n",
            "Epoch 47 : for modelA 2 ==> Training loss= 0.0679 , Validation loss= 0.0684 , Average loss= 0.0684 , Accuracy = 0.7193\n",
            "Epoch 47 : for modelA 3 ==> Training loss= 0.0675 , Validation loss= 0.0675 , Average loss= 0.0677 , Accuracy = 0.7193\n",
            "Epoch 47 : for modelA 4 ==> Training loss= 0.0676 , Validation loss= 0.0679 , Average loss= 0.0681 , Accuracy = 0.7193\n",
            "Epoch 48 : for modelA 0 ==> Training loss= 0.0686 , Validation loss= 0.0700 , Average loss= 0.0701 , Accuracy = 0.7193\n",
            "Epoch 48 : for modelA 1 ==> Training loss= 0.0677 , Validation loss= 0.0678 , Average loss= 0.0680 , Accuracy = 0.7193\n",
            "Epoch 48 : for modelA 2 ==> Training loss= 0.0678 , Validation loss= 0.0683 , Average loss= 0.0685 , Accuracy = 0.7193\n",
            "Epoch 48 : for modelA 3 ==> Training loss= 0.0675 , Validation loss= 0.0672 , Average loss= 0.0677 , Accuracy = 0.7193\n",
            "Epoch 48 : for modelA 4 ==> Training loss= 0.0676 , Validation loss= 0.0677 , Average loss= 0.0677 , Accuracy = 0.7193\n",
            "Epoch 49 : for modelA 0 ==> Training loss= 0.0685 , Validation loss= 0.0700 , Average loss= 0.0700 , Accuracy = 0.7193\n",
            "Epoch 49 : for modelA 1 ==> Training loss= 0.0678 , Validation loss= 0.0681 , Average loss= 0.0679 , Accuracy = 0.7193\n",
            "Epoch 49 : for modelA 2 ==> Training loss= 0.0678 , Validation loss= 0.0684 , Average loss= 0.0684 , Accuracy = 0.7193\n",
            "Epoch 49 : for modelA 3 ==> Training loss= 0.0674 , Validation loss= 0.0672 , Average loss= 0.0670 , Accuracy = 0.7193\n",
            "Epoch 49 : for modelA 4 ==> Training loss= 0.0677 , Validation loss= 0.0676 , Average loss= 0.0676 , Accuracy = 0.7193\n",
            "Epoch 50 : for modelA 0 ==> Training loss= 0.0686 , Validation loss= 0.0700 , Average loss= 0.0700 , Accuracy = 0.7193\n",
            "Epoch 50 : for modelA 1 ==> Training loss= 0.0677 , Validation loss= 0.0680 , Average loss= 0.0678 , Accuracy = 0.7193\n",
            "Epoch 50 : for modelA 2 ==> Training loss= 0.0678 , Validation loss= 0.0682 , Average loss= 0.0684 , Accuracy = 0.7193\n",
            "Epoch 50 : for modelA 3 ==> Training loss= 0.0674 , Validation loss= 0.0674 , Average loss= 0.0669 , Accuracy = 0.7193\n",
            "Epoch 50 : for modelA 4 ==> Training loss= 0.0676 , Validation loss= 0.0675 , Average loss= 0.0673 , Accuracy = 0.7193\n",
            "Epoch 51 : for modelA 0 ==> Training loss= 0.0685 , Validation loss= 0.0700 , Average loss= 0.0699 , Accuracy = 0.7193\n",
            "Epoch 51 : for modelA 1 ==> Training loss= 0.0677 , Validation loss= 0.0676 , Average loss= 0.0682 , Accuracy = 0.7193\n",
            "Epoch 51 : for modelA 2 ==> Training loss= 0.0679 , Validation loss= 0.0683 , Average loss= 0.0681 , Accuracy = 0.7193\n",
            "Epoch 51 : for modelA 3 ==> Training loss= 0.0674 , Validation loss= 0.0675 , Average loss= 0.0673 , Accuracy = 0.7193\n",
            "Epoch 51 : for modelA 4 ==> Training loss= 0.0676 , Validation loss= 0.0679 , Average loss= 0.0673 , Accuracy = 0.7193\n",
            "Epoch 52 : for modelA 0 ==> Training loss= 0.0685 , Validation loss= 0.0699 , Average loss= 0.0698 , Accuracy = 0.7193\n",
            "Epoch 52 : for modelA 1 ==> Training loss= 0.0676 , Validation loss= 0.0679 , Average loss= 0.0679 , Accuracy = 0.7193\n",
            "Epoch 52 : for modelA 2 ==> Training loss= 0.0677 , Validation loss= 0.0684 , Average loss= 0.0681 , Accuracy = 0.7193\n",
            "Epoch 52 : for modelA 3 ==> Training loss= 0.0674 , Validation loss= 0.0677 , Average loss= 0.0671 , Accuracy = 0.7193\n",
            "Epoch 52 : for modelA 4 ==> Training loss= 0.0675 , Validation loss= 0.0678 , Average loss= 0.0674 , Accuracy = 0.7193\n",
            "Epoch 53 : for modelA 0 ==> Training loss= 0.0684 , Validation loss= 0.0699 , Average loss= 0.0700 , Accuracy = 0.7193\n",
            "Epoch 53 : for modelA 1 ==> Training loss= 0.0676 , Validation loss= 0.0678 , Average loss= 0.0675 , Accuracy = 0.7193\n",
            "Epoch 53 : for modelA 2 ==> Training loss= 0.0678 , Validation loss= 0.0684 , Average loss= 0.0682 , Accuracy = 0.7193\n",
            "Epoch 53 : for modelA 3 ==> Training loss= 0.0674 , Validation loss= 0.0677 , Average loss= 0.0672 , Accuracy = 0.7193\n",
            "Epoch 53 : for modelA 4 ==> Training loss= 0.0675 , Validation loss= 0.0682 , Average loss= 0.0674 , Accuracy = 0.7193\n",
            "Epoch 54 : for modelA 0 ==> Training loss= 0.0684 , Validation loss= 0.0697 , Average loss= 0.0697 , Accuracy = 0.7193\n",
            "Epoch 54 : for modelA 1 ==> Training loss= 0.0677 , Validation loss= 0.0678 , Average loss= 0.0678 , Accuracy = 0.7193\n",
            "Epoch 54 : for modelA 2 ==> Training loss= 0.0677 , Validation loss= 0.0683 , Average loss= 0.0681 , Accuracy = 0.7193\n",
            "Epoch 54 : for modelA 3 ==> Training loss= 0.0673 , Validation loss= 0.0670 , Average loss= 0.0670 , Accuracy = 0.7193\n",
            "Epoch 54 : for modelA 4 ==> Training loss= 0.0675 , Validation loss= 0.0675 , Average loss= 0.0673 , Accuracy = 0.7193\n",
            "Epoch 55 : for modelA 0 ==> Training loss= 0.0683 , Validation loss= 0.0696 , Average loss= 0.0697 , Accuracy = 0.7193\n",
            "Epoch 55 : for modelA 1 ==> Training loss= 0.0676 , Validation loss= 0.0679 , Average loss= 0.0675 , Accuracy = 0.7193\n",
            "Epoch 55 : for modelA 2 ==> Training loss= 0.0678 , Validation loss= 0.0677 , Average loss= 0.0682 , Accuracy = 0.7193\n",
            "Epoch 55 : for modelA 3 ==> Training loss= 0.0674 , Validation loss= 0.0674 , Average loss= 0.0667 , Accuracy = 0.7193\n",
            "Epoch 55 : for modelA 4 ==> Training loss= 0.0675 , Validation loss= 0.0672 , Average loss= 0.0674 , Accuracy = 0.7193\n",
            "Epoch 56 : for modelA 0 ==> Training loss= 0.0683 , Validation loss= 0.0694 , Average loss= 0.0695 , Accuracy = 0.7193\n",
            "Epoch 56 : for modelA 1 ==> Training loss= 0.0675 , Validation loss= 0.0675 , Average loss= 0.0677 , Accuracy = 0.7193\n",
            "Epoch 56 : for modelA 2 ==> Training loss= 0.0677 , Validation loss= 0.0680 , Average loss= 0.0682 , Accuracy = 0.7193\n",
            "Epoch 56 : for modelA 3 ==> Training loss= 0.0674 , Validation loss= 0.0671 , Average loss= 0.0674 , Accuracy = 0.7193\n",
            "Epoch 56 : for modelA 4 ==> Training loss= 0.0675 , Validation loss= 0.0674 , Average loss= 0.0674 , Accuracy = 0.7193\n",
            "Epoch 57 : for modelA 0 ==> Training loss= 0.0683 , Validation loss= 0.0694 , Average loss= 0.0695 , Accuracy = 0.7193\n",
            "Epoch 57 : for modelA 1 ==> Training loss= 0.0676 , Validation loss= 0.0676 , Average loss= 0.0676 , Accuracy = 0.7193\n",
            "Epoch 57 : for modelA 2 ==> Training loss= 0.0677 , Validation loss= 0.0680 , Average loss= 0.0678 , Accuracy = 0.7193\n",
            "Epoch 57 : for modelA 3 ==> Training loss= 0.0675 , Validation loss= 0.0673 , Average loss= 0.0671 , Accuracy = 0.7193\n",
            "Epoch 57 : for modelA 4 ==> Training loss= 0.0674 , Validation loss= 0.0673 , Average loss= 0.0673 , Accuracy = 0.7193\n",
            "Epoch 58 : for modelA 0 ==> Training loss= 0.0683 , Validation loss= 0.0693 , Average loss= 0.0695 , Accuracy = 0.7193\n",
            "Epoch 58 : for modelA 1 ==> Training loss= 0.0676 , Validation loss= 0.0680 , Average loss= 0.0676 , Accuracy = 0.7193\n",
            "Epoch 58 : for modelA 2 ==> Training loss= 0.0676 , Validation loss= 0.0679 , Average loss= 0.0677 , Accuracy = 0.7193\n",
            "Epoch 58 : for modelA 3 ==> Training loss= 0.0673 , Validation loss= 0.0668 , Average loss= 0.0673 , Accuracy = 0.7193\n",
            "Epoch 58 : for modelA 4 ==> Training loss= 0.0675 , Validation loss= 0.0673 , Average loss= 0.0673 , Accuracy = 0.7193\n",
            "Epoch 59 : for modelA 0 ==> Training loss= 0.0683 , Validation loss= 0.0693 , Average loss= 0.0693 , Accuracy = 0.7193\n",
            "Epoch 59 : for modelA 1 ==> Training loss= 0.0676 , Validation loss= 0.0677 , Average loss= 0.0679 , Accuracy = 0.7193\n",
            "Epoch 59 : for modelA 2 ==> Training loss= 0.0676 , Validation loss= 0.0680 , Average loss= 0.0680 , Accuracy = 0.7193\n",
            "Epoch 59 : for modelA 3 ==> Training loss= 0.0673 , Validation loss= 0.0670 , Average loss= 0.0665 , Accuracy = 0.7193\n",
            "Epoch 59 : for modelA 4 ==> Training loss= 0.0675 , Validation loss= 0.0672 , Average loss= 0.0668 , Accuracy = 0.7193\n",
            "Epoch 60 : for modelA 0 ==> Training loss= 0.0682 , Validation loss= 0.0691 , Average loss= 0.0693 , Accuracy = 0.7193\n",
            "Epoch 60 : for modelA 1 ==> Training loss= 0.0674 , Validation loss= 0.0679 , Average loss= 0.0675 , Accuracy = 0.7193\n",
            "Epoch 60 : for modelA 2 ==> Training loss= 0.0675 , Validation loss= 0.0682 , Average loss= 0.0682 , Accuracy = 0.7193\n",
            "Epoch 60 : for modelA 3 ==> Training loss= 0.0673 , Validation loss= 0.0672 , Average loss= 0.0670 , Accuracy = 0.7193\n",
            "Epoch 60 : for modelA 4 ==> Training loss= 0.0674 , Validation loss= 0.0669 , Average loss= 0.0672 , Accuracy = 0.7193\n",
            "Epoch 61 : for modelA 0 ==> Training loss= 0.0681 , Validation loss= 0.0694 , Average loss= 0.0692 , Accuracy = 0.7193\n",
            "Epoch 61 : for modelA 1 ==> Training loss= 0.0675 , Validation loss= 0.0674 , Average loss= 0.0676 , Accuracy = 0.7193\n",
            "Epoch 61 : for modelA 2 ==> Training loss= 0.0676 , Validation loss= 0.0677 , Average loss= 0.0677 , Accuracy = 0.7193\n",
            "Epoch 61 : for modelA 3 ==> Training loss= 0.0674 , Validation loss= 0.0669 , Average loss= 0.0669 , Accuracy = 0.7193\n",
            "Epoch 61 : for modelA 4 ==> Training loss= 0.0674 , Validation loss= 0.0667 , Average loss= 0.0674 , Accuracy = 0.7193\n",
            "Epoch 62 : for modelA 0 ==> Training loss= 0.0681 , Validation loss= 0.0694 , Average loss= 0.0693 , Accuracy = 0.7193\n",
            "Epoch 62 : for modelA 1 ==> Training loss= 0.0674 , Validation loss= 0.0672 , Average loss= 0.0674 , Accuracy = 0.7193\n",
            "Epoch 62 : for modelA 2 ==> Training loss= 0.0675 , Validation loss= 0.0679 , Average loss= 0.0679 , Accuracy = 0.7193\n",
            "Epoch 62 : for modelA 3 ==> Training loss= 0.0673 , Validation loss= 0.0667 , Average loss= 0.0667 , Accuracy = 0.7193\n",
            "Epoch 62 : for modelA 4 ==> Training loss= 0.0673 , Validation loss= 0.0669 , Average loss= 0.0669 , Accuracy = 0.7193\n",
            "Epoch 63 : for modelA 0 ==> Training loss= 0.0682 , Validation loss= 0.0694 , Average loss= 0.0691 , Accuracy = 0.7193\n",
            "Epoch 63 : for modelA 1 ==> Training loss= 0.0674 , Validation loss= 0.0671 , Average loss= 0.0675 , Accuracy = 0.7193\n",
            "Epoch 63 : for modelA 2 ==> Training loss= 0.0675 , Validation loss= 0.0682 , Average loss= 0.0674 , Accuracy = 0.7193\n",
            "Epoch 63 : for modelA 3 ==> Training loss= 0.0672 , Validation loss= 0.0671 , Average loss= 0.0666 , Accuracy = 0.7193\n",
            "Epoch 63 : for modelA 4 ==> Training loss= 0.0673 , Validation loss= 0.0670 , Average loss= 0.0670 , Accuracy = 0.7193\n",
            "Epoch 64 : for modelA 0 ==> Training loss= 0.0680 , Validation loss= 0.0689 , Average loss= 0.0689 , Accuracy = 0.7193\n",
            "Epoch 64 : for modelA 1 ==> Training loss= 0.0676 , Validation loss= 0.0675 , Average loss= 0.0675 , Accuracy = 0.7193\n",
            "Epoch 64 : for modelA 2 ==> Training loss= 0.0676 , Validation loss= 0.0680 , Average loss= 0.0676 , Accuracy = 0.7193\n",
            "Epoch 64 : for modelA 3 ==> Training loss= 0.0673 , Validation loss= 0.0668 , Average loss= 0.0668 , Accuracy = 0.7193\n",
            "Epoch 64 : for modelA 4 ==> Training loss= 0.0673 , Validation loss= 0.0672 , Average loss= 0.0667 , Accuracy = 0.7193\n",
            "Epoch 65 : for modelA 0 ==> Training loss= 0.0680 , Validation loss= 0.0691 , Average loss= 0.0690 , Accuracy = 0.7193\n",
            "Epoch 65 : for modelA 1 ==> Training loss= 0.0675 , Validation loss= 0.0677 , Average loss= 0.0674 , Accuracy = 0.7193\n",
            "Epoch 65 : for modelA 2 ==> Training loss= 0.0674 , Validation loss= 0.0675 , Average loss= 0.0677 , Accuracy = 0.7193\n",
            "Epoch 65 : for modelA 3 ==> Training loss= 0.0673 , Validation loss= 0.0671 , Average loss= 0.0671 , Accuracy = 0.7193\n",
            "Epoch 65 : for modelA 4 ==> Training loss= 0.0672 , Validation loss= 0.0669 , Average loss= 0.0669 , Accuracy = 0.7193\n",
            "Epoch 66 : for modelA 0 ==> Training loss= 0.0680 , Validation loss= 0.0692 , Average loss= 0.0689 , Accuracy = 0.7193\n",
            "Epoch 66 : for modelA 1 ==> Training loss= 0.0674 , Validation loss= 0.0674 , Average loss= 0.0672 , Accuracy = 0.7193\n",
            "Epoch 66 : for modelA 2 ==> Training loss= 0.0674 , Validation loss= 0.0679 , Average loss= 0.0675 , Accuracy = 0.7193\n",
            "Epoch 66 : for modelA 3 ==> Training loss= 0.0673 , Validation loss= 0.0665 , Average loss= 0.0665 , Accuracy = 0.7193\n",
            "Epoch 66 : for modelA 4 ==> Training loss= 0.0671 , Validation loss= 0.0666 , Average loss= 0.0671 , Accuracy = 0.7193\n",
            "Epoch 67 : for modelA 0 ==> Training loss= 0.0680 , Validation loss= 0.0690 , Average loss= 0.0685 , Accuracy = 0.7193\n",
            "Epoch 67 : for modelA 1 ==> Training loss= 0.0674 , Validation loss= 0.0669 , Average loss= 0.0671 , Accuracy = 0.7193\n",
            "Epoch 67 : for modelA 2 ==> Training loss= 0.0675 , Validation loss= 0.0670 , Average loss= 0.0674 , Accuracy = 0.7193\n",
            "Epoch 67 : for modelA 3 ==> Training loss= 0.0673 , Validation loss= 0.0667 , Average loss= 0.0665 , Accuracy = 0.7193\n",
            "Epoch 67 : for modelA 4 ==> Training loss= 0.0674 , Validation loss= 0.0671 , Average loss= 0.0668 , Accuracy = 0.7193\n",
            "Epoch 68 : for modelA 0 ==> Training loss= 0.0681 , Validation loss= 0.0686 , Average loss= 0.0689 , Accuracy = 0.7193\n",
            "Epoch 68 : for modelA 1 ==> Training loss= 0.0673 , Validation loss= 0.0668 , Average loss= 0.0675 , Accuracy = 0.7193\n",
            "Epoch 68 : for modelA 2 ==> Training loss= 0.0676 , Validation loss= 0.0676 , Average loss= 0.0670 , Accuracy = 0.7193\n",
            "Epoch 68 : for modelA 3 ==> Training loss= 0.0673 , Validation loss= 0.0665 , Average loss= 0.0662 , Accuracy = 0.7193\n",
            "Epoch 68 : for modelA 4 ==> Training loss= 0.0674 , Validation loss= 0.0670 , Average loss= 0.0670 , Accuracy = 0.7193\n",
            "Epoch 69 : for modelA 0 ==> Training loss= 0.0679 , Validation loss= 0.0686 , Average loss= 0.0686 , Accuracy = 0.7193\n",
            "Epoch 69 : for modelA 1 ==> Training loss= 0.0673 , Validation loss= 0.0666 , Average loss= 0.0670 , Accuracy = 0.7193\n",
            "Epoch 69 : for modelA 2 ==> Training loss= 0.0673 , Validation loss= 0.0674 , Average loss= 0.0676 , Accuracy = 0.7193\n",
            "Epoch 69 : for modelA 3 ==> Training loss= 0.0673 , Validation loss= 0.0672 , Average loss= 0.0667 , Accuracy = 0.7193\n",
            "Epoch 69 : for modelA 4 ==> Training loss= 0.0672 , Validation loss= 0.0668 , Average loss= 0.0663 , Accuracy = 0.7193\n",
            "Epoch 70 : for modelA 0 ==> Training loss= 0.0679 , Validation loss= 0.0685 , Average loss= 0.0687 , Accuracy = 0.7193\n",
            "Epoch 70 : for modelA 1 ==> Training loss= 0.0673 , Validation loss= 0.0672 , Average loss= 0.0665 , Accuracy = 0.7193\n",
            "Epoch 70 : for modelA 2 ==> Training loss= 0.0674 , Validation loss= 0.0675 , Average loss= 0.0675 , Accuracy = 0.7193\n",
            "Epoch 70 : for modelA 3 ==> Training loss= 0.0671 , Validation loss= 0.0664 , Average loss= 0.0672 , Accuracy = 0.7193\n",
            "Epoch 70 : for modelA 4 ==> Training loss= 0.0672 , Validation loss= 0.0672 , Average loss= 0.0670 , Accuracy = 0.7193\n",
            "Epoch 71 : for modelA 0 ==> Training loss= 0.0679 , Validation loss= 0.0683 , Average loss= 0.0688 , Accuracy = 0.7193\n",
            "Epoch 71 : for modelA 1 ==> Training loss= 0.0672 , Validation loss= 0.0667 , Average loss= 0.0670 , Accuracy = 0.7193\n",
            "Epoch 71 : for modelA 2 ==> Training loss= 0.0674 , Validation loss= 0.0675 , Average loss= 0.0670 , Accuracy = 0.7193\n",
            "Epoch 71 : for modelA 3 ==> Training loss= 0.0671 , Validation loss= 0.0671 , Average loss= 0.0664 , Accuracy = 0.7193\n",
            "Epoch 71 : for modelA 4 ==> Training loss= 0.0672 , Validation loss= 0.0667 , Average loss= 0.0664 , Accuracy = 0.7193\n",
            "Epoch 72 : for modelA 0 ==> Training loss= 0.0679 , Validation loss= 0.0686 , Average loss= 0.0686 , Accuracy = 0.7193\n",
            "Epoch 72 : for modelA 1 ==> Training loss= 0.0672 , Validation loss= 0.0667 , Average loss= 0.0672 , Accuracy = 0.7193\n",
            "Epoch 72 : for modelA 2 ==> Training loss= 0.0674 , Validation loss= 0.0670 , Average loss= 0.0674 , Accuracy = 0.7193\n",
            "Epoch 72 : for modelA 3 ==> Training loss= 0.0671 , Validation loss= 0.0663 , Average loss= 0.0663 , Accuracy = 0.7193\n",
            "Epoch 72 : for modelA 4 ==> Training loss= 0.0674 , Validation loss= 0.0671 , Average loss= 0.0669 , Accuracy = 0.7193\n",
            "Epoch 73 : for modelA 0 ==> Training loss= 0.0678 , Validation loss= 0.0685 , Average loss= 0.0685 , Accuracy = 0.7193\n",
            "Epoch 73 : for modelA 1 ==> Training loss= 0.0673 , Validation loss= 0.0666 , Average loss= 0.0669 , Accuracy = 0.7193\n",
            "Epoch 73 : for modelA 2 ==> Training loss= 0.0674 , Validation loss= 0.0669 , Average loss= 0.0672 , Accuracy = 0.7193\n",
            "Epoch 73 : for modelA 3 ==> Training loss= 0.0672 , Validation loss= 0.0660 , Average loss= 0.0660 , Accuracy = 0.7193\n",
            "Epoch 73 : for modelA 4 ==> Training loss= 0.0672 , Validation loss= 0.0663 , Average loss= 0.0666 , Accuracy = 0.7193\n",
            "Epoch 74 : for modelA 0 ==> Training loss= 0.0680 , Validation loss= 0.0684 , Average loss= 0.0684 , Accuracy = 0.7193\n",
            "Epoch 74 : for modelA 1 ==> Training loss= 0.0675 , Validation loss= 0.0671 , Average loss= 0.0666 , Accuracy = 0.7193\n",
            "Epoch 74 : for modelA 2 ==> Training loss= 0.0673 , Validation loss= 0.0671 , Average loss= 0.0671 , Accuracy = 0.7193\n",
            "Epoch 74 : for modelA 3 ==> Training loss= 0.0671 , Validation loss= 0.0668 , Average loss= 0.0671 , Accuracy = 0.7193\n",
            "Epoch 74 : for modelA 4 ==> Training loss= 0.0672 , Validation loss= 0.0660 , Average loss= 0.0666 , Accuracy = 0.7193\n",
            "Epoch 75 : for modelA 0 ==> Training loss= 0.0678 , Validation loss= 0.0684 , Average loss= 0.0682 , Accuracy = 0.7193\n",
            "Epoch 75 : for modelA 1 ==> Training loss= 0.0672 , Validation loss= 0.0666 , Average loss= 0.0670 , Accuracy = 0.7193\n",
            "Epoch 75 : for modelA 2 ==> Training loss= 0.0674 , Validation loss= 0.0669 , Average loss= 0.0673 , Accuracy = 0.7193\n",
            "Epoch 75 : for modelA 3 ==> Training loss= 0.0671 , Validation loss= 0.0670 , Average loss= 0.0665 , Accuracy = 0.7193\n",
            "Epoch 75 : for modelA 4 ==> Training loss= 0.0672 , Validation loss= 0.0663 , Average loss= 0.0665 , Accuracy = 0.7193\n",
            "Epoch 76 : for modelA 0 ==> Training loss= 0.0679 , Validation loss= 0.0683 , Average loss= 0.0680 , Accuracy = 0.7193\n",
            "Epoch 76 : for modelA 1 ==> Training loss= 0.0672 , Validation loss= 0.0665 , Average loss= 0.0667 , Accuracy = 0.7193\n",
            "Epoch 76 : for modelA 2 ==> Training loss= 0.0673 , Validation loss= 0.0668 , Average loss= 0.0668 , Accuracy = 0.7193\n",
            "Epoch 76 : for modelA 3 ==> Training loss= 0.0670 , Validation loss= 0.0667 , Average loss= 0.0665 , Accuracy = 0.7193\n",
            "Epoch 76 : for modelA 4 ==> Training loss= 0.0671 , Validation loss= 0.0667 , Average loss= 0.0662 , Accuracy = 0.7193\n",
            "Epoch 77 : for modelA 0 ==> Training loss= 0.0678 , Validation loss= 0.0686 , Average loss= 0.0681 , Accuracy = 0.7193\n",
            "Epoch 77 : for modelA 1 ==> Training loss= 0.0672 , Validation loss= 0.0662 , Average loss= 0.0670 , Accuracy = 0.7193\n",
            "Epoch 77 : for modelA 2 ==> Training loss= 0.0673 , Validation loss= 0.0665 , Average loss= 0.0675 , Accuracy = 0.7193\n",
            "Epoch 77 : for modelA 3 ==> Training loss= 0.0671 , Validation loss= 0.0667 , Average loss= 0.0659 , Accuracy = 0.7193\n",
            "Epoch 77 : for modelA 4 ==> Training loss= 0.0671 , Validation loss= 0.0662 , Average loss= 0.0670 , Accuracy = 0.7193\n",
            "Epoch 78 : for modelA 0 ==> Training loss= 0.0677 , Validation loss= 0.0681 , Average loss= 0.0684 , Accuracy = 0.7193\n",
            "Epoch 78 : for modelA 1 ==> Training loss= 0.0673 , Validation loss= 0.0672 , Average loss= 0.0664 , Accuracy = 0.7193\n",
            "Epoch 78 : for modelA 2 ==> Training loss= 0.0672 , Validation loss= 0.0672 , Average loss= 0.0667 , Accuracy = 0.7193\n",
            "Epoch 78 : for modelA 3 ==> Training loss= 0.0672 , Validation loss= 0.0667 , Average loss= 0.0667 , Accuracy = 0.7193\n",
            "Epoch 78 : for modelA 4 ==> Training loss= 0.0673 , Validation loss= 0.0667 , Average loss= 0.0667 , Accuracy = 0.7193\n",
            "Epoch 79 : for modelA 0 ==> Training loss= 0.0678 , Validation loss= 0.0686 , Average loss= 0.0680 , Accuracy = 0.7193\n",
            "Epoch 79 : for modelA 1 ==> Training loss= 0.0671 , Validation loss= 0.0666 , Average loss= 0.0666 , Accuracy = 0.7193\n",
            "Epoch 79 : for modelA 2 ==> Training loss= 0.0673 , Validation loss= 0.0669 , Average loss= 0.0667 , Accuracy = 0.7193\n",
            "Epoch 79 : for modelA 3 ==> Training loss= 0.0671 , Validation loss= 0.0672 , Average loss= 0.0661 , Accuracy = 0.7193\n",
            "Epoch 79 : for modelA 4 ==> Training loss= 0.0671 , Validation loss= 0.0672 , Average loss= 0.0664 , Accuracy = 0.7193\n",
            "Epoch 80 : for modelA 0 ==> Training loss= 0.0678 , Validation loss= 0.0680 , Average loss= 0.0683 , Accuracy = 0.7193\n",
            "Epoch 80 : for modelA 1 ==> Training loss= 0.0670 , Validation loss= 0.0671 , Average loss= 0.0666 , Accuracy = 0.7193\n",
            "Epoch 80 : for modelA 2 ==> Training loss= 0.0672 , Validation loss= 0.0669 , Average loss= 0.0666 , Accuracy = 0.7193\n",
            "Epoch 80 : for modelA 3 ==> Training loss= 0.0670 , Validation loss= 0.0661 , Average loss= 0.0666 , Accuracy = 0.7193\n",
            "Epoch 80 : for modelA 4 ==> Training loss= 0.0672 , Validation loss= 0.0663 , Average loss= 0.0663 , Accuracy = 0.7193\n",
            "Epoch 81 : for modelA 0 ==> Training loss= 0.0677 , Validation loss= 0.0683 , Average loss= 0.0681 , Accuracy = 0.7193\n",
            "Epoch 81 : for modelA 1 ==> Training loss= 0.0672 , Validation loss= 0.0666 , Average loss= 0.0668 , Accuracy = 0.7193\n",
            "Epoch 81 : for modelA 2 ==> Training loss= 0.0673 , Validation loss= 0.0673 , Average loss= 0.0671 , Accuracy = 0.7193\n",
            "Epoch 81 : for modelA 3 ==> Training loss= 0.0670 , Validation loss= 0.0661 , Average loss= 0.0661 , Accuracy = 0.7193\n",
            "Epoch 81 : for modelA 4 ==> Training loss= 0.0673 , Validation loss= 0.0663 , Average loss= 0.0668 , Accuracy = 0.7193\n",
            "Epoch 82 : for modelA 0 ==> Training loss= 0.0676 , Validation loss= 0.0684 , Average loss= 0.0681 , Accuracy = 0.7193\n",
            "Epoch 82 : for modelA 1 ==> Training loss= 0.0672 , Validation loss= 0.0665 , Average loss= 0.0668 , Accuracy = 0.7193\n",
            "Epoch 82 : for modelA 2 ==> Training loss= 0.0671 , Validation loss= 0.0668 , Average loss= 0.0668 , Accuracy = 0.7193\n",
            "Epoch 82 : for modelA 3 ==> Training loss= 0.0671 , Validation loss= 0.0663 , Average loss= 0.0663 , Accuracy = 0.7193\n",
            "Epoch 82 : for modelA 4 ==> Training loss= 0.0672 , Validation loss= 0.0663 , Average loss= 0.0668 , Accuracy = 0.7193\n",
            "Epoch 83 : for modelA 0 ==> Training loss= 0.0677 , Validation loss= 0.0678 , Average loss= 0.0678 , Accuracy = 0.7193\n",
            "Epoch 83 : for modelA 1 ==> Training loss= 0.0672 , Validation loss= 0.0660 , Average loss= 0.0665 , Accuracy = 0.7193\n",
            "Epoch 83 : for modelA 2 ==> Training loss= 0.0672 , Validation loss= 0.0670 , Average loss= 0.0663 , Accuracy = 0.7193\n",
            "Epoch 83 : for modelA 3 ==> Training loss= 0.0670 , Validation loss= 0.0663 , Average loss= 0.0657 , Accuracy = 0.7193\n",
            "Epoch 83 : for modelA 4 ==> Training loss= 0.0673 , Validation loss= 0.0665 , Average loss= 0.0657 , Accuracy = 0.7193\n",
            "Epoch 84 : for modelA 0 ==> Training loss= 0.0676 , Validation loss= 0.0682 , Average loss= 0.0680 , Accuracy = 0.7193\n",
            "Epoch 84 : for modelA 1 ==> Training loss= 0.0672 , Validation loss= 0.0662 , Average loss= 0.0659 , Accuracy = 0.7193\n",
            "Epoch 84 : for modelA 2 ==> Training loss= 0.0672 , Validation loss= 0.0670 , Average loss= 0.0667 , Accuracy = 0.7193\n",
            "Epoch 84 : for modelA 3 ==> Training loss= 0.0670 , Validation loss= 0.0663 , Average loss= 0.0665 , Accuracy = 0.7193\n",
            "Epoch 84 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0671 , Average loss= 0.0662 , Accuracy = 0.7193\n",
            "Epoch 85 : for modelA 0 ==> Training loss= 0.0676 , Validation loss= 0.0681 , Average loss= 0.0677 , Accuracy = 0.7193\n",
            "Epoch 85 : for modelA 1 ==> Training loss= 0.0670 , Validation loss= 0.0664 , Average loss= 0.0664 , Accuracy = 0.7193\n",
            "Epoch 85 : for modelA 2 ==> Training loss= 0.0671 , Validation loss= 0.0667 , Average loss= 0.0672 , Accuracy = 0.7193\n",
            "Epoch 85 : for modelA 3 ==> Training loss= 0.0670 , Validation loss= 0.0665 , Average loss= 0.0662 , Accuracy = 0.7193\n",
            "Epoch 85 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0659 , Average loss= 0.0665 , Accuracy = 0.7193\n",
            "Epoch 86 : for modelA 0 ==> Training loss= 0.0676 , Validation loss= 0.0679 , Average loss= 0.0675 , Accuracy = 0.7193\n",
            "Epoch 86 : for modelA 1 ==> Training loss= 0.0672 , Validation loss= 0.0664 , Average loss= 0.0661 , Accuracy = 0.7193\n",
            "Epoch 86 : for modelA 2 ==> Training loss= 0.0672 , Validation loss= 0.0669 , Average loss= 0.0664 , Accuracy = 0.7193\n",
            "Epoch 86 : for modelA 3 ==> Training loss= 0.0670 , Validation loss= 0.0665 , Average loss= 0.0665 , Accuracy = 0.7193\n",
            "Epoch 86 : for modelA 4 ==> Training loss= 0.0671 , Validation loss= 0.0656 , Average loss= 0.0659 , Accuracy = 0.7193\n",
            "Epoch 87 : for modelA 0 ==> Training loss= 0.0676 , Validation loss= 0.0680 , Average loss= 0.0674 , Accuracy = 0.7193\n",
            "Epoch 87 : for modelA 1 ==> Training loss= 0.0670 , Validation loss= 0.0661 , Average loss= 0.0664 , Accuracy = 0.7193\n",
            "Epoch 87 : for modelA 2 ==> Training loss= 0.0672 , Validation loss= 0.0664 , Average loss= 0.0666 , Accuracy = 0.7193\n",
            "Epoch 87 : for modelA 3 ==> Training loss= 0.0671 , Validation loss= 0.0665 , Average loss= 0.0656 , Accuracy = 0.7193\n",
            "Epoch 87 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0661 , Average loss= 0.0658 , Accuracy = 0.7193\n",
            "Epoch 88 : for modelA 0 ==> Training loss= 0.0675 , Validation loss= 0.0678 , Average loss= 0.0678 , Accuracy = 0.7193\n",
            "Epoch 88 : for modelA 1 ==> Training loss= 0.0670 , Validation loss= 0.0663 , Average loss= 0.0661 , Accuracy = 0.7193\n",
            "Epoch 88 : for modelA 2 ==> Training loss= 0.0672 , Validation loss= 0.0666 , Average loss= 0.0666 , Accuracy = 0.7193\n",
            "Epoch 88 : for modelA 3 ==> Training loss= 0.0671 , Validation loss= 0.0664 , Average loss= 0.0664 , Accuracy = 0.7193\n",
            "Epoch 88 : for modelA 4 ==> Training loss= 0.0671 , Validation loss= 0.0655 , Average loss= 0.0664 , Accuracy = 0.7193\n",
            "Epoch 89 : for modelA 0 ==> Training loss= 0.0676 , Validation loss= 0.0677 , Average loss= 0.0677 , Accuracy = 0.7193\n",
            "Epoch 89 : for modelA 1 ==> Training loss= 0.0672 , Validation loss= 0.0663 , Average loss= 0.0660 , Accuracy = 0.7193\n",
            "Epoch 89 : for modelA 2 ==> Training loss= 0.0673 , Validation loss= 0.0663 , Average loss= 0.0666 , Accuracy = 0.7193\n",
            "Epoch 89 : for modelA 3 ==> Training loss= 0.0672 , Validation loss= 0.0661 , Average loss= 0.0661 , Accuracy = 0.7193\n",
            "Epoch 89 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0655 , Average loss= 0.0664 , Accuracy = 0.7193\n",
            "Epoch 90 : for modelA 0 ==> Training loss= 0.0676 , Validation loss= 0.0675 , Average loss= 0.0677 , Accuracy = 0.7193\n",
            "Epoch 90 : for modelA 1 ==> Training loss= 0.0672 , Validation loss= 0.0657 , Average loss= 0.0657 , Accuracy = 0.7193\n",
            "Epoch 90 : for modelA 2 ==> Training loss= 0.0672 , Validation loss= 0.0668 , Average loss= 0.0668 , Accuracy = 0.7193\n",
            "Epoch 90 : for modelA 3 ==> Training loss= 0.0671 , Validation loss= 0.0661 , Average loss= 0.0658 , Accuracy = 0.7193\n",
            "Epoch 90 : for modelA 4 ==> Training loss= 0.0671 , Validation loss= 0.0654 , Average loss= 0.0657 , Accuracy = 0.7193\n",
            "Epoch 91 : for modelA 0 ==> Training loss= 0.0674 , Validation loss= 0.0674 , Average loss= 0.0674 , Accuracy = 0.7193\n",
            "Epoch 91 : for modelA 1 ==> Training loss= 0.0670 , Validation loss= 0.0660 , Average loss= 0.0662 , Accuracy = 0.7193\n",
            "Epoch 91 : for modelA 2 ==> Training loss= 0.0671 , Validation loss= 0.0665 , Average loss= 0.0662 , Accuracy = 0.7193\n",
            "Epoch 91 : for modelA 3 ==> Training loss= 0.0670 , Validation loss= 0.0658 , Average loss= 0.0667 , Accuracy = 0.7193\n",
            "Epoch 91 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0654 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 92 : for modelA 0 ==> Training loss= 0.0676 , Validation loss= 0.0674 , Average loss= 0.0678 , Accuracy = 0.7193\n",
            "Epoch 92 : for modelA 1 ==> Training loss= 0.0670 , Validation loss= 0.0662 , Average loss= 0.0659 , Accuracy = 0.7193\n",
            "Epoch 92 : for modelA 2 ==> Training loss= 0.0671 , Validation loss= 0.0659 , Average loss= 0.0662 , Accuracy = 0.7193\n",
            "Epoch 92 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0664 , Average loss= 0.0655 , Accuracy = 0.7193\n",
            "Epoch 92 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0660 , Average loss= 0.0657 , Accuracy = 0.7193\n",
            "Epoch 93 : for modelA 0 ==> Training loss= 0.0675 , Validation loss= 0.0674 , Average loss= 0.0678 , Accuracy = 0.7193\n",
            "Epoch 93 : for modelA 1 ==> Training loss= 0.0672 , Validation loss= 0.0659 , Average loss= 0.0662 , Accuracy = 0.7193\n",
            "Epoch 93 : for modelA 2 ==> Training loss= 0.0671 , Validation loss= 0.0662 , Average loss= 0.0664 , Accuracy = 0.7193\n",
            "Epoch 93 : for modelA 3 ==> Training loss= 0.0670 , Validation loss= 0.0658 , Average loss= 0.0655 , Accuracy = 0.7193\n",
            "Epoch 93 : for modelA 4 ==> Training loss= 0.0671 , Validation loss= 0.0659 , Average loss= 0.0657 , Accuracy = 0.7193\n",
            "Epoch 94 : for modelA 0 ==> Training loss= 0.0674 , Validation loss= 0.0675 , Average loss= 0.0673 , Accuracy = 0.7193\n",
            "Epoch 94 : for modelA 1 ==> Training loss= 0.0670 , Validation loss= 0.0659 , Average loss= 0.0664 , Accuracy = 0.7193\n",
            "Epoch 94 : for modelA 2 ==> Training loss= 0.0672 , Validation loss= 0.0667 , Average loss= 0.0667 , Accuracy = 0.7193\n",
            "Epoch 94 : for modelA 3 ==> Training loss= 0.0672 , Validation loss= 0.0660 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 94 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0656 , Average loss= 0.0659 , Accuracy = 0.7193\n",
            "Epoch 95 : for modelA 0 ==> Training loss= 0.0674 , Validation loss= 0.0673 , Average loss= 0.0673 , Accuracy = 0.7193\n",
            "Epoch 95 : for modelA 1 ==> Training loss= 0.0671 , Validation loss= 0.0664 , Average loss= 0.0656 , Accuracy = 0.7193\n",
            "Epoch 95 : for modelA 2 ==> Training loss= 0.0670 , Validation loss= 0.0667 , Average loss= 0.0664 , Accuracy = 0.7193\n",
            "Epoch 95 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0660 , Average loss= 0.0663 , Accuracy = 0.7193\n",
            "Epoch 95 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0659 , Average loss= 0.0656 , Accuracy = 0.7193\n",
            "Epoch 96 : for modelA 0 ==> Training loss= 0.0675 , Validation loss= 0.0677 , Average loss= 0.0674 , Accuracy = 0.7193\n",
            "Epoch 96 : for modelA 1 ==> Training loss= 0.0671 , Validation loss= 0.0661 , Average loss= 0.0655 , Accuracy = 0.7193\n",
            "Epoch 96 : for modelA 2 ==> Training loss= 0.0671 , Validation loss= 0.0666 , Average loss= 0.0666 , Accuracy = 0.7193\n",
            "Epoch 96 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0657 , Average loss= 0.0663 , Accuracy = 0.7193\n",
            "Epoch 96 : for modelA 4 ==> Training loss= 0.0671 , Validation loss= 0.0662 , Average loss= 0.0656 , Accuracy = 0.7193\n",
            "Epoch 97 : for modelA 0 ==> Training loss= 0.0675 , Validation loss= 0.0674 , Average loss= 0.0672 , Accuracy = 0.7193\n",
            "Epoch 97 : for modelA 1 ==> Training loss= 0.0671 , Validation loss= 0.0658 , Average loss= 0.0661 , Accuracy = 0.7193\n",
            "Epoch 97 : for modelA 2 ==> Training loss= 0.0670 , Validation loss= 0.0661 , Average loss= 0.0669 , Accuracy = 0.7193\n",
            "Epoch 97 : for modelA 3 ==> Training loss= 0.0672 , Validation loss= 0.0660 , Average loss= 0.0660 , Accuracy = 0.7193\n",
            "Epoch 97 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0661 , Average loss= 0.0661 , Accuracy = 0.7193\n",
            "Epoch 98 : for modelA 0 ==> Training loss= 0.0674 , Validation loss= 0.0676 , Average loss= 0.0669 , Accuracy = 0.7193\n",
            "Epoch 98 : for modelA 1 ==> Training loss= 0.0670 , Validation loss= 0.0658 , Average loss= 0.0661 , Accuracy = 0.7193\n",
            "Epoch 98 : for modelA 2 ==> Training loss= 0.0672 , Validation loss= 0.0663 , Average loss= 0.0657 , Accuracy = 0.7193\n",
            "Epoch 98 : for modelA 3 ==> Training loss= 0.0670 , Validation loss= 0.0662 , Average loss= 0.0659 , Accuracy = 0.7193\n",
            "Epoch 98 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0661 , Average loss= 0.0658 , Accuracy = 0.7193\n",
            "Epoch 99 : for modelA 0 ==> Training loss= 0.0674 , Validation loss= 0.0680 , Average loss= 0.0676 , Accuracy = 0.7193\n",
            "Epoch 99 : for modelA 1 ==> Training loss= 0.0670 , Validation loss= 0.0660 , Average loss= 0.0663 , Accuracy = 0.7193\n",
            "Epoch 99 : for modelA 2 ==> Training loss= 0.0671 , Validation loss= 0.0663 , Average loss= 0.0665 , Accuracy = 0.7193\n",
            "Epoch 99 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0653 , Average loss= 0.0659 , Accuracy = 0.7193\n",
            "Epoch 99 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0658 , Average loss= 0.0655 , Accuracy = 0.7193\n",
            "Epoch 100 : for modelA 0 ==> Training loss= 0.0675 , Validation loss= 0.0668 , Average loss= 0.0671 , Accuracy = 0.7193\n",
            "Epoch 100 : for modelA 1 ==> Training loss= 0.0672 , Validation loss= 0.0657 , Average loss= 0.0663 , Accuracy = 0.7193\n",
            "Epoch 100 : for modelA 2 ==> Training loss= 0.0670 , Validation loss= 0.0662 , Average loss= 0.0660 , Accuracy = 0.7193\n",
            "Epoch 100 : for modelA 3 ==> Training loss= 0.0671 , Validation loss= 0.0662 , Average loss= 0.0662 , Accuracy = 0.7193\n",
            "Epoch 100 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0658 , Average loss= 0.0661 , Accuracy = 0.7193\n",
            "Epoch 101 : for modelA 0 ==> Training loss= 0.0674 , Validation loss= 0.0673 , Average loss= 0.0675 , Accuracy = 0.7193\n",
            "Epoch 101 : for modelA 1 ==> Training loss= 0.0670 , Validation loss= 0.0660 , Average loss= 0.0660 , Accuracy = 0.7193\n",
            "Epoch 101 : for modelA 2 ==> Training loss= 0.0670 , Validation loss= 0.0665 , Average loss= 0.0659 , Accuracy = 0.7193\n",
            "Epoch 101 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0659 , Average loss= 0.0659 , Accuracy = 0.7193\n",
            "Epoch 101 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0658 , Average loss= 0.0651 , Accuracy = 0.7193\n",
            "Epoch 102 : for modelA 0 ==> Training loss= 0.0675 , Validation loss= 0.0670 , Average loss= 0.0668 , Accuracy = 0.7193\n",
            "Epoch 102 : for modelA 1 ==> Training loss= 0.0670 , Validation loss= 0.0657 , Average loss= 0.0657 , Accuracy = 0.7193\n",
            "Epoch 102 : for modelA 2 ==> Training loss= 0.0672 , Validation loss= 0.0665 , Average loss= 0.0667 , Accuracy = 0.7193\n",
            "Epoch 102 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0659 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 102 : for modelA 4 ==> Training loss= 0.0671 , Validation loss= 0.0651 , Average loss= 0.0651 , Accuracy = 0.7193\n",
            "Epoch 103 : for modelA 0 ==> Training loss= 0.0675 , Validation loss= 0.0672 , Average loss= 0.0674 , Accuracy = 0.7193\n",
            "Epoch 103 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0665 , Average loss= 0.0662 , Accuracy = 0.7193\n",
            "Epoch 103 : for modelA 2 ==> Training loss= 0.0672 , Validation loss= 0.0659 , Average loss= 0.0667 , Accuracy = 0.7193\n",
            "Epoch 103 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0658 , Average loss= 0.0661 , Accuracy = 0.7193\n",
            "Epoch 103 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0657 , Average loss= 0.0657 , Accuracy = 0.7193\n",
            "Epoch 104 : for modelA 0 ==> Training loss= 0.0673 , Validation loss= 0.0672 , Average loss= 0.0674 , Accuracy = 0.7193\n",
            "Epoch 104 : for modelA 1 ==> Training loss= 0.0670 , Validation loss= 0.0662 , Average loss= 0.0662 , Accuracy = 0.7193\n",
            "Epoch 104 : for modelA 2 ==> Training loss= 0.0670 , Validation loss= 0.0661 , Average loss= 0.0658 , Accuracy = 0.7193\n",
            "Epoch 104 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0661 , Average loss= 0.0652 , Accuracy = 0.7193\n",
            "Epoch 104 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0663 , Average loss= 0.0651 , Accuracy = 0.7193\n",
            "Epoch 105 : for modelA 0 ==> Training loss= 0.0675 , Validation loss= 0.0671 , Average loss= 0.0671 , Accuracy = 0.7193\n",
            "Epoch 105 : for modelA 1 ==> Training loss= 0.0671 , Validation loss= 0.0662 , Average loss= 0.0656 , Accuracy = 0.7193\n",
            "Epoch 105 : for modelA 2 ==> Training loss= 0.0671 , Validation loss= 0.0655 , Average loss= 0.0664 , Accuracy = 0.7193\n",
            "Epoch 105 : for modelA 3 ==> Training loss= 0.0671 , Validation loss= 0.0658 , Average loss= 0.0658 , Accuracy = 0.7193\n",
            "Epoch 105 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0660 , Average loss= 0.0657 , Accuracy = 0.7193\n",
            "Epoch 106 : for modelA 0 ==> Training loss= 0.0674 , Validation loss= 0.0673 , Average loss= 0.0669 , Accuracy = 0.7193\n",
            "Epoch 106 : for modelA 1 ==> Training loss= 0.0670 , Validation loss= 0.0662 , Average loss= 0.0656 , Accuracy = 0.7193\n",
            "Epoch 106 : for modelA 2 ==> Training loss= 0.0670 , Validation loss= 0.0658 , Average loss= 0.0658 , Accuracy = 0.7193\n",
            "Epoch 106 : for modelA 3 ==> Training loss= 0.0670 , Validation loss= 0.0655 , Average loss= 0.0655 , Accuracy = 0.7193\n",
            "Epoch 106 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0657 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 107 : for modelA 0 ==> Training loss= 0.0673 , Validation loss= 0.0668 , Average loss= 0.0671 , Accuracy = 0.7193\n",
            "Epoch 107 : for modelA 1 ==> Training loss= 0.0670 , Validation loss= 0.0659 , Average loss= 0.0668 , Accuracy = 0.7193\n",
            "Epoch 107 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0655 , Average loss= 0.0658 , Accuracy = 0.7193\n",
            "Epoch 107 : for modelA 3 ==> Training loss= 0.0670 , Validation loss= 0.0655 , Average loss= 0.0655 , Accuracy = 0.7193\n",
            "Epoch 107 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0656 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 108 : for modelA 0 ==> Training loss= 0.0673 , Validation loss= 0.0673 , Average loss= 0.0673 , Accuracy = 0.7193\n",
            "Epoch 108 : for modelA 1 ==> Training loss= 0.0670 , Validation loss= 0.0658 , Average loss= 0.0655 , Accuracy = 0.7193\n",
            "Epoch 108 : for modelA 2 ==> Training loss= 0.0670 , Validation loss= 0.0657 , Average loss= 0.0655 , Accuracy = 0.7193\n",
            "Epoch 108 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0655 , Average loss= 0.0658 , Accuracy = 0.7193\n",
            "Epoch 108 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0659 , Average loss= 0.0656 , Accuracy = 0.7193\n",
            "Epoch 109 : for modelA 0 ==> Training loss= 0.0674 , Validation loss= 0.0667 , Average loss= 0.0667 , Accuracy = 0.7193\n",
            "Epoch 109 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0661 , Average loss= 0.0658 , Accuracy = 0.7193\n",
            "Epoch 109 : for modelA 2 ==> Training loss= 0.0672 , Validation loss= 0.0660 , Average loss= 0.0663 , Accuracy = 0.7193\n",
            "Epoch 109 : for modelA 3 ==> Training loss= 0.0670 , Validation loss= 0.0664 , Average loss= 0.0657 , Accuracy = 0.7193\n",
            "Epoch 109 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0656 , Average loss= 0.0659 , Accuracy = 0.7193\n",
            "Epoch 110 : for modelA 0 ==> Training loss= 0.0673 , Validation loss= 0.0670 , Average loss= 0.0672 , Accuracy = 0.7193\n",
            "Epoch 110 : for modelA 1 ==> Training loss= 0.0670 , Validation loss= 0.0661 , Average loss= 0.0655 , Accuracy = 0.7193\n",
            "Epoch 110 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0663 , Average loss= 0.0666 , Accuracy = 0.7193\n",
            "Epoch 110 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0660 , Average loss= 0.0663 , Accuracy = 0.7193\n",
            "Epoch 110 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0656 , Average loss= 0.0659 , Accuracy = 0.7193\n",
            "Epoch 111 : for modelA 0 ==> Training loss= 0.0672 , Validation loss= 0.0667 , Average loss= 0.0669 , Accuracy = 0.7193\n",
            "Epoch 111 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0655 , Average loss= 0.0661 , Accuracy = 0.7193\n",
            "Epoch 111 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0663 , Average loss= 0.0660 , Accuracy = 0.7193\n",
            "Epoch 111 : for modelA 3 ==> Training loss= 0.0671 , Validation loss= 0.0654 , Average loss= 0.0657 , Accuracy = 0.7193\n",
            "Epoch 111 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0656 , Average loss= 0.0652 , Accuracy = 0.7193\n",
            "Epoch 112 : for modelA 0 ==> Training loss= 0.0672 , Validation loss= 0.0666 , Average loss= 0.0671 , Accuracy = 0.7193\n",
            "Epoch 112 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0661 , Average loss= 0.0661 , Accuracy = 0.7193\n",
            "Epoch 112 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0668 , Average loss= 0.0659 , Accuracy = 0.7193\n",
            "Epoch 112 : for modelA 3 ==> Training loss= 0.0670 , Validation loss= 0.0657 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 112 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0652 , Average loss= 0.0659 , Accuracy = 0.7193\n",
            "Epoch 113 : for modelA 0 ==> Training loss= 0.0673 , Validation loss= 0.0669 , Average loss= 0.0674 , Accuracy = 0.7193\n",
            "Epoch 113 : for modelA 1 ==> Training loss= 0.0670 , Validation loss= 0.0660 , Average loss= 0.0657 , Accuracy = 0.7193\n",
            "Epoch 113 : for modelA 2 ==> Training loss= 0.0670 , Validation loss= 0.0659 , Average loss= 0.0656 , Accuracy = 0.7193\n",
            "Epoch 113 : for modelA 3 ==> Training loss= 0.0670 , Validation loss= 0.0660 , Average loss= 0.0660 , Accuracy = 0.7193\n",
            "Epoch 113 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0655 , Average loss= 0.0652 , Accuracy = 0.7193\n",
            "Epoch 114 : for modelA 0 ==> Training loss= 0.0674 , Validation loss= 0.0663 , Average loss= 0.0666 , Accuracy = 0.7193\n",
            "Epoch 114 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0654 , Average loss= 0.0663 , Accuracy = 0.7193\n",
            "Epoch 114 : for modelA 2 ==> Training loss= 0.0670 , Validation loss= 0.0662 , Average loss= 0.0659 , Accuracy = 0.7193\n",
            "Epoch 114 : for modelA 3 ==> Training loss= 0.0670 , Validation loss= 0.0657 , Average loss= 0.0657 , Accuracy = 0.7193\n",
            "Epoch 114 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0652 , Average loss= 0.0655 , Accuracy = 0.7193\n",
            "Epoch 115 : for modelA 0 ==> Training loss= 0.0673 , Validation loss= 0.0666 , Average loss= 0.0670 , Accuracy = 0.7193\n",
            "Epoch 115 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0654 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 115 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0659 , Average loss= 0.0656 , Accuracy = 0.7193\n",
            "Epoch 115 : for modelA 3 ==> Training loss= 0.0671 , Validation loss= 0.0650 , Average loss= 0.0663 , Accuracy = 0.7193\n",
            "Epoch 115 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0655 , Average loss= 0.0655 , Accuracy = 0.7193\n",
            "Epoch 116 : for modelA 0 ==> Training loss= 0.0672 , Validation loss= 0.0668 , Average loss= 0.0668 , Accuracy = 0.7193\n",
            "Epoch 116 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0660 , Average loss= 0.0660 , Accuracy = 0.7193\n",
            "Epoch 116 : for modelA 2 ==> Training loss= 0.0671 , Validation loss= 0.0652 , Average loss= 0.0661 , Accuracy = 0.7193\n",
            "Epoch 116 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0656 , Average loss= 0.0656 , Accuracy = 0.7193\n",
            "Epoch 116 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0648 , Average loss= 0.0651 , Accuracy = 0.7193\n",
            "Epoch 117 : for modelA 0 ==> Training loss= 0.0673 , Validation loss= 0.0672 , Average loss= 0.0667 , Accuracy = 0.7193\n",
            "Epoch 117 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0657 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 117 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0655 , Average loss= 0.0652 , Accuracy = 0.7193\n",
            "Epoch 117 : for modelA 3 ==> Training loss= 0.0670 , Validation loss= 0.0656 , Average loss= 0.0656 , Accuracy = 0.7193\n",
            "Epoch 117 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0658 , Average loss= 0.0655 , Accuracy = 0.7193\n",
            "Epoch 118 : for modelA 0 ==> Training loss= 0.0672 , Validation loss= 0.0672 , Average loss= 0.0667 , Accuracy = 0.7193\n",
            "Epoch 118 : for modelA 1 ==> Training loss= 0.0670 , Validation loss= 0.0656 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 118 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0658 , Average loss= 0.0655 , Accuracy = 0.7193\n",
            "Epoch 118 : for modelA 3 ==> Training loss= 0.0670 , Validation loss= 0.0653 , Average loss= 0.0659 , Accuracy = 0.7193\n",
            "Epoch 118 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0651 , Average loss= 0.0661 , Accuracy = 0.7193\n",
            "Epoch 119 : for modelA 0 ==> Training loss= 0.0672 , Validation loss= 0.0664 , Average loss= 0.0672 , Accuracy = 0.7193\n",
            "Epoch 119 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0659 , Average loss= 0.0659 , Accuracy = 0.7193\n",
            "Epoch 119 : for modelA 2 ==> Training loss= 0.0670 , Validation loss= 0.0652 , Average loss= 0.0661 , Accuracy = 0.7193\n",
            "Epoch 119 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0656 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 119 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0648 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 120 : for modelA 0 ==> Training loss= 0.0672 , Validation loss= 0.0664 , Average loss= 0.0667 , Accuracy = 0.7193\n",
            "Epoch 120 : for modelA 1 ==> Training loss= 0.0671 , Validation loss= 0.0659 , Average loss= 0.0656 , Accuracy = 0.7193\n",
            "Epoch 120 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0661 , Average loss= 0.0661 , Accuracy = 0.7193\n",
            "Epoch 120 : for modelA 3 ==> Training loss= 0.0671 , Validation loss= 0.0653 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 120 : for modelA 4 ==> Training loss= 0.0671 , Validation loss= 0.0647 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 121 : for modelA 0 ==> Training loss= 0.0670 , Validation loss= 0.0664 , Average loss= 0.0671 , Accuracy = 0.7193\n",
            "Epoch 121 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0656 , Average loss= 0.0656 , Accuracy = 0.7193\n",
            "Epoch 121 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0651 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 121 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0662 , Average loss= 0.0656 , Accuracy = 0.7193\n",
            "Epoch 121 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0651 , Average loss= 0.0657 , Accuracy = 0.7193\n",
            "Epoch 122 : for modelA 0 ==> Training loss= 0.0671 , Validation loss= 0.0661 , Average loss= 0.0666 , Accuracy = 0.7193\n",
            "Epoch 122 : for modelA 1 ==> Training loss= 0.0671 , Validation loss= 0.0659 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 122 : for modelA 2 ==> Training loss= 0.0670 , Validation loss= 0.0657 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 122 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0656 , Average loss= 0.0659 , Accuracy = 0.7193\n",
            "Epoch 122 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0654 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 123 : for modelA 0 ==> Training loss= 0.0673 , Validation loss= 0.0663 , Average loss= 0.0666 , Accuracy = 0.7193\n",
            "Epoch 123 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0652 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 123 : for modelA 2 ==> Training loss= 0.0670 , Validation loss= 0.0657 , Average loss= 0.0660 , Accuracy = 0.7193\n",
            "Epoch 123 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0659 , Average loss= 0.0655 , Accuracy = 0.7193\n",
            "Epoch 123 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0657 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 124 : for modelA 0 ==> Training loss= 0.0670 , Validation loss= 0.0663 , Average loss= 0.0663 , Accuracy = 0.7193\n",
            "Epoch 124 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0662 , Average loss= 0.0659 , Accuracy = 0.7193\n",
            "Epoch 124 : for modelA 2 ==> Training loss= 0.0670 , Validation loss= 0.0654 , Average loss= 0.0663 , Accuracy = 0.7193\n",
            "Epoch 124 : for modelA 3 ==> Training loss= 0.0670 , Validation loss= 0.0652 , Average loss= 0.0658 , Accuracy = 0.7193\n",
            "Epoch 124 : for modelA 4 ==> Training loss= 0.0671 , Validation loss= 0.0660 , Average loss= 0.0657 , Accuracy = 0.7193\n",
            "Epoch 125 : for modelA 0 ==> Training loss= 0.0672 , Validation loss= 0.0665 , Average loss= 0.0665 , Accuracy = 0.7193\n",
            "Epoch 125 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0655 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 125 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0660 , Average loss= 0.0663 , Accuracy = 0.7193\n",
            "Epoch 125 : for modelA 3 ==> Training loss= 0.0670 , Validation loss= 0.0658 , Average loss= 0.0655 , Accuracy = 0.7193\n",
            "Epoch 125 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0653 , Average loss= 0.0657 , Accuracy = 0.7193\n",
            "Epoch 126 : for modelA 0 ==> Training loss= 0.0673 , Validation loss= 0.0665 , Average loss= 0.0667 , Accuracy = 0.7193\n",
            "Epoch 126 : for modelA 1 ==> Training loss= 0.0670 , Validation loss= 0.0652 , Average loss= 0.0652 , Accuracy = 0.7193\n",
            "Epoch 126 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0660 , Average loss= 0.0656 , Accuracy = 0.7193\n",
            "Epoch 126 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0658 , Average loss= 0.0652 , Accuracy = 0.7193\n",
            "Epoch 126 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0660 , Average loss= 0.0657 , Accuracy = 0.7193\n",
            "Epoch 127 : for modelA 0 ==> Training loss= 0.0671 , Validation loss= 0.0659 , Average loss= 0.0662 , Accuracy = 0.7193\n",
            "Epoch 127 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0661 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 127 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0656 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 127 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0655 , Average loss= 0.0652 , Accuracy = 0.7193\n",
            "Epoch 127 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0650 , Average loss= 0.0656 , Accuracy = 0.7193\n",
            "Epoch 128 : for modelA 0 ==> Training loss= 0.0671 , Validation loss= 0.0664 , Average loss= 0.0664 , Accuracy = 0.7193\n",
            "Epoch 128 : for modelA 1 ==> Training loss= 0.0671 , Validation loss= 0.0655 , Average loss= 0.0652 , Accuracy = 0.7193\n",
            "Epoch 128 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0656 , Accuracy = 0.7193\n",
            "Epoch 128 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0651 , Average loss= 0.0651 , Accuracy = 0.7193\n",
            "Epoch 128 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0650 , Average loss= 0.0656 , Accuracy = 0.7193\n",
            "Epoch 129 : for modelA 0 ==> Training loss= 0.0671 , Validation loss= 0.0664 , Average loss= 0.0667 , Accuracy = 0.7193\n",
            "Epoch 129 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0658 , Average loss= 0.0655 , Accuracy = 0.7193\n",
            "Epoch 129 : for modelA 2 ==> Training loss= 0.0670 , Validation loss= 0.0656 , Average loss= 0.0656 , Accuracy = 0.7193\n",
            "Epoch 129 : for modelA 3 ==> Training loss= 0.0670 , Validation loss= 0.0655 , Average loss= 0.0655 , Accuracy = 0.7193\n",
            "Epoch 129 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0653 , Average loss= 0.0660 , Accuracy = 0.7193\n",
            "Epoch 130 : for modelA 0 ==> Training loss= 0.0672 , Validation loss= 0.0667 , Average loss= 0.0661 , Accuracy = 0.7193\n",
            "Epoch 130 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0655 , Average loss= 0.0658 , Accuracy = 0.7193\n",
            "Epoch 130 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0659 , Average loss= 0.0656 , Accuracy = 0.7193\n",
            "Epoch 130 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0661 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 130 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0646 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 131 : for modelA 0 ==> Training loss= 0.0672 , Validation loss= 0.0658 , Average loss= 0.0658 , Accuracy = 0.7193\n",
            "Epoch 131 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0654 , Average loss= 0.0658 , Accuracy = 0.7193\n",
            "Epoch 131 : for modelA 2 ==> Training loss= 0.0671 , Validation loss= 0.0662 , Average loss= 0.0656 , Accuracy = 0.7193\n",
            "Epoch 131 : for modelA 3 ==> Training loss= 0.0671 , Validation loss= 0.0664 , Average loss= 0.0658 , Accuracy = 0.7193\n",
            "Epoch 131 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0659 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 132 : for modelA 0 ==> Training loss= 0.0671 , Validation loss= 0.0663 , Average loss= 0.0658 , Accuracy = 0.7193\n",
            "Epoch 132 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0648 , Average loss= 0.0648 , Accuracy = 0.7193\n",
            "Epoch 132 : for modelA 2 ==> Training loss= 0.0670 , Validation loss= 0.0652 , Average loss= 0.0655 , Accuracy = 0.7193\n",
            "Epoch 132 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0654 , Average loss= 0.0658 , Accuracy = 0.7193\n",
            "Epoch 132 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0653 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 133 : for modelA 0 ==> Training loss= 0.0670 , Validation loss= 0.0660 , Average loss= 0.0660 , Accuracy = 0.7193\n",
            "Epoch 133 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0651 , Average loss= 0.0651 , Accuracy = 0.7193\n",
            "Epoch 133 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0652 , Average loss= 0.0659 , Accuracy = 0.7193\n",
            "Epoch 133 : for modelA 3 ==> Training loss= 0.0670 , Validation loss= 0.0648 , Average loss= 0.0661 , Accuracy = 0.7193\n",
            "Epoch 133 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0649 , Average loss= 0.0652 , Accuracy = 0.7193\n",
            "Epoch 134 : for modelA 0 ==> Training loss= 0.0670 , Validation loss= 0.0666 , Average loss= 0.0666 , Accuracy = 0.7193\n",
            "Epoch 134 : for modelA 1 ==> Training loss= 0.0670 , Validation loss= 0.0651 , Average loss= 0.0657 , Accuracy = 0.7193\n",
            "Epoch 134 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0652 , Average loss= 0.0652 , Accuracy = 0.7193\n",
            "Epoch 134 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0661 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 134 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0649 , Average loss= 0.0652 , Accuracy = 0.7193\n",
            "Epoch 135 : for modelA 0 ==> Training loss= 0.0671 , Validation loss= 0.0663 , Average loss= 0.0665 , Accuracy = 0.7193\n",
            "Epoch 135 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0654 , Average loss= 0.0660 , Accuracy = 0.7193\n",
            "Epoch 135 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0658 , Average loss= 0.0655 , Accuracy = 0.7193\n",
            "Epoch 135 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0651 , Average loss= 0.0651 , Accuracy = 0.7193\n",
            "Epoch 135 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0645 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 136 : for modelA 0 ==> Training loss= 0.0671 , Validation loss= 0.0660 , Average loss= 0.0660 , Accuracy = 0.7193\n",
            "Epoch 136 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0657 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 136 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0652 , Average loss= 0.0658 , Accuracy = 0.7193\n",
            "Epoch 136 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0657 , Accuracy = 0.7193\n",
            "Epoch 136 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0652 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 137 : for modelA 0 ==> Training loss= 0.0672 , Validation loss= 0.0662 , Average loss= 0.0659 , Accuracy = 0.7193\n",
            "Epoch 137 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 137 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0658 , Average loss= 0.0655 , Accuracy = 0.7193\n",
            "Epoch 137 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0654 , Average loss= 0.0657 , Accuracy = 0.7193\n",
            "Epoch 137 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0652 , Average loss= 0.0648 , Accuracy = 0.7193\n",
            "Epoch 138 : for modelA 0 ==> Training loss= 0.0670 , Validation loss= 0.0662 , Average loss= 0.0668 , Accuracy = 0.7193\n",
            "Epoch 138 : for modelA 1 ==> Training loss= 0.0670 , Validation loss= 0.0654 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 138 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0654 , Average loss= 0.0651 , Accuracy = 0.7193\n",
            "Epoch 138 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0654 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 138 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0655 , Average loss= 0.0662 , Accuracy = 0.7193\n",
            "Epoch 139 : for modelA 0 ==> Training loss= 0.0672 , Validation loss= 0.0662 , Average loss= 0.0662 , Accuracy = 0.7193\n",
            "Epoch 139 : for modelA 1 ==> Training loss= 0.0670 , Validation loss= 0.0653 , Average loss= 0.0657 , Accuracy = 0.7193\n",
            "Epoch 139 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0664 , Average loss= 0.0648 , Accuracy = 0.7193\n",
            "Epoch 139 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0653 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 139 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0658 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 140 : for modelA 0 ==> Training loss= 0.0671 , Validation loss= 0.0667 , Average loss= 0.0667 , Accuracy = 0.7193\n",
            "Epoch 140 : for modelA 1 ==> Training loss= 0.0670 , Validation loss= 0.0653 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 140 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0651 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 140 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0657 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 140 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0655 , Average loss= 0.0652 , Accuracy = 0.7193\n",
            "Epoch 141 : for modelA 0 ==> Training loss= 0.0670 , Validation loss= 0.0659 , Average loss= 0.0659 , Accuracy = 0.7193\n",
            "Epoch 141 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0647 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 141 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0657 , Average loss= 0.0661 , Accuracy = 0.7193\n",
            "Epoch 141 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0653 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 141 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0655 , Average loss= 0.0665 , Accuracy = 0.7193\n",
            "Epoch 142 : for modelA 0 ==> Training loss= 0.0671 , Validation loss= 0.0658 , Average loss= 0.0661 , Accuracy = 0.7193\n",
            "Epoch 142 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0656 , Accuracy = 0.7193\n",
            "Epoch 142 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0654 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 142 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0660 , Average loss= 0.0660 , Accuracy = 0.7193\n",
            "Epoch 142 : for modelA 4 ==> Training loss= 0.0671 , Validation loss= 0.0651 , Average loss= 0.0651 , Accuracy = 0.7193\n",
            "Epoch 143 : for modelA 0 ==> Training loss= 0.0671 , Validation loss= 0.0661 , Average loss= 0.0664 , Accuracy = 0.7193\n",
            "Epoch 143 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0653 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 143 : for modelA 2 ==> Training loss= 0.0670 , Validation loss= 0.0647 , Average loss= 0.0657 , Accuracy = 0.7193\n",
            "Epoch 143 : for modelA 3 ==> Training loss= 0.0670 , Validation loss= 0.0656 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 143 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0658 , Average loss= 0.0644 , Accuracy = 0.7193\n",
            "Epoch 144 : for modelA 0 ==> Training loss= 0.0670 , Validation loss= 0.0655 , Average loss= 0.0661 , Accuracy = 0.7193\n",
            "Epoch 144 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0656 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 144 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0647 , Average loss= 0.0660 , Accuracy = 0.7193\n",
            "Epoch 144 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0649 , Average loss= 0.0656 , Accuracy = 0.7193\n",
            "Epoch 144 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0644 , Average loss= 0.0651 , Accuracy = 0.7193\n",
            "Epoch 145 : for modelA 0 ==> Training loss= 0.0670 , Validation loss= 0.0660 , Average loss= 0.0666 , Accuracy = 0.7193\n",
            "Epoch 145 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0653 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 145 : for modelA 2 ==> Training loss= 0.0670 , Validation loss= 0.0654 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 145 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0659 , Average loss= 0.0656 , Accuracy = 0.7193\n",
            "Epoch 145 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0651 , Average loss= 0.0644 , Accuracy = 0.7193\n",
            "Epoch 146 : for modelA 0 ==> Training loss= 0.0671 , Validation loss= 0.0657 , Average loss= 0.0657 , Accuracy = 0.7193\n",
            "Epoch 146 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0653 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 146 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0650 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 146 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0653 , Average loss= 0.0656 , Accuracy = 0.7193\n",
            "Epoch 146 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0651 , Average loss= 0.0644 , Accuracy = 0.7193\n",
            "Epoch 147 : for modelA 0 ==> Training loss= 0.0670 , Validation loss= 0.0663 , Average loss= 0.0660 , Accuracy = 0.7193\n",
            "Epoch 147 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0652 , Average loss= 0.0659 , Accuracy = 0.7193\n",
            "Epoch 147 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0653 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 147 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0656 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 147 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0647 , Average loss= 0.0651 , Accuracy = 0.7193\n",
            "Epoch 148 : for modelA 0 ==> Training loss= 0.0671 , Validation loss= 0.0657 , Average loss= 0.0657 , Accuracy = 0.7193\n",
            "Epoch 148 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0649 , Average loss= 0.0652 , Accuracy = 0.7193\n",
            "Epoch 148 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0656 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 148 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0652 , Average loss= 0.0656 , Accuracy = 0.7193\n",
            "Epoch 148 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0647 , Average loss= 0.0651 , Accuracy = 0.7193\n",
            "Epoch 149 : for modelA 0 ==> Training loss= 0.0671 , Validation loss= 0.0660 , Average loss= 0.0660 , Accuracy = 0.7193\n",
            "Epoch 149 : for modelA 1 ==> Training loss= 0.0670 , Validation loss= 0.0649 , Average loss= 0.0652 , Accuracy = 0.7193\n",
            "Epoch 149 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0660 , Average loss= 0.0660 , Accuracy = 0.7193\n",
            "Epoch 149 : for modelA 3 ==> Training loss= 0.0670 , Validation loss= 0.0656 , Average loss= 0.0659 , Accuracy = 0.7193\n",
            "Epoch 149 : for modelA 4 ==> Training loss= 0.0671 , Validation loss= 0.0654 , Average loss= 0.0651 , Accuracy = 0.7193\n",
            "Epoch 150 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0663 , Average loss= 0.0660 , Accuracy = 0.7193\n",
            "Epoch 150 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0656 , Average loss= 0.0656 , Accuracy = 0.7193\n",
            "Epoch 150 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0646 , Average loss= 0.0656 , Accuracy = 0.7193\n",
            "Epoch 150 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0645 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 150 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0651 , Average loss= 0.0651 , Accuracy = 0.7193\n",
            "Epoch 151 : for modelA 0 ==> Training loss= 0.0671 , Validation loss= 0.0659 , Average loss= 0.0656 , Accuracy = 0.7193\n",
            "Epoch 151 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0649 , Average loss= 0.0655 , Accuracy = 0.7193\n",
            "Epoch 151 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0649 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 151 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0649 , Average loss= 0.0655 , Accuracy = 0.7193\n",
            "Epoch 151 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0651 , Average loss= 0.0651 , Accuracy = 0.7193\n",
            "Epoch 152 : for modelA 0 ==> Training loss= 0.0672 , Validation loss= 0.0656 , Average loss= 0.0656 , Accuracy = 0.7193\n",
            "Epoch 152 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0655 , Average loss= 0.0655 , Accuracy = 0.7193\n",
            "Epoch 152 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0656 , Average loss= 0.0659 , Accuracy = 0.7193\n",
            "Epoch 152 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0645 , Average loss= 0.0655 , Accuracy = 0.7193\n",
            "Epoch 152 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0643 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 153 : for modelA 0 ==> Training loss= 0.0671 , Validation loss= 0.0656 , Average loss= 0.0665 , Accuracy = 0.7193\n",
            "Epoch 153 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0655 , Average loss= 0.0652 , Accuracy = 0.7193\n",
            "Epoch 153 : for modelA 2 ==> Training loss= 0.0670 , Validation loss= 0.0649 , Average loss= 0.0652 , Accuracy = 0.7193\n",
            "Epoch 153 : for modelA 3 ==> Training loss= 0.0670 , Validation loss= 0.0652 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 153 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0654 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 154 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0656 , Average loss= 0.0659 , Accuracy = 0.7193\n",
            "Epoch 154 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0645 , Average loss= 0.0648 , Accuracy = 0.7193\n",
            "Epoch 154 : for modelA 2 ==> Training loss= 0.0670 , Validation loss= 0.0652 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 154 : for modelA 3 ==> Training loss= 0.0670 , Validation loss= 0.0645 , Average loss= 0.0655 , Accuracy = 0.7193\n",
            "Epoch 154 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0647 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 155 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0659 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 155 : for modelA 1 ==> Training loss= 0.0672 , Validation loss= 0.0655 , Average loss= 0.0652 , Accuracy = 0.7193\n",
            "Epoch 155 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0649 , Average loss= 0.0656 , Accuracy = 0.7193\n",
            "Epoch 155 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0655 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 155 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 156 : for modelA 0 ==> Training loss= 0.0671 , Validation loss= 0.0658 , Average loss= 0.0655 , Accuracy = 0.7193\n",
            "Epoch 156 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0648 , Average loss= 0.0652 , Accuracy = 0.7193\n",
            "Epoch 156 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0652 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 156 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0648 , Average loss= 0.0652 , Accuracy = 0.7193\n",
            "Epoch 156 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0647 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 157 : for modelA 0 ==> Training loss= 0.0670 , Validation loss= 0.0658 , Average loss= 0.0661 , Accuracy = 0.7193\n",
            "Epoch 157 : for modelA 1 ==> Training loss= 0.0671 , Validation loss= 0.0658 , Average loss= 0.0652 , Accuracy = 0.7193\n",
            "Epoch 157 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0649 , Average loss= 0.0652 , Accuracy = 0.7193\n",
            "Epoch 157 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0645 , Average loss= 0.0652 , Accuracy = 0.7193\n",
            "Epoch 157 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0654 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 158 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0658 , Average loss= 0.0658 , Accuracy = 0.7193\n",
            "Epoch 158 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0655 , Average loss= 0.0655 , Accuracy = 0.7193\n",
            "Epoch 158 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0649 , Average loss= 0.0652 , Accuracy = 0.7193\n",
            "Epoch 158 : for modelA 3 ==> Training loss= 0.0671 , Validation loss= 0.0651 , Average loss= 0.0651 , Accuracy = 0.7193\n",
            "Epoch 158 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0643 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 159 : for modelA 0 ==> Training loss= 0.0671 , Validation loss= 0.0667 , Average loss= 0.0658 , Accuracy = 0.7193\n",
            "Epoch 159 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0655 , Average loss= 0.0651 , Accuracy = 0.7193\n",
            "Epoch 159 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0655 , Average loss= 0.0652 , Accuracy = 0.7193\n",
            "Epoch 159 : for modelA 3 ==> Training loss= 0.0670 , Validation loss= 0.0645 , Average loss= 0.0648 , Accuracy = 0.7193\n",
            "Epoch 159 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 160 : for modelA 0 ==> Training loss= 0.0671 , Validation loss= 0.0652 , Average loss= 0.0658 , Accuracy = 0.7193\n",
            "Epoch 160 : for modelA 1 ==> Training loss= 0.0671 , Validation loss= 0.0658 , Average loss= 0.0651 , Accuracy = 0.7193\n",
            "Epoch 160 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0652 , Average loss= 0.0659 , Accuracy = 0.7193\n",
            "Epoch 160 : for modelA 3 ==> Training loss= 0.0671 , Validation loss= 0.0655 , Average loss= 0.0651 , Accuracy = 0.7193\n",
            "Epoch 160 : for modelA 4 ==> Training loss= 0.0671 , Validation loss= 0.0653 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 161 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0655 , Average loss= 0.0658 , Accuracy = 0.7193\n",
            "Epoch 161 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0651 , Average loss= 0.0658 , Accuracy = 0.7193\n",
            "Epoch 161 : for modelA 2 ==> Training loss= 0.0670 , Validation loss= 0.0652 , Average loss= 0.0659 , Accuracy = 0.7193\n",
            "Epoch 161 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0648 , Average loss= 0.0651 , Accuracy = 0.7193\n",
            "Epoch 161 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0653 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 162 : for modelA 0 ==> Training loss= 0.0670 , Validation loss= 0.0661 , Average loss= 0.0657 , Accuracy = 0.7193\n",
            "Epoch 162 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0648 , Average loss= 0.0648 , Accuracy = 0.7193\n",
            "Epoch 162 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0648 , Average loss= 0.0655 , Accuracy = 0.7193\n",
            "Epoch 162 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0648 , Average loss= 0.0648 , Accuracy = 0.7193\n",
            "Epoch 162 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0657 , Accuracy = 0.7193\n",
            "Epoch 163 : for modelA 0 ==> Training loss= 0.0670 , Validation loss= 0.0660 , Average loss= 0.0657 , Accuracy = 0.7193\n",
            "Epoch 163 : for modelA 1 ==> Training loss= 0.0670 , Validation loss= 0.0651 , Average loss= 0.0648 , Accuracy = 0.7193\n",
            "Epoch 163 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0648 , Average loss= 0.0655 , Accuracy = 0.7193\n",
            "Epoch 163 : for modelA 3 ==> Training loss= 0.0670 , Validation loss= 0.0648 , Average loss= 0.0651 , Accuracy = 0.7193\n",
            "Epoch 163 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0646 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 164 : for modelA 0 ==> Training loss= 0.0671 , Validation loss= 0.0663 , Average loss= 0.0651 , Accuracy = 0.7193\n",
            "Epoch 164 : for modelA 1 ==> Training loss= 0.0671 , Validation loss= 0.0647 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 164 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0648 , Average loss= 0.0651 , Accuracy = 0.7193\n",
            "Epoch 164 : for modelA 3 ==> Training loss= 0.0670 , Validation loss= 0.0651 , Average loss= 0.0648 , Accuracy = 0.7193\n",
            "Epoch 164 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0660 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 165 : for modelA 0 ==> Training loss= 0.0671 , Validation loss= 0.0657 , Average loss= 0.0657 , Accuracy = 0.7193\n",
            "Epoch 165 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0647 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 165 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0648 , Average loss= 0.0655 , Accuracy = 0.7193\n",
            "Epoch 165 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0644 , Average loss= 0.0651 , Accuracy = 0.7193\n",
            "Epoch 165 : for modelA 4 ==> Training loss= 0.0671 , Validation loss= 0.0646 , Average loss= 0.0657 , Accuracy = 0.7193\n",
            "Epoch 166 : for modelA 0 ==> Training loss= 0.0670 , Validation loss= 0.0660 , Average loss= 0.0651 , Accuracy = 0.7193\n",
            "Epoch 166 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0654 , Average loss= 0.0651 , Accuracy = 0.7193\n",
            "Epoch 166 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0658 , Average loss= 0.0648 , Accuracy = 0.7193\n",
            "Epoch 166 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0654 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 166 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 167 : for modelA 0 ==> Training loss= 0.0670 , Validation loss= 0.0660 , Average loss= 0.0657 , Accuracy = 0.7193\n",
            "Epoch 167 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0647 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 167 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0648 , Average loss= 0.0651 , Accuracy = 0.7193\n",
            "Epoch 167 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0654 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 167 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0642 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 168 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0653 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 168 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0651 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 168 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0658 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 168 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0654 , Average loss= 0.0644 , Accuracy = 0.7193\n",
            "Epoch 168 : for modelA 4 ==> Training loss= 0.0666 , Validation loss= 0.0642 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 169 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0656 , Average loss= 0.0656 , Accuracy = 0.7193\n",
            "Epoch 169 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0654 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 169 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0644 , Average loss= 0.0644 , Accuracy = 0.7193\n",
            "Epoch 169 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0654 , Average loss= 0.0661 , Accuracy = 0.7193\n",
            "Epoch 169 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0649 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 170 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0653 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 170 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 170 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0654 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 170 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0647 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 170 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0653 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 171 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0653 , Average loss= 0.0662 , Accuracy = 0.7193\n",
            "Epoch 171 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 171 : for modelA 2 ==> Training loss= 0.0670 , Validation loss= 0.0654 , Average loss= 0.0651 , Accuracy = 0.7193\n",
            "Epoch 171 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0654 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 171 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0649 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 172 : for modelA 0 ==> Training loss= 0.0670 , Validation loss= 0.0656 , Average loss= 0.0656 , Accuracy = 0.7193\n",
            "Epoch 172 : for modelA 1 ==> Training loss= 0.0670 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 172 : for modelA 2 ==> Training loss= 0.0670 , Validation loss= 0.0651 , Average loss= 0.0661 , Accuracy = 0.7193\n",
            "Epoch 172 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0654 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 172 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0649 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 173 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0659 , Average loss= 0.0659 , Accuracy = 0.7193\n",
            "Epoch 173 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 173 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0651 , Average loss= 0.0658 , Accuracy = 0.7193\n",
            "Epoch 173 : for modelA 3 ==> Training loss= 0.0670 , Validation loss= 0.0650 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 173 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0653 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 174 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0653 , Average loss= 0.0656 , Accuracy = 0.7193\n",
            "Epoch 174 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0647 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 174 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0647 , Average loss= 0.0651 , Accuracy = 0.7193\n",
            "Epoch 174 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0654 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 174 : for modelA 4 ==> Training loss= 0.0666 , Validation loss= 0.0653 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 175 : for modelA 0 ==> Training loss= 0.0670 , Validation loss= 0.0656 , Average loss= 0.0652 , Accuracy = 0.7193\n",
            "Epoch 175 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0643 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 175 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0657 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 175 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0643 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 175 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0645 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 176 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0655 , Average loss= 0.0659 , Accuracy = 0.7193\n",
            "Epoch 176 : for modelA 1 ==> Training loss= 0.0670 , Validation loss= 0.0646 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 176 : for modelA 2 ==> Training loss= 0.0670 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 176 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 176 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0649 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 177 : for modelA 0 ==> Training loss= 0.0670 , Validation loss= 0.0658 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 177 : for modelA 1 ==> Training loss= 0.0666 , Validation loss= 0.0646 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 177 : for modelA 2 ==> Training loss= 0.0671 , Validation loss= 0.0647 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 177 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 177 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0649 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 178 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0658 , Average loss= 0.0655 , Accuracy = 0.7193\n",
            "Epoch 178 : for modelA 1 ==> Training loss= 0.0670 , Validation loss= 0.0657 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 178 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0650 , Average loss= 0.0661 , Accuracy = 0.7193\n",
            "Epoch 178 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0657 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 178 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0645 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 179 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0655 , Average loss= 0.0655 , Accuracy = 0.7193\n",
            "Epoch 179 : for modelA 1 ==> Training loss= 0.0670 , Validation loss= 0.0643 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 179 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0647 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 179 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0653 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 179 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0641 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 180 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0648 , Average loss= 0.0648 , Accuracy = 0.7193\n",
            "Epoch 180 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0653 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 180 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0650 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 180 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 180 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0645 , Average loss= 0.0656 , Accuracy = 0.7193\n",
            "Epoch 181 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0652 , Average loss= 0.0655 , Accuracy = 0.7193\n",
            "Epoch 181 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0646 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 181 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 181 : for modelA 3 ==> Training loss= 0.0670 , Validation loss= 0.0643 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 181 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0652 , Average loss= 0.0652 , Accuracy = 0.7193\n",
            "Epoch 182 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0651 , Average loss= 0.0658 , Accuracy = 0.7193\n",
            "Epoch 182 : for modelA 1 ==> Training loss= 0.0671 , Validation loss= 0.0653 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 182 : for modelA 2 ==> Training loss= 0.0670 , Validation loss= 0.0650 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 182 : for modelA 3 ==> Training loss= 0.0670 , Validation loss= 0.0646 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 182 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0645 , Average loss= 0.0656 , Accuracy = 0.7193\n",
            "Epoch 183 : for modelA 0 ==> Training loss= 0.0667 , Validation loss= 0.0648 , Average loss= 0.0655 , Accuracy = 0.7193\n",
            "Epoch 183 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 183 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0653 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 183 : for modelA 3 ==> Training loss= 0.0670 , Validation loss= 0.0650 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 183 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0645 , Average loss= 0.0648 , Accuracy = 0.7193\n",
            "Epoch 184 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0651 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 184 : for modelA 1 ==> Training loss= 0.0670 , Validation loss= 0.0650 , Average loss= 0.0660 , Accuracy = 0.7193\n",
            "Epoch 184 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0646 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 184 : for modelA 3 ==> Training loss= 0.0671 , Validation loss= 0.0650 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 184 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0648 , Average loss= 0.0648 , Accuracy = 0.7193\n",
            "Epoch 185 : for modelA 0 ==> Training loss= 0.0671 , Validation loss= 0.0651 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 185 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0653 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 185 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0646 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 185 : for modelA 3 ==> Training loss= 0.0670 , Validation loss= 0.0653 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 185 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0648 , Average loss= 0.0648 , Accuracy = 0.7193\n",
            "Epoch 186 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0654 , Average loss= 0.0648 , Accuracy = 0.7193\n",
            "Epoch 186 : for modelA 1 ==> Training loss= 0.0670 , Validation loss= 0.0646 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 186 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 186 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0650 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 186 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0652 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 187 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0654 , Average loss= 0.0657 , Accuracy = 0.7193\n",
            "Epoch 187 : for modelA 1 ==> Training loss= 0.0670 , Validation loss= 0.0649 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 187 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0646 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 187 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0650 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 187 : for modelA 4 ==> Training loss= 0.0666 , Validation loss= 0.0645 , Average loss= 0.0648 , Accuracy = 0.7193\n",
            "Epoch 188 : for modelA 0 ==> Training loss= 0.0671 , Validation loss= 0.0654 , Average loss= 0.0651 , Accuracy = 0.7193\n",
            "Epoch 188 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0646 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 188 : for modelA 2 ==> Training loss= 0.0670 , Validation loss= 0.0646 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 188 : for modelA 3 ==> Training loss= 0.0666 , Validation loss= 0.0653 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 188 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0645 , Average loss= 0.0648 , Accuracy = 0.7193\n",
            "Epoch 189 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0651 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 189 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0653 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 189 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0646 , Average loss= 0.0657 , Accuracy = 0.7193\n",
            "Epoch 189 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0653 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 189 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0652 , Average loss= 0.0652 , Accuracy = 0.7193\n",
            "Epoch 190 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0647 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 190 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0656 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 190 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0657 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 190 : for modelA 3 ==> Training loss= 0.0666 , Validation loss= 0.0649 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 190 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0652 , Average loss= 0.0648 , Accuracy = 0.7193\n",
            "Epoch 191 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0654 , Average loss= 0.0657 , Accuracy = 0.7193\n",
            "Epoch 191 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0649 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 191 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0646 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 191 : for modelA 3 ==> Training loss= 0.0666 , Validation loss= 0.0646 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 191 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0652 , Average loss= 0.0648 , Accuracy = 0.7193\n",
            "Epoch 192 : for modelA 0 ==> Training loss= 0.0670 , Validation loss= 0.0653 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 192 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0645 , Average loss= 0.0656 , Accuracy = 0.7193\n",
            "Epoch 192 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0653 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 192 : for modelA 3 ==> Training loss= 0.0666 , Validation loss= 0.0642 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 192 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0644 , Average loss= 0.0652 , Accuracy = 0.7193\n",
            "Epoch 193 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 193 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0653 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 193 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0660 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 193 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0653 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 193 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0648 , Average loss= 0.0648 , Accuracy = 0.7193\n",
            "Epoch 194 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0653 , Average loss= 0.0657 , Accuracy = 0.7193\n",
            "Epoch 194 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0642 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 194 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0646 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 194 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0642 , Average loss= 0.0656 , Accuracy = 0.7193\n",
            "Epoch 194 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0648 , Average loss= 0.0652 , Accuracy = 0.7193\n",
            "Epoch 195 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0650 , Average loss= 0.0660 , Accuracy = 0.7193\n",
            "Epoch 195 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0653 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 195 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0645 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 195 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0649 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 195 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0648 , Average loss= 0.0659 , Accuracy = 0.7193\n",
            "Epoch 196 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 196 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0649 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 196 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0645 , Average loss= 0.0660 , Accuracy = 0.7193\n",
            "Epoch 196 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0642 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 196 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0648 , Average loss= 0.0644 , Accuracy = 0.7193\n",
            "Epoch 197 : for modelA 0 ==> Training loss= 0.0670 , Validation loss= 0.0653 , Average loss= 0.0656 , Accuracy = 0.7193\n",
            "Epoch 197 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0649 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 197 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0645 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 197 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0656 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 197 : for modelA 4 ==> Training loss= 0.0671 , Validation loss= 0.0648 , Average loss= 0.0640 , Accuracy = 0.7193\n",
            "Epoch 198 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 198 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0649 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 198 : for modelA 2 ==> Training loss= 0.0666 , Validation loss= 0.0653 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 198 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0649 , Average loss= 0.0656 , Accuracy = 0.7193\n",
            "Epoch 198 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0648 , Average loss= 0.0651 , Accuracy = 0.7193\n",
            "Epoch 199 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0656 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 199 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0652 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 199 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0649 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 199 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0645 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 199 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0648 , Average loss= 0.0655 , Accuracy = 0.7193\n",
            "Epoch 200 : for modelA 0 ==> Training loss= 0.0671 , Validation loss= 0.0656 , Average loss= 0.0656 , Accuracy = 0.7193\n",
            "Epoch 200 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0649 , Average loss= 0.0656 , Accuracy = 0.7193\n",
            "Epoch 200 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0645 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 200 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0642 , Average loss= 0.0652 , Accuracy = 0.7193\n",
            "Epoch 200 : for modelA 4 ==> Training loss= 0.0666 , Validation loss= 0.0648 , Average loss= 0.0644 , Accuracy = 0.7193\n",
            "Epoch 201 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0656 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 201 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0645 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 201 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0645 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 201 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0641 , Average loss= 0.0652 , Accuracy = 0.7193\n",
            "Epoch 201 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0648 , Average loss= 0.0644 , Accuracy = 0.7193\n",
            "Epoch 202 : for modelA 0 ==> Training loss= 0.0667 , Validation loss= 0.0649 , Average loss= 0.0659 , Accuracy = 0.7193\n",
            "Epoch 202 : for modelA 1 ==> Training loss= 0.0666 , Validation loss= 0.0649 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 202 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0652 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 202 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0652 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 202 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0651 , Average loss= 0.0648 , Accuracy = 0.7193\n",
            "Epoch 203 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0652 , Average loss= 0.0656 , Accuracy = 0.7193\n",
            "Epoch 203 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0641 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 203 : for modelA 2 ==> Training loss= 0.0671 , Validation loss= 0.0656 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 203 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0645 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 203 : for modelA 4 ==> Training loss= 0.0671 , Validation loss= 0.0655 , Average loss= 0.0640 , Accuracy = 0.7193\n",
            "Epoch 204 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0652 , Average loss= 0.0652 , Accuracy = 0.7193\n",
            "Epoch 204 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0645 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 204 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0652 , Average loss= 0.0652 , Accuracy = 0.7193\n",
            "Epoch 204 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0652 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 204 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0651 , Average loss= 0.0640 , Accuracy = 0.7193\n",
            "Epoch 205 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0652 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 205 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0649 , Average loss= 0.0652 , Accuracy = 0.7193\n",
            "Epoch 205 : for modelA 2 ==> Training loss= 0.0671 , Validation loss= 0.0645 , Average loss= 0.0652 , Accuracy = 0.7193\n",
            "Epoch 205 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0652 , Average loss= 0.0652 , Accuracy = 0.7193\n",
            "Epoch 205 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0644 , Average loss= 0.0648 , Accuracy = 0.7193\n",
            "Epoch 206 : for modelA 0 ==> Training loss= 0.0670 , Validation loss= 0.0645 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 206 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0645 , Average loss= 0.0652 , Accuracy = 0.7193\n",
            "Epoch 206 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0649 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 206 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0648 , Average loss= 0.0652 , Accuracy = 0.7193\n",
            "Epoch 206 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0651 , Average loss= 0.0651 , Accuracy = 0.7193\n",
            "Epoch 207 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0649 , Average loss= 0.0652 , Accuracy = 0.7193\n",
            "Epoch 207 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0645 , Average loss= 0.0656 , Accuracy = 0.7193\n",
            "Epoch 207 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0641 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 207 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0656 , Average loss= 0.0648 , Accuracy = 0.7193\n",
            "Epoch 207 : for modelA 4 ==> Training loss= 0.0671 , Validation loss= 0.0644 , Average loss= 0.0644 , Accuracy = 0.7193\n",
            "Epoch 208 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0652 , Average loss= 0.0652 , Accuracy = 0.7193\n",
            "Epoch 208 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0648 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 208 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0645 , Average loss= 0.0652 , Accuracy = 0.7193\n",
            "Epoch 208 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0652 , Average loss= 0.0648 , Accuracy = 0.7193\n",
            "Epoch 208 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0647 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 209 : for modelA 0 ==> Training loss= 0.0670 , Validation loss= 0.0648 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 209 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0648 , Average loss= 0.0648 , Accuracy = 0.7193\n",
            "Epoch 209 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0652 , Average loss= 0.0652 , Accuracy = 0.7193\n",
            "Epoch 209 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0648 , Average loss= 0.0641 , Accuracy = 0.7193\n",
            "Epoch 209 : for modelA 4 ==> Training loss= 0.0671 , Validation loss= 0.0647 , Average loss= 0.0644 , Accuracy = 0.7193\n",
            "Epoch 210 : for modelA 0 ==> Training loss= 0.0670 , Validation loss= 0.0652 , Average loss= 0.0655 , Accuracy = 0.7193\n",
            "Epoch 210 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0648 , Average loss= 0.0648 , Accuracy = 0.7193\n",
            "Epoch 210 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0648 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 210 : for modelA 3 ==> Training loss= 0.0666 , Validation loss= 0.0648 , Average loss= 0.0648 , Accuracy = 0.7193\n",
            "Epoch 210 : for modelA 4 ==> Training loss= 0.0666 , Validation loss= 0.0651 , Average loss= 0.0651 , Accuracy = 0.7193\n",
            "Epoch 211 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0648 , Average loss= 0.0652 , Accuracy = 0.7193\n",
            "Epoch 211 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0645 , Average loss= 0.0659 , Accuracy = 0.7193\n",
            "Epoch 211 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0648 , Average loss= 0.0648 , Accuracy = 0.7193\n",
            "Epoch 211 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0656 , Average loss= 0.0648 , Accuracy = 0.7193\n",
            "Epoch 211 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0647 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 212 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0655 , Average loss= 0.0652 , Accuracy = 0.7193\n",
            "Epoch 212 : for modelA 1 ==> Training loss= 0.0666 , Validation loss= 0.0648 , Average loss= 0.0641 , Accuracy = 0.7193\n",
            "Epoch 212 : for modelA 2 ==> Training loss= 0.0666 , Validation loss= 0.0648 , Average loss= 0.0641 , Accuracy = 0.7193\n",
            "Epoch 212 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0645 , Average loss= 0.0648 , Accuracy = 0.7193\n",
            "Epoch 212 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0655 , Average loss= 0.0640 , Accuracy = 0.7193\n",
            "Epoch 213 : for modelA 0 ==> Training loss= 0.0670 , Validation loss= 0.0652 , Average loss= 0.0652 , Accuracy = 0.7193\n",
            "Epoch 213 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0648 , Average loss= 0.0648 , Accuracy = 0.7193\n",
            "Epoch 213 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0652 , Average loss= 0.0641 , Accuracy = 0.7193\n",
            "Epoch 213 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0645 , Average loss= 0.0648 , Accuracy = 0.7193\n",
            "Epoch 213 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0644 , Average loss= 0.0651 , Accuracy = 0.7193\n",
            "Epoch 214 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0655 , Average loss= 0.0655 , Accuracy = 0.7193\n",
            "Epoch 214 : for modelA 1 ==> Training loss= 0.0666 , Validation loss= 0.0641 , Average loss= 0.0652 , Accuracy = 0.7193\n",
            "Epoch 214 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0652 , Average loss= 0.0641 , Accuracy = 0.7193\n",
            "Epoch 214 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0648 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 214 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0647 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 215 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0645 , Average loss= 0.0648 , Accuracy = 0.7193\n",
            "Epoch 215 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0648 , Average loss= 0.0641 , Accuracy = 0.7193\n",
            "Epoch 215 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0652 , Average loss= 0.0648 , Accuracy = 0.7193\n",
            "Epoch 215 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0652 , Average loss= 0.0641 , Accuracy = 0.7193\n",
            "Epoch 215 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0640 , Average loss= 0.0651 , Accuracy = 0.7193\n",
            "Epoch 216 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0651 , Average loss= 0.0651 , Accuracy = 0.7193\n",
            "Epoch 216 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0641 , Average loss= 0.0648 , Accuracy = 0.7193\n",
            "Epoch 216 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0644 , Average loss= 0.0648 , Accuracy = 0.7193\n",
            "Epoch 216 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0652 , Average loss= 0.0648 , Accuracy = 0.7193\n",
            "Epoch 216 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0651 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 217 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0658 , Average loss= 0.0655 , Accuracy = 0.7193\n",
            "Epoch 217 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0655 , Average loss= 0.0640 , Accuracy = 0.7193\n",
            "Epoch 217 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0648 , Average loss= 0.0641 , Accuracy = 0.7193\n",
            "Epoch 217 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0652 , Average loss= 0.0652 , Accuracy = 0.7193\n",
            "Epoch 217 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0651 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 218 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0648 , Average loss= 0.0655 , Accuracy = 0.7193\n",
            "Epoch 218 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0644 , Average loss= 0.0644 , Accuracy = 0.7193\n",
            "Epoch 218 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0652 , Average loss= 0.0641 , Accuracy = 0.7193\n",
            "Epoch 218 : for modelA 3 ==> Training loss= 0.0672 , Validation loss= 0.0652 , Average loss= 0.0648 , Accuracy = 0.7193\n",
            "Epoch 218 : for modelA 4 ==> Training loss= 0.0666 , Validation loss= 0.0643 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 219 : for modelA 0 ==> Training loss= 0.0667 , Validation loss= 0.0651 , Average loss= 0.0658 , Accuracy = 0.7193\n",
            "Epoch 219 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0644 , Average loss= 0.0648 , Accuracy = 0.7193\n",
            "Epoch 219 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0648 , Average loss= 0.0644 , Accuracy = 0.7193\n",
            "Epoch 219 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0644 , Average loss= 0.0644 , Accuracy = 0.7193\n",
            "Epoch 219 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0647 , Average loss= 0.0651 , Accuracy = 0.7193\n",
            "Epoch 220 : for modelA 0 ==> Training loss= 0.0671 , Validation loss= 0.0654 , Average loss= 0.0651 , Accuracy = 0.7193\n",
            "Epoch 220 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0648 , Average loss= 0.0648 , Accuracy = 0.7193\n",
            "Epoch 220 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0644 , Average loss= 0.0655 , Accuracy = 0.7193\n",
            "Epoch 220 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0641 , Average loss= 0.0648 , Accuracy = 0.7193\n",
            "Epoch 220 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0658 , Average loss= 0.0651 , Accuracy = 0.7193\n",
            "Epoch 221 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0654 , Average loss= 0.0651 , Accuracy = 0.7193\n",
            "Epoch 221 : for modelA 1 ==> Training loss= 0.0666 , Validation loss= 0.0651 , Average loss= 0.0651 , Accuracy = 0.7193\n",
            "Epoch 221 : for modelA 2 ==> Training loss= 0.0666 , Validation loss= 0.0648 , Average loss= 0.0655 , Accuracy = 0.7193\n",
            "Epoch 221 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0644 , Average loss= 0.0644 , Accuracy = 0.7193\n",
            "Epoch 221 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0647 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 222 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0647 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 222 : for modelA 1 ==> Training loss= 0.0671 , Validation loss= 0.0640 , Average loss= 0.0640 , Accuracy = 0.7193\n",
            "Epoch 222 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0648 , Average loss= 0.0652 , Accuracy = 0.7193\n",
            "Epoch 222 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0652 , Average loss= 0.0644 , Accuracy = 0.7193\n",
            "Epoch 222 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0651 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 223 : for modelA 0 ==> Training loss= 0.0670 , Validation loss= 0.0651 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 223 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0651 , Average loss= 0.0644 , Accuracy = 0.7193\n",
            "Epoch 223 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0644 , Average loss= 0.0640 , Accuracy = 0.7193\n",
            "Epoch 223 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0641 , Average loss= 0.0641 , Accuracy = 0.7193\n",
            "Epoch 223 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0651 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 224 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0651 , Average loss= 0.0651 , Accuracy = 0.7193\n",
            "Epoch 224 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0644 , Average loss= 0.0644 , Accuracy = 0.7193\n",
            "Epoch 224 : for modelA 2 ==> Training loss= 0.0671 , Validation loss= 0.0648 , Average loss= 0.0652 , Accuracy = 0.7193\n",
            "Epoch 224 : for modelA 3 ==> Training loss= 0.0671 , Validation loss= 0.0652 , Average loss= 0.0641 , Accuracy = 0.7193\n",
            "Epoch 224 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0647 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 225 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0651 , Average loss= 0.0661 , Accuracy = 0.7193\n",
            "Epoch 225 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0640 , Average loss= 0.0644 , Accuracy = 0.7193\n",
            "Epoch 225 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0652 , Average loss= 0.0652 , Accuracy = 0.7193\n",
            "Epoch 225 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0648 , Average loss= 0.0652 , Accuracy = 0.7193\n",
            "Epoch 225 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0639 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 226 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0647 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 226 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0648 , Average loss= 0.0655 , Accuracy = 0.7193\n",
            "Epoch 226 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0644 , Average loss= 0.0648 , Accuracy = 0.7193\n",
            "Epoch 226 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0648 , Average loss= 0.0655 , Accuracy = 0.7193\n",
            "Epoch 226 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0647 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 227 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0654 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 227 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0659 , Average loss= 0.0648 , Accuracy = 0.7193\n",
            "Epoch 227 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0644 , Average loss= 0.0648 , Accuracy = 0.7193\n",
            "Epoch 227 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0648 , Average loss= 0.0648 , Accuracy = 0.7193\n",
            "Epoch 227 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0647 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 228 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0647 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 228 : for modelA 1 ==> Training loss= 0.0666 , Validation loss= 0.0644 , Average loss= 0.0644 , Accuracy = 0.7193\n",
            "Epoch 228 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0648 , Average loss= 0.0648 , Accuracy = 0.7193\n",
            "Epoch 228 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0648 , Average loss= 0.0651 , Accuracy = 0.7193\n",
            "Epoch 228 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0651 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 229 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0657 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 229 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0644 , Average loss= 0.0644 , Accuracy = 0.7193\n",
            "Epoch 229 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0648 , Average loss= 0.0648 , Accuracy = 0.7193\n",
            "Epoch 229 : for modelA 3 ==> Training loss= 0.0666 , Validation loss= 0.0651 , Average loss= 0.0644 , Accuracy = 0.7193\n",
            "Epoch 229 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0647 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 230 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 230 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0651 , Average loss= 0.0655 , Accuracy = 0.7193\n",
            "Epoch 230 : for modelA 2 ==> Training loss= 0.0671 , Validation loss= 0.0651 , Average loss= 0.0644 , Accuracy = 0.7193\n",
            "Epoch 230 : for modelA 3 ==> Training loss= 0.0666 , Validation loss= 0.0644 , Average loss= 0.0648 , Accuracy = 0.7193\n",
            "Epoch 230 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0651 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 231 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0654 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 231 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0651 , Average loss= 0.0651 , Accuracy = 0.7193\n",
            "Epoch 231 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0644 , Average loss= 0.0644 , Accuracy = 0.7193\n",
            "Epoch 231 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0640 , Average loss= 0.0648 , Accuracy = 0.7193\n",
            "Epoch 231 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0651 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 232 : for modelA 0 ==> Training loss= 0.0666 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 232 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0644 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 232 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0655 , Average loss= 0.0644 , Accuracy = 0.7193\n",
            "Epoch 232 : for modelA 3 ==> Training loss= 0.0671 , Validation loss= 0.0640 , Average loss= 0.0644 , Accuracy = 0.7193\n",
            "Epoch 232 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0651 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 233 : for modelA 0 ==> Training loss= 0.0670 , Validation loss= 0.0654 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 233 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0647 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 233 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0644 , Average loss= 0.0648 , Accuracy = 0.7193\n",
            "Epoch 233 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0640 , Average loss= 0.0651 , Accuracy = 0.7193\n",
            "Epoch 233 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0651 , Average loss= 0.0651 , Accuracy = 0.7193\n",
            "Epoch 234 : for modelA 0 ==> Training loss= 0.0666 , Validation loss= 0.0654 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 234 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0647 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 234 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0648 , Average loss= 0.0651 , Accuracy = 0.7193\n",
            "Epoch 234 : for modelA 3 ==> Training loss= 0.0666 , Validation loss= 0.0648 , Average loss= 0.0640 , Accuracy = 0.7193\n",
            "Epoch 234 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0643 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 235 : for modelA 0 ==> Training loss= 0.0672 , Validation loss= 0.0653 , Average loss= 0.0657 , Accuracy = 0.7193\n",
            "Epoch 235 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0644 , Average loss= 0.0651 , Accuracy = 0.7193\n",
            "Epoch 235 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0644 , Average loss= 0.0648 , Accuracy = 0.7193\n",
            "Epoch 235 : for modelA 3 ==> Training loss= 0.0666 , Validation loss= 0.0644 , Average loss= 0.0648 , Accuracy = 0.7193\n",
            "Epoch 235 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0643 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 236 : for modelA 0 ==> Training loss= 0.0666 , Validation loss= 0.0657 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 236 : for modelA 1 ==> Training loss= 0.0666 , Validation loss= 0.0651 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 236 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0659 , Average loss= 0.0651 , Accuracy = 0.7193\n",
            "Epoch 236 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0651 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 236 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0643 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 237 : for modelA 0 ==> Training loss= 0.0667 , Validation loss= 0.0650 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 237 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0647 , Average loss= 0.0640 , Accuracy = 0.7193\n",
            "Epoch 237 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0651 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 237 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0647 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 237 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0643 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 238 : for modelA 0 ==> Training loss= 0.0667 , Validation loss= 0.0657 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 238 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0647 , Average loss= 0.0651 , Accuracy = 0.7193\n",
            "Epoch 238 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0651 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 238 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0640 , Average loss= 0.0651 , Accuracy = 0.7193\n",
            "Epoch 238 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 239 : for modelA 0 ==> Training loss= 0.0670 , Validation loss= 0.0657 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 239 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0643 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 239 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0647 , Average loss= 0.0640 , Accuracy = 0.7193\n",
            "Epoch 239 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0647 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 239 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0639 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 240 : for modelA 0 ==> Training loss= 0.0671 , Validation loss= 0.0653 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 240 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0651 , Average loss= 0.0655 , Accuracy = 0.7193\n",
            "Epoch 240 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0647 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 240 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0655 , Average loss= 0.0640 , Accuracy = 0.7193\n",
            "Epoch 240 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0647 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 241 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 241 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0647 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 241 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0647 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 241 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0644 , Average loss= 0.0644 , Accuracy = 0.7193\n",
            "Epoch 241 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0643 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 242 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 242 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0643 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 242 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0651 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 242 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0644 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 242 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0650 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 243 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0650 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 243 : for modelA 1 ==> Training loss= 0.0666 , Validation loss= 0.0651 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 243 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0647 , Average loss= 0.0644 , Accuracy = 0.7193\n",
            "Epoch 243 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0640 , Average loss= 0.0640 , Accuracy = 0.7193\n",
            "Epoch 243 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 244 : for modelA 0 ==> Training loss= 0.0670 , Validation loss= 0.0646 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 244 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0651 , Average loss= 0.0655 , Accuracy = 0.7193\n",
            "Epoch 244 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0640 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 244 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0644 , Average loss= 0.0651 , Accuracy = 0.7193\n",
            "Epoch 244 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0643 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 245 : for modelA 0 ==> Training loss= 0.0666 , Validation loss= 0.0649 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 245 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0643 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 245 : for modelA 2 ==> Training loss= 0.0666 , Validation loss= 0.0651 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 245 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0647 , Average loss= 0.0651 , Accuracy = 0.7193\n",
            "Epoch 245 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0639 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 246 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0649 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 246 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0643 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 246 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0647 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 246 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0651 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 246 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0639 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 247 : for modelA 0 ==> Training loss= 0.0671 , Validation loss= 0.0653 , Average loss= 0.0656 , Accuracy = 0.7193\n",
            "Epoch 247 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0647 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 247 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0651 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 247 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0643 , Average loss= 0.0651 , Accuracy = 0.7193\n",
            "Epoch 247 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0647 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 248 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0646 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 248 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0643 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 248 : for modelA 2 ==> Training loss= 0.0671 , Validation loss= 0.0651 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 248 : for modelA 3 ==> Training loss= 0.0671 , Validation loss= 0.0655 , Average loss= 0.0651 , Accuracy = 0.7193\n",
            "Epoch 248 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0646 , Average loss= 0.0639 , Accuracy = 0.7193\n",
            "Epoch 249 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0649 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 249 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0643 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 249 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0640 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 249 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0643 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 249 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0639 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 250 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0649 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 250 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0643 , Average loss= 0.0651 , Accuracy = 0.7193\n",
            "Epoch 250 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0655 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 250 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0651 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 250 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0643 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 251 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0642 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 251 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0647 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 251 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0647 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 251 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0643 , Average loss= 0.0651 , Accuracy = 0.7193\n",
            "Epoch 251 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0643 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 252 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0642 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 252 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0647 , Average loss= 0.0639 , Accuracy = 0.7193\n",
            "Epoch 252 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0647 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 252 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0643 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 252 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0643 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 253 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0653 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 253 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0643 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 253 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0647 , Average loss= 0.0651 , Accuracy = 0.7193\n",
            "Epoch 253 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0647 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 253 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0643 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 254 : for modelA 0 ==> Training loss= 0.0667 , Validation loss= 0.0653 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 254 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0651 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 254 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0651 , Average loss= 0.0640 , Accuracy = 0.7193\n",
            "Epoch 254 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0647 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 254 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0646 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 255 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0649 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 255 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0654 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 255 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0651 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 255 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0647 , Average loss= 0.0639 , Accuracy = 0.7193\n",
            "Epoch 255 : for modelA 4 ==> Training loss= 0.0671 , Validation loss= 0.0639 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 256 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0645 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 256 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0639 , Average loss= 0.0639 , Accuracy = 0.7193\n",
            "Epoch 256 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0647 , Average loss= 0.0651 , Accuracy = 0.7193\n",
            "Epoch 256 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0643 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 256 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0646 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 257 : for modelA 0 ==> Training loss= 0.0667 , Validation loss= 0.0649 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 257 : for modelA 1 ==> Training loss= 0.0671 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 257 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0647 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 257 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0643 , Average loss= 0.0651 , Accuracy = 0.7193\n",
            "Epoch 257 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0646 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 258 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0652 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 258 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 258 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0639 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 258 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0647 , Average loss= 0.0639 , Accuracy = 0.7193\n",
            "Epoch 258 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0643 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 259 : for modelA 0 ==> Training loss= 0.0666 , Validation loss= 0.0652 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 259 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0647 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 259 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0643 , Average loss= 0.0651 , Accuracy = 0.7193\n",
            "Epoch 259 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0647 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 259 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0642 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 260 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0652 , Average loss= 0.0660 , Accuracy = 0.7193\n",
            "Epoch 260 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0647 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 260 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0639 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 260 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0651 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 260 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0639 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 261 : for modelA 0 ==> Training loss= 0.0666 , Validation loss= 0.0641 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 261 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0647 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 261 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0654 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 261 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0647 , Average loss= 0.0639 , Accuracy = 0.7193\n",
            "Epoch 261 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 262 : for modelA 0 ==> Training loss= 0.0667 , Validation loss= 0.0649 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 262 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 262 : for modelA 2 ==> Training loss= 0.0671 , Validation loss= 0.0643 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 262 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0639 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 262 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0642 , Average loss= 0.0639 , Accuracy = 0.7193\n",
            "Epoch 263 : for modelA 0 ==> Training loss= 0.0667 , Validation loss= 0.0649 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 263 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0658 , Average loss= 0.0639 , Accuracy = 0.7193\n",
            "Epoch 263 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0647 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 263 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0651 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 263 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0650 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 264 : for modelA 0 ==> Training loss= 0.0667 , Validation loss= 0.0645 , Average loss= 0.0648 , Accuracy = 0.7193\n",
            "Epoch 264 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0647 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 264 : for modelA 2 ==> Training loss= 0.0671 , Validation loss= 0.0651 , Average loss= 0.0651 , Accuracy = 0.7193\n",
            "Epoch 264 : for modelA 3 ==> Training loss= 0.0666 , Validation loss= 0.0643 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 264 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0646 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 265 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0648 , Average loss= 0.0648 , Accuracy = 0.7193\n",
            "Epoch 265 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0647 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 265 : for modelA 2 ==> Training loss= 0.0666 , Validation loss= 0.0647 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 265 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0651 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 265 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 266 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0656 , Average loss= 0.0648 , Accuracy = 0.7193\n",
            "Epoch 266 : for modelA 1 ==> Training loss= 0.0671 , Validation loss= 0.0643 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 266 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0643 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 266 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0654 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 266 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0646 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 267 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0648 , Average loss= 0.0652 , Accuracy = 0.7193\n",
            "Epoch 267 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 267 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0647 , Average loss= 0.0651 , Accuracy = 0.7193\n",
            "Epoch 267 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0647 , Average loss= 0.0651 , Accuracy = 0.7193\n",
            "Epoch 267 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0639 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 268 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0645 , Average loss= 0.0648 , Accuracy = 0.7193\n",
            "Epoch 268 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0650 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 268 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0647 , Average loss= 0.0639 , Accuracy = 0.7193\n",
            "Epoch 268 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0643 , Average loss= 0.0651 , Accuracy = 0.7193\n",
            "Epoch 268 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0642 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 269 : for modelA 0 ==> Training loss= 0.0666 , Validation loss= 0.0648 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 269 : for modelA 1 ==> Training loss= 0.0666 , Validation loss= 0.0650 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 269 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0643 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 269 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0647 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 269 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0646 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 270 : for modelA 0 ==> Training loss= 0.0666 , Validation loss= 0.0648 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 270 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0646 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 270 : for modelA 2 ==> Training loss= 0.0666 , Validation loss= 0.0643 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 270 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0654 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 270 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0646 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 271 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0648 , Average loss= 0.0641 , Accuracy = 0.7193\n",
            "Epoch 271 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0654 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 271 : for modelA 2 ==> Training loss= 0.0666 , Validation loss= 0.0650 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 271 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0647 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 271 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 272 : for modelA 0 ==> Training loss= 0.0667 , Validation loss= 0.0648 , Average loss= 0.0648 , Accuracy = 0.7193\n",
            "Epoch 272 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0647 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 272 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0647 , Average loss= 0.0639 , Accuracy = 0.7193\n",
            "Epoch 272 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0647 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 272 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0646 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 273 : for modelA 0 ==> Training loss= 0.0667 , Validation loss= 0.0652 , Average loss= 0.0648 , Accuracy = 0.7193\n",
            "Epoch 273 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0647 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 273 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0639 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 273 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0643 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 273 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0638 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 274 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0648 , Average loss= 0.0655 , Accuracy = 0.7193\n",
            "Epoch 274 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0643 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 274 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0650 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 274 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0643 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 274 : for modelA 4 ==> Training loss= 0.0666 , Validation loss= 0.0646 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 275 : for modelA 0 ==> Training loss= 0.0667 , Validation loss= 0.0644 , Average loss= 0.0648 , Accuracy = 0.7193\n",
            "Epoch 275 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0643 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 275 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0643 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 275 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0643 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 275 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0654 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 276 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0655 , Average loss= 0.0644 , Accuracy = 0.7193\n",
            "Epoch 276 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0646 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 276 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 276 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0643 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 276 : for modelA 4 ==> Training loss= 0.0666 , Validation loss= 0.0642 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 277 : for modelA 0 ==> Training loss= 0.0667 , Validation loss= 0.0648 , Average loss= 0.0652 , Accuracy = 0.7193\n",
            "Epoch 277 : for modelA 1 ==> Training loss= 0.0666 , Validation loss= 0.0646 , Average loss= 0.0639 , Accuracy = 0.7193\n",
            "Epoch 277 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0646 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 277 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0650 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 277 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0658 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 278 : for modelA 0 ==> Training loss= 0.0667 , Validation loss= 0.0655 , Average loss= 0.0644 , Accuracy = 0.7193\n",
            "Epoch 278 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0646 , Average loss= 0.0639 , Accuracy = 0.7193\n",
            "Epoch 278 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0646 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 278 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0647 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 278 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0650 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 279 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0652 , Average loss= 0.0648 , Accuracy = 0.7193\n",
            "Epoch 279 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0639 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 279 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0646 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 279 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0647 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 279 : for modelA 4 ==> Training loss= 0.0671 , Validation loss= 0.0642 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 280 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0648 , Average loss= 0.0655 , Accuracy = 0.7193\n",
            "Epoch 280 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0643 , Average loss= 0.0639 , Accuracy = 0.7193\n",
            "Epoch 280 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0650 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 280 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0647 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 280 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 281 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0648 , Average loss= 0.0651 , Accuracy = 0.7193\n",
            "Epoch 281 : for modelA 1 ==> Training loss= 0.0671 , Validation loss= 0.0646 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 281 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0646 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 281 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0647 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 281 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0646 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 282 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0655 , Average loss= 0.0648 , Accuracy = 0.7193\n",
            "Epoch 282 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0643 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 282 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0639 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 282 : for modelA 3 ==> Training loss= 0.0671 , Validation loss= 0.0643 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 282 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0642 , Average loss= 0.0657 , Accuracy = 0.7193\n",
            "Epoch 283 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0644 , Average loss= 0.0640 , Accuracy = 0.7193\n",
            "Epoch 283 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0642 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 283 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0650 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 283 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0650 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 283 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0646 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 284 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0640 , Average loss= 0.0648 , Accuracy = 0.7193\n",
            "Epoch 284 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0642 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 284 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0650 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 284 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0650 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 284 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0646 , Average loss= 0.0638 , Accuracy = 0.7193\n",
            "Epoch 285 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0651 , Average loss= 0.0644 , Accuracy = 0.7193\n",
            "Epoch 285 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0650 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 285 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0646 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 285 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0643 , Average loss= 0.0639 , Accuracy = 0.7193\n",
            "Epoch 285 : for modelA 4 ==> Training loss= 0.0671 , Validation loss= 0.0650 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 286 : for modelA 0 ==> Training loss= 0.0666 , Validation loss= 0.0655 , Average loss= 0.0651 , Accuracy = 0.7193\n",
            "Epoch 286 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0639 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 286 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 286 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0650 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 286 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0650 , Average loss= 0.0638 , Accuracy = 0.7193\n",
            "Epoch 287 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0651 , Average loss= 0.0640 , Accuracy = 0.7193\n",
            "Epoch 287 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0650 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 287 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0646 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 287 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0643 , Average loss= 0.0639 , Accuracy = 0.7193\n",
            "Epoch 287 : for modelA 4 ==> Training loss= 0.0666 , Validation loss= 0.0642 , Average loss= 0.0638 , Accuracy = 0.7193\n",
            "Epoch 288 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0648 , Average loss= 0.0644 , Accuracy = 0.7193\n",
            "Epoch 288 : for modelA 1 ==> Training loss= 0.0671 , Validation loss= 0.0650 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 288 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0646 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 288 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0650 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 288 : for modelA 4 ==> Training loss= 0.0671 , Validation loss= 0.0646 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 289 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0644 , Average loss= 0.0644 , Accuracy = 0.7193\n",
            "Epoch 289 : for modelA 1 ==> Training loss= 0.0666 , Validation loss= 0.0650 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 289 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0646 , Average loss= 0.0639 , Accuracy = 0.7193\n",
            "Epoch 289 : for modelA 3 ==> Training loss= 0.0666 , Validation loss= 0.0643 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 289 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0646 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 290 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0644 , Average loss= 0.0644 , Accuracy = 0.7193\n",
            "Epoch 290 : for modelA 1 ==> Training loss= 0.0671 , Validation loss= 0.0646 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 290 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0646 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 290 : for modelA 3 ==> Training loss= 0.0666 , Validation loss= 0.0643 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 290 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0646 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 291 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0647 , Average loss= 0.0644 , Accuracy = 0.7193\n",
            "Epoch 291 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0646 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 291 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0646 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 291 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0654 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 291 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0646 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 292 : for modelA 0 ==> Training loss= 0.0667 , Validation loss= 0.0648 , Average loss= 0.0648 , Accuracy = 0.7193\n",
            "Epoch 292 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0639 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 292 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0646 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 292 : for modelA 3 ==> Training loss= 0.0671 , Validation loss= 0.0642 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 292 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0646 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 293 : for modelA 0 ==> Training loss= 0.0671 , Validation loss= 0.0647 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 293 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0642 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 293 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0650 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 293 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 293 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0646 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 294 : for modelA 0 ==> Training loss= 0.0672 , Validation loss= 0.0644 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 294 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0650 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 294 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0650 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 294 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0646 , Average loss= 0.0639 , Accuracy = 0.7193\n",
            "Epoch 294 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0638 , Average loss= 0.0638 , Accuracy = 0.7193\n",
            "Epoch 295 : for modelA 0 ==> Training loss= 0.0666 , Validation loss= 0.0647 , Average loss= 0.0640 , Accuracy = 0.7193\n",
            "Epoch 295 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0642 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 295 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0650 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 295 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0642 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 295 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0642 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 296 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0662 , Average loss= 0.0655 , Accuracy = 0.7193\n",
            "Epoch 296 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0654 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 296 : for modelA 2 ==> Training loss= 0.0666 , Validation loss= 0.0646 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 296 : for modelA 3 ==> Training loss= 0.0671 , Validation loss= 0.0646 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 296 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0646 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 297 : for modelA 0 ==> Training loss= 0.0666 , Validation loss= 0.0648 , Average loss= 0.0651 , Accuracy = 0.7193\n",
            "Epoch 297 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0654 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 297 : for modelA 2 ==> Training loss= 0.0671 , Validation loss= 0.0646 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 297 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0646 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 297 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 298 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0647 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 298 : for modelA 1 ==> Training loss= 0.0666 , Validation loss= 0.0650 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 298 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0646 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 298 : for modelA 3 ==> Training loss= 0.0671 , Validation loss= 0.0646 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 298 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0642 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 299 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0647 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 299 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0646 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 299 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0642 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 299 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0642 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 299 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0642 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 300 : for modelA 0 ==> Training loss= 0.0670 , Validation loss= 0.0651 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 300 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 300 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0654 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 300 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0642 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 300 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0642 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 301 : for modelA 0 ==> Training loss= 0.0666 , Validation loss= 0.0647 , Average loss= 0.0651 , Accuracy = 0.7193\n",
            "Epoch 301 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0646 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 301 : for modelA 2 ==> Training loss= 0.0671 , Validation loss= 0.0638 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 301 : for modelA 3 ==> Training loss= 0.0666 , Validation loss= 0.0642 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 301 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0654 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 302 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0640 , Average loss= 0.0644 , Accuracy = 0.7193\n",
            "Epoch 302 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0642 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 302 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0646 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 302 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0654 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 302 : for modelA 4 ==> Training loss= 0.0671 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 303 : for modelA 0 ==> Training loss= 0.0667 , Validation loss= 0.0647 , Average loss= 0.0655 , Accuracy = 0.7193\n",
            "Epoch 303 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 303 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0642 , Average loss= 0.0638 , Accuracy = 0.7193\n",
            "Epoch 303 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0650 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 303 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0638 , Accuracy = 0.7193\n",
            "Epoch 304 : for modelA 0 ==> Training loss= 0.0666 , Validation loss= 0.0651 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 304 : for modelA 1 ==> Training loss= 0.0666 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 304 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0650 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 304 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0646 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 304 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0646 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 305 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0651 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 305 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0657 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 305 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0650 , Average loss= 0.0657 , Accuracy = 0.7193\n",
            "Epoch 305 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0654 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 305 : for modelA 4 ==> Training loss= 0.0666 , Validation loss= 0.0646 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 306 : for modelA 0 ==> Training loss= 0.0666 , Validation loss= 0.0640 , Average loss= 0.0651 , Accuracy = 0.7193\n",
            "Epoch 306 : for modelA 1 ==> Training loss= 0.0666 , Validation loss= 0.0654 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 306 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0642 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 306 : for modelA 3 ==> Training loss= 0.0666 , Validation loss= 0.0642 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 306 : for modelA 4 ==> Training loss= 0.0671 , Validation loss= 0.0650 , Average loss= 0.0638 , Accuracy = 0.7193\n",
            "Epoch 307 : for modelA 0 ==> Training loss= 0.0667 , Validation loss= 0.0655 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 307 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0646 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 307 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0646 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 307 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0646 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 307 : for modelA 4 ==> Training loss= 0.0666 , Validation loss= 0.0650 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 308 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0643 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 308 : for modelA 1 ==> Training loss= 0.0666 , Validation loss= 0.0642 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 308 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0642 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 308 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0654 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 308 : for modelA 4 ==> Training loss= 0.0666 , Validation loss= 0.0650 , Average loss= 0.0638 , Accuracy = 0.7193\n",
            "Epoch 309 : for modelA 0 ==> Training loss= 0.0666 , Validation loss= 0.0643 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 309 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0642 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 309 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0658 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 309 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0646 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 309 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 310 : for modelA 0 ==> Training loss= 0.0667 , Validation loss= 0.0643 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 310 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0654 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 310 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 310 : for modelA 3 ==> Training loss= 0.0666 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 310 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0653 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 311 : for modelA 0 ==> Training loss= 0.0671 , Validation loss= 0.0651 , Average loss= 0.0651 , Accuracy = 0.7193\n",
            "Epoch 311 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0646 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 311 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0642 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 311 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0638 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 311 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0646 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 312 : for modelA 0 ==> Training loss= 0.0667 , Validation loss= 0.0647 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 312 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0650 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 312 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0642 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 312 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0650 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 312 : for modelA 4 ==> Training loss= 0.0666 , Validation loss= 0.0650 , Average loss= 0.0638 , Accuracy = 0.7193\n",
            "Epoch 313 : for modelA 0 ==> Training loss= 0.0667 , Validation loss= 0.0640 , Average loss= 0.0651 , Accuracy = 0.7193\n",
            "Epoch 313 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0642 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 313 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0646 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 313 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0646 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 313 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0646 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 314 : for modelA 0 ==> Training loss= 0.0667 , Validation loss= 0.0647 , Average loss= 0.0651 , Accuracy = 0.7193\n",
            "Epoch 314 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0650 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 314 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0646 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 314 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0646 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 314 : for modelA 4 ==> Training loss= 0.0671 , Validation loss= 0.0646 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 315 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0643 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 315 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0638 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 315 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0646 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 315 : for modelA 3 ==> Training loss= 0.0671 , Validation loss= 0.0646 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 315 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 316 : for modelA 0 ==> Training loss= 0.0667 , Validation loss= 0.0647 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 316 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0642 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 316 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 316 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0650 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 316 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0646 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 317 : for modelA 0 ==> Training loss= 0.0667 , Validation loss= 0.0643 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 317 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0646 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 317 : for modelA 2 ==> Training loss= 0.0671 , Validation loss= 0.0642 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 317 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 317 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0642 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 318 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0643 , Average loss= 0.0639 , Accuracy = 0.7193\n",
            "Epoch 318 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0638 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 318 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0650 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 318 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0638 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 318 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0653 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 319 : for modelA 0 ==> Training loss= 0.0666 , Validation loss= 0.0643 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 319 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0650 , Average loss= 0.0638 , Accuracy = 0.7193\n",
            "Epoch 319 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0638 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 319 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 319 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0650 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 320 : for modelA 0 ==> Training loss= 0.0666 , Validation loss= 0.0643 , Average loss= 0.0639 , Accuracy = 0.7193\n",
            "Epoch 320 : for modelA 1 ==> Training loss= 0.0666 , Validation loss= 0.0653 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 320 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0642 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 320 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0646 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 320 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0638 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 321 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0651 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 321 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0642 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 321 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0646 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 321 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0650 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 321 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0646 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 322 : for modelA 0 ==> Training loss= 0.0667 , Validation loss= 0.0643 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 322 : for modelA 1 ==> Training loss= 0.0666 , Validation loss= 0.0646 , Average loss= 0.0638 , Accuracy = 0.7193\n",
            "Epoch 322 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0646 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 322 : for modelA 3 ==> Training loss= 0.0666 , Validation loss= 0.0646 , Average loss= 0.0638 , Accuracy = 0.7193\n",
            "Epoch 322 : for modelA 4 ==> Training loss= 0.0671 , Validation loss= 0.0646 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 323 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0647 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 323 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0653 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 323 : for modelA 2 ==> Training loss= 0.0666 , Validation loss= 0.0646 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 323 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0646 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 323 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0653 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 324 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0647 , Average loss= 0.0639 , Accuracy = 0.7193\n",
            "Epoch 324 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0646 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 324 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0642 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 324 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0654 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 324 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0642 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 325 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0647 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 325 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0646 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 325 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0646 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 325 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0650 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 325 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0650 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 326 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0639 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 326 : for modelA 1 ==> Training loss= 0.0671 , Validation loss= 0.0642 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 326 : for modelA 2 ==> Training loss= 0.0671 , Validation loss= 0.0642 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 326 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0646 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 326 : for modelA 4 ==> Training loss= 0.0666 , Validation loss= 0.0642 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 327 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0654 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 327 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0646 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 327 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0650 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 327 : for modelA 3 ==> Training loss= 0.0666 , Validation loss= 0.0642 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 327 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0646 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 328 : for modelA 0 ==> Training loss= 0.0667 , Validation loss= 0.0643 , Average loss= 0.0639 , Accuracy = 0.7193\n",
            "Epoch 328 : for modelA 1 ==> Training loss= 0.0671 , Validation loss= 0.0657 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 328 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0642 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 328 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0654 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 328 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0638 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 329 : for modelA 0 ==> Training loss= 0.0667 , Validation loss= 0.0643 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 329 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0642 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 329 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0646 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 329 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0650 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 329 : for modelA 4 ==> Training loss= 0.0666 , Validation loss= 0.0642 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 330 : for modelA 0 ==> Training loss= 0.0667 , Validation loss= 0.0647 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 330 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0642 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 330 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0654 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 330 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0642 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 330 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0646 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 331 : for modelA 0 ==> Training loss= 0.0672 , Validation loss= 0.0650 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 331 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0642 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 331 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0646 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 331 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0650 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 331 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0646 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 332 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0647 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 332 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0650 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 332 : for modelA 2 ==> Training loss= 0.0666 , Validation loss= 0.0642 , Average loss= 0.0638 , Accuracy = 0.7193\n",
            "Epoch 332 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0646 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 332 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0646 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 333 : for modelA 0 ==> Training loss= 0.0667 , Validation loss= 0.0643 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 333 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0646 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 333 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0650 , Average loss= 0.0638 , Accuracy = 0.7193\n",
            "Epoch 333 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0646 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 333 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0646 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 334 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0643 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 334 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0646 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 334 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0642 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 334 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 334 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0645 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 335 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0646 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 335 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0638 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 335 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 335 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0642 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 335 : for modelA 4 ==> Training loss= 0.0671 , Validation loss= 0.0642 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 336 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0647 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 336 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0650 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 336 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0646 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 336 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0653 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 336 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0645 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 337 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0647 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 337 : for modelA 1 ==> Training loss= 0.0671 , Validation loss= 0.0646 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 337 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0646 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 337 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0642 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 337 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0649 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 338 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0647 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 338 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0650 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 338 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0642 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 338 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0642 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 338 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0642 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 339 : for modelA 0 ==> Training loss= 0.0667 , Validation loss= 0.0650 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 339 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0646 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 339 : for modelA 2 ==> Training loss= 0.0666 , Validation loss= 0.0650 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 339 : for modelA 3 ==> Training loss= 0.0666 , Validation loss= 0.0646 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 339 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0645 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 340 : for modelA 0 ==> Training loss= 0.0667 , Validation loss= 0.0646 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 340 : for modelA 1 ==> Training loss= 0.0671 , Validation loss= 0.0638 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 340 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0646 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 340 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0646 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 340 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0645 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 341 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0654 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 341 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0646 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 341 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0642 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 341 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0650 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 341 : for modelA 4 ==> Training loss= 0.0666 , Validation loss= 0.0649 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 342 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0643 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 342 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0638 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 342 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0642 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 342 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0646 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 342 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0649 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 343 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0643 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 343 : for modelA 1 ==> Training loss= 0.0666 , Validation loss= 0.0642 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 343 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0642 , Average loss= 0.0638 , Accuracy = 0.7193\n",
            "Epoch 343 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 343 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0641 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 344 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0646 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 344 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0646 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 344 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0650 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 344 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0642 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 344 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0642 , Average loss= 0.0638 , Accuracy = 0.7193\n",
            "Epoch 345 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0643 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 345 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0653 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 345 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0649 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 345 : for modelA 3 ==> Training loss= 0.0672 , Validation loss= 0.0646 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 345 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0642 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 346 : for modelA 0 ==> Training loss= 0.0667 , Validation loss= 0.0643 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 346 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0649 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 346 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0649 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 346 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0642 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 346 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0645 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 347 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0650 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 347 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0642 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 347 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0642 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 347 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0642 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 347 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0657 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 348 : for modelA 0 ==> Training loss= 0.0667 , Validation loss= 0.0643 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 348 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0642 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 348 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0642 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 348 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 348 : for modelA 4 ==> Training loss= 0.0666 , Validation loss= 0.0653 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 349 : for modelA 0 ==> Training loss= 0.0667 , Validation loss= 0.0650 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 349 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0642 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 349 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0642 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 349 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0646 , Average loss= 0.0657 , Accuracy = 0.7193\n",
            "Epoch 349 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0645 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 350 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0650 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 350 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0642 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 350 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0642 , Average loss= 0.0657 , Accuracy = 0.7193\n",
            "Epoch 350 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0646 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 350 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0657 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 351 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0643 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 351 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0638 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 351 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0646 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 351 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0646 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 351 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0645 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 352 : for modelA 0 ==> Training loss= 0.0667 , Validation loss= 0.0639 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 352 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0649 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 352 : for modelA 2 ==> Training loss= 0.0666 , Validation loss= 0.0646 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 352 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0646 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 352 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0657 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 353 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0646 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 353 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0642 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 353 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0649 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 353 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0646 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 353 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0641 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 354 : for modelA 0 ==> Training loss= 0.0667 , Validation loss= 0.0650 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 354 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0649 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 354 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0649 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 354 : for modelA 3 ==> Training loss= 0.0666 , Validation loss= 0.0642 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 354 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0642 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 355 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0642 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 355 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0642 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 355 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0646 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 355 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0646 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 355 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0645 , Average loss= 0.0641 , Accuracy = 0.7193\n",
            "Epoch 356 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0650 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 356 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0649 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 356 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0650 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 356 : for modelA 3 ==> Training loss= 0.0666 , Validation loss= 0.0646 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 356 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0645 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 357 : for modelA 0 ==> Training loss= 0.0666 , Validation loss= 0.0642 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 357 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0649 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 357 : for modelA 2 ==> Training loss= 0.0672 , Validation loss= 0.0642 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 357 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0646 , Average loss= 0.0638 , Accuracy = 0.7193\n",
            "Epoch 357 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0645 , Average loss= 0.0641 , Accuracy = 0.7193\n",
            "Epoch 358 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0654 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 358 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0646 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 358 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0642 , Average loss= 0.0638 , Accuracy = 0.7193\n",
            "Epoch 358 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0649 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 358 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0649 , Average loss= 0.0641 , Accuracy = 0.7193\n",
            "Epoch 359 : for modelA 0 ==> Training loss= 0.0667 , Validation loss= 0.0642 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 359 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0642 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 359 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0653 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 359 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0638 , Average loss= 0.0638 , Accuracy = 0.7193\n",
            "Epoch 359 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0653 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 360 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0654 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 360 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0642 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 360 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0642 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 360 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0646 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 360 : for modelA 4 ==> Training loss= 0.0666 , Validation loss= 0.0649 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 361 : for modelA 0 ==> Training loss= 0.0667 , Validation loss= 0.0642 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 361 : for modelA 1 ==> Training loss= 0.0666 , Validation loss= 0.0646 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 361 : for modelA 2 ==> Training loss= 0.0671 , Validation loss= 0.0646 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 361 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0649 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 361 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0645 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 362 : for modelA 0 ==> Training loss= 0.0667 , Validation loss= 0.0642 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 362 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0642 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 362 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 362 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0638 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 362 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0645 , Average loss= 0.0638 , Accuracy = 0.7193\n",
            "Epoch 363 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0654 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 363 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0642 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 363 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0646 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 363 : for modelA 3 ==> Training loss= 0.0672 , Validation loss= 0.0646 , Average loss= 0.0657 , Accuracy = 0.7193\n",
            "Epoch 363 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0645 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 364 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0658 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 364 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0638 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 364 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0649 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 364 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0649 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 364 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0645 , Average loss= 0.0638 , Accuracy = 0.7193\n",
            "Epoch 365 : for modelA 0 ==> Training loss= 0.0666 , Validation loss= 0.0639 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 365 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0645 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 365 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0642 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 365 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0638 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 365 : for modelA 4 ==> Training loss= 0.0666 , Validation loss= 0.0642 , Average loss= 0.0638 , Accuracy = 0.7193\n",
            "Epoch 366 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0642 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 366 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0645 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 366 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0642 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 366 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0653 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 366 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0645 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 367 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0642 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 367 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0649 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 367 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0646 , Average loss= 0.0638 , Accuracy = 0.7193\n",
            "Epoch 367 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0642 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 367 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0641 , Average loss= 0.0641 , Accuracy = 0.7193\n",
            "Epoch 368 : for modelA 0 ==> Training loss= 0.0666 , Validation loss= 0.0650 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 368 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0645 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 368 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0642 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 368 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0653 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 368 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0641 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 369 : for modelA 0 ==> Training loss= 0.0667 , Validation loss= 0.0638 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 369 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0649 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 369 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0646 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 369 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0649 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 369 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0649 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 370 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0646 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 370 : for modelA 1 ==> Training loss= 0.0666 , Validation loss= 0.0649 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 370 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0649 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 370 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0646 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 370 : for modelA 4 ==> Training loss= 0.0666 , Validation loss= 0.0645 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 371 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0646 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 371 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0649 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 371 : for modelA 2 ==> Training loss= 0.0666 , Validation loss= 0.0646 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 371 : for modelA 3 ==> Training loss= 0.0671 , Validation loss= 0.0646 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 371 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0638 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 372 : for modelA 0 ==> Training loss= 0.0667 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 372 : for modelA 1 ==> Training loss= 0.0666 , Validation loss= 0.0653 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 372 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0642 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 372 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0646 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 372 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0638 , Average loss= 0.0638 , Accuracy = 0.7193\n",
            "Epoch 373 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 373 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0645 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 373 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0649 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 373 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0646 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 373 : for modelA 4 ==> Training loss= 0.0666 , Validation loss= 0.0645 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 374 : for modelA 0 ==> Training loss= 0.0667 , Validation loss= 0.0646 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 374 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0645 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 374 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0646 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 374 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0642 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 374 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0645 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 375 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0638 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 375 : for modelA 1 ==> Training loss= 0.0671 , Validation loss= 0.0641 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 375 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0646 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 375 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0642 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 375 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0638 , Average loss= 0.0641 , Accuracy = 0.7193\n",
            "Epoch 376 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 376 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0638 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 376 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0642 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 376 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0638 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 376 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0645 , Average loss= 0.0641 , Accuracy = 0.7193\n",
            "Epoch 377 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0646 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 377 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0653 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 377 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0638 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 377 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0649 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 377 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0645 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 378 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0646 , Average loss= 0.0638 , Accuracy = 0.7193\n",
            "Epoch 378 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0653 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 378 : for modelA 2 ==> Training loss= 0.0666 , Validation loss= 0.0646 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 378 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0649 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 378 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0649 , Average loss= 0.0641 , Accuracy = 0.7193\n",
            "Epoch 379 : for modelA 0 ==> Training loss= 0.0666 , Validation loss= 0.0646 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 379 : for modelA 1 ==> Training loss= 0.0671 , Validation loss= 0.0653 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 379 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0642 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 379 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0642 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 379 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0653 , Average loss= 0.0641 , Accuracy = 0.7193\n",
            "Epoch 380 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0654 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 380 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0653 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 380 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0653 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 380 : for modelA 3 ==> Training loss= 0.0671 , Validation loss= 0.0646 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 380 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0645 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 381 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0650 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 381 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0642 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 381 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0642 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 381 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0642 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 381 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0645 , Average loss= 0.0641 , Accuracy = 0.7193\n",
            "Epoch 382 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0646 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 382 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0645 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 382 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0638 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 382 : for modelA 3 ==> Training loss= 0.0666 , Validation loss= 0.0646 , Average loss= 0.0638 , Accuracy = 0.7193\n",
            "Epoch 382 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0645 , Average loss= 0.0638 , Accuracy = 0.7193\n",
            "Epoch 383 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 383 : for modelA 1 ==> Training loss= 0.0666 , Validation loss= 0.0638 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 383 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0649 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 383 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0642 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 383 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0653 , Average loss= 0.0641 , Accuracy = 0.7193\n",
            "Epoch 384 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0650 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 384 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0642 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 384 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0649 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 384 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0646 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 384 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0657 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 385 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0654 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 385 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0653 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 385 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0645 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 385 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0646 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 385 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0645 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 386 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0646 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 386 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0642 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 386 : for modelA 2 ==> Training loss= 0.0672 , Validation loss= 0.0646 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 386 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0646 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 386 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0645 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 387 : for modelA 0 ==> Training loss= 0.0666 , Validation loss= 0.0646 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 387 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0645 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 387 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0649 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 387 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0642 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 387 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0645 , Average loss= 0.0637 , Accuracy = 0.7193\n",
            "Epoch 388 : for modelA 0 ==> Training loss= 0.0667 , Validation loss= 0.0654 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 388 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0642 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 388 : for modelA 2 ==> Training loss= 0.0671 , Validation loss= 0.0646 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 388 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0649 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 388 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0645 , Average loss= 0.0641 , Accuracy = 0.7193\n",
            "Epoch 389 : for modelA 0 ==> Training loss= 0.0667 , Validation loss= 0.0646 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 389 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0645 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 389 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0646 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 389 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0646 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 389 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0638 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 390 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0646 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 390 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0642 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 390 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0642 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 390 : for modelA 3 ==> Training loss= 0.0671 , Validation loss= 0.0653 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 390 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0645 , Average loss= 0.0641 , Accuracy = 0.7193\n",
            "Epoch 391 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0646 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 391 : for modelA 1 ==> Training loss= 0.0671 , Validation loss= 0.0649 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 391 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0657 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 391 : for modelA 3 ==> Training loss= 0.0666 , Validation loss= 0.0646 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 391 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0649 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 392 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0646 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 392 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0649 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 392 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0642 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 392 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0642 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 392 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0641 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 393 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 393 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0645 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 393 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0646 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 393 : for modelA 3 ==> Training loss= 0.0671 , Validation loss= 0.0649 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 393 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0641 , Average loss= 0.0641 , Accuracy = 0.7193\n",
            "Epoch 394 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0654 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 394 : for modelA 1 ==> Training loss= 0.0666 , Validation loss= 0.0645 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 394 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0642 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 394 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0642 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 394 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0649 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 395 : for modelA 0 ==> Training loss= 0.0667 , Validation loss= 0.0646 , Average loss= 0.0638 , Accuracy = 0.7193\n",
            "Epoch 395 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0653 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 395 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0653 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 395 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0646 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 395 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0645 , Average loss= 0.0638 , Accuracy = 0.7193\n",
            "Epoch 396 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0638 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 396 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0645 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 396 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0642 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 396 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0649 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 396 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0641 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 397 : for modelA 0 ==> Training loss= 0.0666 , Validation loss= 0.0642 , Average loss= 0.0638 , Accuracy = 0.7193\n",
            "Epoch 397 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0649 , Average loss= 0.0641 , Accuracy = 0.7193\n",
            "Epoch 397 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0653 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 397 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0638 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 397 : for modelA 4 ==> Training loss= 0.0666 , Validation loss= 0.0645 , Average loss= 0.0641 , Accuracy = 0.7193\n",
            "Epoch 398 : for modelA 0 ==> Training loss= 0.0667 , Validation loss= 0.0646 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 398 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0649 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 398 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0653 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 398 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0646 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 398 : for modelA 4 ==> Training loss= 0.0666 , Validation loss= 0.0638 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 399 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0642 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 399 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0649 , Average loss= 0.0641 , Accuracy = 0.7193\n",
            "Epoch 399 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0646 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 399 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0645 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 399 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0641 , Average loss= 0.0641 , Accuracy = 0.7193\n",
            "Epoch 400 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0642 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 400 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0645 , Average loss= 0.0638 , Accuracy = 0.7193\n",
            "Epoch 400 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0653 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 400 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0649 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 400 : for modelA 4 ==> Training loss= 0.0666 , Validation loss= 0.0645 , Average loss= 0.0638 , Accuracy = 0.7193\n",
            "Epoch 401 : for modelA 0 ==> Training loss= 0.0667 , Validation loss= 0.0661 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 401 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0645 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 401 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0638 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 401 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0638 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 401 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0657 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 402 : for modelA 0 ==> Training loss= 0.0666 , Validation loss= 0.0642 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 402 : for modelA 1 ==> Training loss= 0.0671 , Validation loss= 0.0645 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 402 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0642 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 402 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0646 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 402 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0649 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 403 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0646 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 403 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0649 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 403 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0642 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 403 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0649 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 403 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0641 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 404 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0646 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 404 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0642 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 404 : for modelA 2 ==> Training loss= 0.0666 , Validation loss= 0.0649 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 404 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0646 , Average loss= 0.0638 , Accuracy = 0.7193\n",
            "Epoch 404 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0649 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 405 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0646 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 405 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0645 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 405 : for modelA 2 ==> Training loss= 0.0666 , Validation loss= 0.0645 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 405 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0649 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 405 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0641 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 406 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 406 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0645 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 406 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0642 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 406 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0642 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 406 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0645 , Average loss= 0.0641 , Accuracy = 0.7193\n",
            "Epoch 407 : for modelA 0 ==> Training loss= 0.0667 , Validation loss= 0.0642 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 407 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0645 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 407 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0645 , Average loss= 0.0638 , Accuracy = 0.7193\n",
            "Epoch 407 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0649 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 407 : for modelA 4 ==> Training loss= 0.0666 , Validation loss= 0.0641 , Average loss= 0.0657 , Accuracy = 0.7193\n",
            "Epoch 408 : for modelA 0 ==> Training loss= 0.0667 , Validation loss= 0.0642 , Average loss= 0.0638 , Accuracy = 0.7193\n",
            "Epoch 408 : for modelA 1 ==> Training loss= 0.0666 , Validation loss= 0.0645 , Average loss= 0.0657 , Accuracy = 0.7193\n",
            "Epoch 408 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0642 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 408 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0642 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 408 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0645 , Average loss= 0.0657 , Accuracy = 0.7193\n",
            "Epoch 409 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0646 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 409 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0645 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 409 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0649 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 409 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0642 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 409 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0649 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 410 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 410 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0641 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 410 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0649 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 410 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0642 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 410 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0649 , Average loss= 0.0641 , Accuracy = 0.7193\n",
            "Epoch 411 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0650 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 411 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0645 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 411 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0642 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 411 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0645 , Average loss= 0.0638 , Accuracy = 0.7193\n",
            "Epoch 411 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0653 , Average loss= 0.0637 , Accuracy = 0.7193\n",
            "Epoch 412 : for modelA 0 ==> Training loss= 0.0666 , Validation loss= 0.0646 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 412 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0645 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 412 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0649 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 412 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0642 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 412 : for modelA 4 ==> Training loss= 0.0666 , Validation loss= 0.0645 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 413 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0646 , Average loss= 0.0638 , Accuracy = 0.7193\n",
            "Epoch 413 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0649 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 413 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0642 , Average loss= 0.0638 , Accuracy = 0.7193\n",
            "Epoch 413 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0642 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 413 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0645 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 414 : for modelA 0 ==> Training loss= 0.0667 , Validation loss= 0.0653 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 414 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0649 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 414 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0653 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 414 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0645 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 414 : for modelA 4 ==> Training loss= 0.0666 , Validation loss= 0.0645 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 415 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0642 , Average loss= 0.0638 , Accuracy = 0.7193\n",
            "Epoch 415 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0645 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 415 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0645 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 415 : for modelA 3 ==> Training loss= 0.0671 , Validation loss= 0.0645 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 415 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0653 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 416 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0653 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 416 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0642 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 416 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0646 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 416 : for modelA 3 ==> Training loss= 0.0671 , Validation loss= 0.0645 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 416 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0641 , Average loss= 0.0641 , Accuracy = 0.7193\n",
            "Epoch 417 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0646 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 417 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0645 , Average loss= 0.0641 , Accuracy = 0.7193\n",
            "Epoch 417 : for modelA 2 ==> Training loss= 0.0666 , Validation loss= 0.0642 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 417 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0638 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 417 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0649 , Average loss= 0.0641 , Accuracy = 0.7193\n",
            "Epoch 418 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0642 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 418 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0649 , Average loss= 0.0638 , Accuracy = 0.7193\n",
            "Epoch 418 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0649 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 418 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0642 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 418 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0637 , Average loss= 0.0637 , Accuracy = 0.7193\n",
            "Epoch 419 : for modelA 0 ==> Training loss= 0.0666 , Validation loss= 0.0642 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 419 : for modelA 1 ==> Training loss= 0.0671 , Validation loss= 0.0642 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 419 : for modelA 2 ==> Training loss= 0.0671 , Validation loss= 0.0645 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 419 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0642 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 419 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0645 , Average loss= 0.0637 , Accuracy = 0.7193\n",
            "Epoch 420 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0646 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 420 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0649 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 420 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0645 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 420 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0642 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 420 : for modelA 4 ==> Training loss= 0.0672 , Validation loss= 0.0641 , Average loss= 0.0641 , Accuracy = 0.7193\n",
            "Epoch 421 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 421 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0661 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 421 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0645 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 421 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0642 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 421 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0645 , Average loss= 0.0641 , Accuracy = 0.7193\n",
            "Epoch 422 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0638 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 422 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0645 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 422 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0649 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 422 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0642 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 422 : for modelA 4 ==> Training loss= 0.0666 , Validation loss= 0.0641 , Average loss= 0.0641 , Accuracy = 0.7193\n",
            "Epoch 423 : for modelA 0 ==> Training loss= 0.0667 , Validation loss= 0.0646 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 423 : for modelA 1 ==> Training loss= 0.0666 , Validation loss= 0.0649 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 423 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0641 , Average loss= 0.0641 , Accuracy = 0.7193\n",
            "Epoch 423 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0645 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 423 : for modelA 4 ==> Training loss= 0.0671 , Validation loss= 0.0641 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 424 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0650 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 424 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0649 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 424 : for modelA 2 ==> Training loss= 0.0666 , Validation loss= 0.0645 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 424 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0642 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 424 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0653 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 425 : for modelA 0 ==> Training loss= 0.0667 , Validation loss= 0.0646 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 425 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0638 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 425 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0645 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 425 : for modelA 3 ==> Training loss= 0.0671 , Validation loss= 0.0645 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 425 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0645 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 426 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0649 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 426 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0638 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 426 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0645 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 426 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0642 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 426 : for modelA 4 ==> Training loss= 0.0666 , Validation loss= 0.0641 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 427 : for modelA 0 ==> Training loss= 0.0667 , Validation loss= 0.0642 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 427 : for modelA 1 ==> Training loss= 0.0666 , Validation loss= 0.0638 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 427 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0641 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 427 : for modelA 3 ==> Training loss= 0.0671 , Validation loss= 0.0649 , Average loss= 0.0657 , Accuracy = 0.7193\n",
            "Epoch 427 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0649 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 428 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0649 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 428 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0653 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 428 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0653 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 428 : for modelA 3 ==> Training loss= 0.0671 , Validation loss= 0.0649 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 428 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0645 , Average loss= 0.0641 , Accuracy = 0.7193\n",
            "Epoch 429 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0646 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 429 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0645 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 429 : for modelA 2 ==> Training loss= 0.0672 , Validation loss= 0.0641 , Average loss= 0.0641 , Accuracy = 0.7193\n",
            "Epoch 429 : for modelA 3 ==> Training loss= 0.0671 , Validation loss= 0.0649 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 429 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0645 , Average loss= 0.0657 , Accuracy = 0.7193\n",
            "Epoch 430 : for modelA 0 ==> Training loss= 0.0666 , Validation loss= 0.0646 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 430 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0645 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 430 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0645 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 430 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0642 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 430 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0645 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 431 : for modelA 0 ==> Training loss= 0.0666 , Validation loss= 0.0642 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 431 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0645 , Average loss= 0.0638 , Accuracy = 0.7193\n",
            "Epoch 431 : for modelA 2 ==> Training loss= 0.0666 , Validation loss= 0.0642 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 431 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0649 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 431 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0641 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 432 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0642 , Average loss= 0.0638 , Accuracy = 0.7193\n",
            "Epoch 432 : for modelA 1 ==> Training loss= 0.0672 , Validation loss= 0.0641 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 432 : for modelA 2 ==> Training loss= 0.0671 , Validation loss= 0.0641 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 432 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0642 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 432 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0645 , Average loss= 0.0637 , Accuracy = 0.7193\n",
            "Epoch 433 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0642 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 433 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0642 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 433 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0645 , Average loss= 0.0638 , Accuracy = 0.7193\n",
            "Epoch 433 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0642 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 433 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0645 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 434 : for modelA 0 ==> Training loss= 0.0667 , Validation loss= 0.0642 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 434 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0642 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 434 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0642 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 434 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0645 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 434 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0641 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 435 : for modelA 0 ==> Training loss= 0.0667 , Validation loss= 0.0646 , Average loss= 0.0638 , Accuracy = 0.7193\n",
            "Epoch 435 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0645 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 435 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0645 , Average loss= 0.0638 , Accuracy = 0.7193\n",
            "Epoch 435 : for modelA 3 ==> Training loss= 0.0671 , Validation loss= 0.0649 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 435 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0649 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 436 : for modelA 0 ==> Training loss= 0.0671 , Validation loss= 0.0649 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 436 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0645 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 436 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0645 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 436 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0642 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 436 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0645 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 437 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0661 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 437 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0645 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 437 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0645 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 437 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0649 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 437 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0645 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 438 : for modelA 0 ==> Training loss= 0.0666 , Validation loss= 0.0646 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 438 : for modelA 1 ==> Training loss= 0.0666 , Validation loss= 0.0649 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 438 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0653 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 438 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0645 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 438 : for modelA 4 ==> Training loss= 0.0666 , Validation loss= 0.0649 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 439 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0649 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 439 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0645 , Average loss= 0.0638 , Accuracy = 0.7193\n",
            "Epoch 439 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0645 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 439 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0657 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 439 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0641 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 440 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0649 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 440 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0645 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 440 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0638 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 440 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0649 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 440 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0649 , Average loss= 0.0641 , Accuracy = 0.7193\n",
            "Epoch 441 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0642 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 441 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0649 , Average loss= 0.0641 , Accuracy = 0.7193\n",
            "Epoch 441 : for modelA 2 ==> Training loss= 0.0666 , Validation loss= 0.0642 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 441 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0645 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 441 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0653 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 442 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0642 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 442 : for modelA 1 ==> Training loss= 0.0666 , Validation loss= 0.0638 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 442 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0642 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 442 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0642 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 442 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0645 , Average loss= 0.0641 , Accuracy = 0.7193\n",
            "Epoch 443 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0638 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 443 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0641 , Average loss= 0.0638 , Accuracy = 0.7193\n",
            "Epoch 443 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0653 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 443 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0645 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 443 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0649 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 444 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0646 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 444 : for modelA 1 ==> Training loss= 0.0666 , Validation loss= 0.0641 , Average loss= 0.0641 , Accuracy = 0.7193\n",
            "Epoch 444 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0642 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 444 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0638 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 444 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0649 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 445 : for modelA 0 ==> Training loss= 0.0667 , Validation loss= 0.0653 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 445 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0657 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 445 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0645 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 445 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0642 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 445 : for modelA 4 ==> Training loss= 0.0671 , Validation loss= 0.0645 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 446 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0646 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 446 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0649 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 446 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0649 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 446 : for modelA 3 ==> Training loss= 0.0671 , Validation loss= 0.0653 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 446 : for modelA 4 ==> Training loss= 0.0666 , Validation loss= 0.0641 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 447 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0642 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 447 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0649 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 447 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0653 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 447 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0646 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 447 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0649 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 448 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0649 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 448 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0641 , Average loss= 0.0638 , Accuracy = 0.7193\n",
            "Epoch 448 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0645 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 448 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0657 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 448 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0645 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 449 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0646 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 449 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0649 , Average loss= 0.0641 , Accuracy = 0.7193\n",
            "Epoch 449 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0645 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 449 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0646 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 449 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0641 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 450 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0646 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 450 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0649 , Average loss= 0.0657 , Accuracy = 0.7193\n",
            "Epoch 450 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0641 , Average loss= 0.0638 , Accuracy = 0.7193\n",
            "Epoch 450 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0645 , Average loss= 0.0638 , Accuracy = 0.7193\n",
            "Epoch 450 : for modelA 4 ==> Training loss= 0.0666 , Validation loss= 0.0649 , Average loss= 0.0641 , Accuracy = 0.7193\n",
            "Epoch 451 : for modelA 0 ==> Training loss= 0.0667 , Validation loss= 0.0649 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 451 : for modelA 1 ==> Training loss= 0.0672 , Validation loss= 0.0645 , Average loss= 0.0641 , Accuracy = 0.7193\n",
            "Epoch 451 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0645 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 451 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0649 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 451 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0641 , Average loss= 0.0637 , Accuracy = 0.7193\n",
            "Epoch 452 : for modelA 0 ==> Training loss= 0.0667 , Validation loss= 0.0649 , Average loss= 0.0638 , Accuracy = 0.7193\n",
            "Epoch 452 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0653 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 452 : for modelA 2 ==> Training loss= 0.0666 , Validation loss= 0.0641 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 452 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0642 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 452 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0645 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 453 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0638 , Average loss= 0.0638 , Accuracy = 0.7193\n",
            "Epoch 453 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0645 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 453 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0641 , Average loss= 0.0641 , Accuracy = 0.7193\n",
            "Epoch 453 : for modelA 3 ==> Training loss= 0.0671 , Validation loss= 0.0649 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 453 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0645 , Average loss= 0.0641 , Accuracy = 0.7193\n",
            "Epoch 454 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0642 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 454 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0645 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 454 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0649 , Average loss= 0.0641 , Accuracy = 0.7193\n",
            "Epoch 454 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0653 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 454 : for modelA 4 ==> Training loss= 0.0666 , Validation loss= 0.0641 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 455 : for modelA 0 ==> Training loss= 0.0671 , Validation loss= 0.0645 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 455 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0649 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 455 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0638 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 455 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0642 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 455 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0649 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 456 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0646 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 456 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0645 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 456 : for modelA 2 ==> Training loss= 0.0666 , Validation loss= 0.0649 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 456 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0642 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 456 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0645 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 457 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0646 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 457 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0645 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 457 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0638 , Average loss= 0.0641 , Accuracy = 0.7193\n",
            "Epoch 457 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0649 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 457 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0649 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 458 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0642 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 458 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0649 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 458 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0645 , Average loss= 0.0641 , Accuracy = 0.7193\n",
            "Epoch 458 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0649 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 458 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0649 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 459 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0649 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 459 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0645 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 459 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0645 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 459 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0653 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 459 : for modelA 4 ==> Training loss= 0.0666 , Validation loss= 0.0645 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 460 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0653 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 460 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0642 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 460 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0645 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 460 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0653 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 460 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0653 , Average loss= 0.0641 , Accuracy = 0.7193\n",
            "Epoch 461 : for modelA 0 ==> Training loss= 0.0671 , Validation loss= 0.0649 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 461 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0645 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 461 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0653 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 461 : for modelA 3 ==> Training loss= 0.0666 , Validation loss= 0.0649 , Average loss= 0.0638 , Accuracy = 0.7193\n",
            "Epoch 461 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0645 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 462 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0646 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 462 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0645 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 462 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0641 , Average loss= 0.0638 , Accuracy = 0.7193\n",
            "Epoch 462 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0645 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 462 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0641 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 463 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0646 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 463 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0641 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 463 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0649 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 463 : for modelA 3 ==> Training loss= 0.0666 , Validation loss= 0.0649 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 463 : for modelA 4 ==> Training loss= 0.0666 , Validation loss= 0.0637 , Average loss= 0.0641 , Accuracy = 0.7193\n",
            "Epoch 464 : for modelA 0 ==> Training loss= 0.0667 , Validation loss= 0.0649 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 464 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0641 , Average loss= 0.0641 , Accuracy = 0.7193\n",
            "Epoch 464 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0645 , Average loss= 0.0641 , Accuracy = 0.7193\n",
            "Epoch 464 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0642 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 464 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0653 , Average loss= 0.0637 , Accuracy = 0.7193\n",
            "Epoch 465 : for modelA 0 ==> Training loss= 0.0671 , Validation loss= 0.0642 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 465 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0642 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 465 : for modelA 2 ==> Training loss= 0.0666 , Validation loss= 0.0645 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 465 : for modelA 3 ==> Training loss= 0.0666 , Validation loss= 0.0649 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 465 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0645 , Average loss= 0.0641 , Accuracy = 0.7193\n",
            "Epoch 466 : for modelA 0 ==> Training loss= 0.0667 , Validation loss= 0.0638 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 466 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0645 , Average loss= 0.0638 , Accuracy = 0.7193\n",
            "Epoch 466 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0645 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 466 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0645 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 466 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0637 , Average loss= 0.0641 , Accuracy = 0.7193\n",
            "Epoch 467 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0653 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 467 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0645 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 467 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0645 , Average loss= 0.0641 , Accuracy = 0.7193\n",
            "Epoch 467 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0645 , Average loss= 0.0638 , Accuracy = 0.7193\n",
            "Epoch 467 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0637 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 468 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0642 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 468 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0649 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 468 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0649 , Average loss= 0.0641 , Accuracy = 0.7193\n",
            "Epoch 468 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0645 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 468 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0653 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 469 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0653 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 469 : for modelA 1 ==> Training loss= 0.0666 , Validation loss= 0.0645 , Average loss= 0.0638 , Accuracy = 0.7193\n",
            "Epoch 469 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0649 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 469 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0653 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 469 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0649 , Average loss= 0.0637 , Accuracy = 0.7193\n",
            "Epoch 470 : for modelA 0 ==> Training loss= 0.0667 , Validation loss= 0.0649 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 470 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0649 , Average loss= 0.0638 , Accuracy = 0.7193\n",
            "Epoch 470 : for modelA 2 ==> Training loss= 0.0671 , Validation loss= 0.0649 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 470 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0645 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 470 : for modelA 4 ==> Training loss= 0.0672 , Validation loss= 0.0645 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 471 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0642 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 471 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0645 , Average loss= 0.0638 , Accuracy = 0.7193\n",
            "Epoch 471 : for modelA 2 ==> Training loss= 0.0671 , Validation loss= 0.0641 , Average loss= 0.0641 , Accuracy = 0.7193\n",
            "Epoch 471 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0638 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 471 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0649 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 472 : for modelA 0 ==> Training loss= 0.0667 , Validation loss= 0.0638 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 472 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0645 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 472 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0645 , Average loss= 0.0638 , Accuracy = 0.7193\n",
            "Epoch 472 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0638 , Average loss= 0.0638 , Accuracy = 0.7193\n",
            "Epoch 472 : for modelA 4 ==> Training loss= 0.0666 , Validation loss= 0.0649 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 473 : for modelA 0 ==> Training loss= 0.0669 , Validation loss= 0.0649 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 473 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0641 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 473 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0649 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 473 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0642 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 473 : for modelA 4 ==> Training loss= 0.0666 , Validation loss= 0.0649 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 474 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0646 , Average loss= 0.0638 , Accuracy = 0.7193\n",
            "Epoch 474 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0645 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 474 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0653 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 474 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0653 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 474 : for modelA 4 ==> Training loss= 0.0671 , Validation loss= 0.0649 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 475 : for modelA 0 ==> Training loss= 0.0667 , Validation loss= 0.0646 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 475 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0645 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 475 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0645 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 475 : for modelA 3 ==> Training loss= 0.0666 , Validation loss= 0.0645 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 475 : for modelA 4 ==> Training loss= 0.0666 , Validation loss= 0.0649 , Average loss= 0.0641 , Accuracy = 0.7193\n",
            "Epoch 476 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0638 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 476 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0642 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 476 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0657 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 476 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0649 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 476 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0657 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 477 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0653 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 477 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0645 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 477 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0645 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 477 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0649 , Average loss= 0.0638 , Accuracy = 0.7193\n",
            "Epoch 477 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0645 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 478 : for modelA 0 ==> Training loss= 0.0667 , Validation loss= 0.0649 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 478 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0645 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 478 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0642 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 478 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0649 , Average loss= 0.0641 , Accuracy = 0.7193\n",
            "Epoch 478 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0645 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 479 : for modelA 0 ==> Training loss= 0.0667 , Validation loss= 0.0646 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 479 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0649 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 479 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0653 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 479 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0645 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 479 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0645 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 480 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0649 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 480 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0645 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 480 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0653 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 480 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0642 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 480 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0645 , Average loss= 0.0641 , Accuracy = 0.7193\n",
            "Epoch 481 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0642 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 481 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0645 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 481 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0642 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 481 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0641 , Average loss= 0.0641 , Accuracy = 0.7193\n",
            "Epoch 481 : for modelA 4 ==> Training loss= 0.0666 , Validation loss= 0.0641 , Average loss= 0.0641 , Accuracy = 0.7193\n",
            "Epoch 482 : for modelA 0 ==> Training loss= 0.0671 , Validation loss= 0.0649 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 482 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0642 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 482 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0645 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 482 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0645 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 482 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0645 , Average loss= 0.0637 , Accuracy = 0.7193\n",
            "Epoch 483 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0649 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 483 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0645 , Average loss= 0.0638 , Accuracy = 0.7193\n",
            "Epoch 483 : for modelA 2 ==> Training loss= 0.0669 , Validation loss= 0.0642 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 483 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0645 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 483 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0649 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 484 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0649 , Average loss= 0.0638 , Accuracy = 0.7193\n",
            "Epoch 484 : for modelA 1 ==> Training loss= 0.0666 , Validation loss= 0.0645 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 484 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0642 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 484 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0642 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 484 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0645 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 485 : for modelA 0 ==> Training loss= 0.0667 , Validation loss= 0.0649 , Average loss= 0.0638 , Accuracy = 0.7193\n",
            "Epoch 485 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0642 , Average loss= 0.0638 , Accuracy = 0.7193\n",
            "Epoch 485 : for modelA 2 ==> Training loss= 0.0666 , Validation loss= 0.0645 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 485 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0645 , Average loss= 0.0638 , Accuracy = 0.7193\n",
            "Epoch 485 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0649 , Average loss= 0.0637 , Accuracy = 0.7193\n",
            "Epoch 486 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0638 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 486 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0645 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 486 : for modelA 2 ==> Training loss= 0.0666 , Validation loss= 0.0638 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 486 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0645 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 486 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0641 , Average loss= 0.0641 , Accuracy = 0.7193\n",
            "Epoch 487 : for modelA 0 ==> Training loss= 0.0666 , Validation loss= 0.0645 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 487 : for modelA 1 ==> Training loss= 0.0671 , Validation loss= 0.0645 , Average loss= 0.0638 , Accuracy = 0.7193\n",
            "Epoch 487 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0642 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 487 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0645 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 487 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0641 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 488 : for modelA 0 ==> Training loss= 0.0667 , Validation loss= 0.0638 , Average loss= 0.0638 , Accuracy = 0.7193\n",
            "Epoch 488 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0649 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 488 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0645 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 488 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0653 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 488 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0649 , Average loss= 0.0641 , Accuracy = 0.7193\n",
            "Epoch 489 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0649 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 489 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0642 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 489 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0661 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 489 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0653 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 489 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0653 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 490 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0645 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 490 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0645 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 490 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0638 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 490 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0642 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 490 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0645 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 491 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0638 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 491 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0642 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 491 : for modelA 2 ==> Training loss= 0.0671 , Validation loss= 0.0645 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 491 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0649 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 491 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0649 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 492 : for modelA 0 ==> Training loss= 0.0667 , Validation loss= 0.0645 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 492 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0638 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 492 : for modelA 2 ==> Training loss= 0.0666 , Validation loss= 0.0649 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 492 : for modelA 3 ==> Training loss= 0.0666 , Validation loss= 0.0638 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 492 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0641 , Average loss= 0.0641 , Accuracy = 0.7193\n",
            "Epoch 493 : for modelA 0 ==> Training loss= 0.0671 , Validation loss= 0.0649 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 493 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0638 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 493 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0653 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 493 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0649 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 493 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0641 , Average loss= 0.0641 , Accuracy = 0.7193\n",
            "Epoch 494 : for modelA 0 ==> Training loss= 0.0667 , Validation loss= 0.0646 , Average loss= 0.0646 , Accuracy = 0.7193\n",
            "Epoch 494 : for modelA 1 ==> Training loss= 0.0667 , Validation loss= 0.0653 , Average loss= 0.0638 , Accuracy = 0.7193\n",
            "Epoch 494 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0649 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 494 : for modelA 3 ==> Training loss= 0.0671 , Validation loss= 0.0649 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 494 : for modelA 4 ==> Training loss= 0.0671 , Validation loss= 0.0641 , Average loss= 0.0641 , Accuracy = 0.7193\n",
            "Epoch 495 : for modelA 0 ==> Training loss= 0.0668 , Validation loss= 0.0645 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 495 : for modelA 1 ==> Training loss= 0.0666 , Validation loss= 0.0642 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 495 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0642 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 495 : for modelA 3 ==> Training loss= 0.0667 , Validation loss= 0.0649 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 495 : for modelA 4 ==> Training loss= 0.0668 , Validation loss= 0.0649 , Average loss= 0.0641 , Accuracy = 0.7193\n",
            "Epoch 496 : for modelA 0 ==> Training loss= 0.0667 , Validation loss= 0.0649 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 496 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0645 , Average loss= 0.0641 , Accuracy = 0.7193\n",
            "Epoch 496 : for modelA 2 ==> Training loss= 0.0672 , Validation loss= 0.0645 , Average loss= 0.0641 , Accuracy = 0.7193\n",
            "Epoch 496 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0649 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 496 : for modelA 4 ==> Training loss= 0.0671 , Validation loss= 0.0641 , Average loss= 0.0637 , Accuracy = 0.7193\n",
            "Epoch 497 : for modelA 0 ==> Training loss= 0.0671 , Validation loss= 0.0638 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 497 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0642 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 497 : for modelA 2 ==> Training loss= 0.0668 , Validation loss= 0.0653 , Average loss= 0.0638 , Accuracy = 0.7193\n",
            "Epoch 497 : for modelA 3 ==> Training loss= 0.0668 , Validation loss= 0.0645 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 497 : for modelA 4 ==> Training loss= 0.0666 , Validation loss= 0.0641 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 498 : for modelA 0 ==> Training loss= 0.0666 , Validation loss= 0.0642 , Average loss= 0.0638 , Accuracy = 0.7193\n",
            "Epoch 498 : for modelA 1 ==> Training loss= 0.0669 , Validation loss= 0.0645 , Average loss= 0.0641 , Accuracy = 0.7193\n",
            "Epoch 498 : for modelA 2 ==> Training loss= 0.0666 , Validation loss= 0.0645 , Average loss= 0.0653 , Accuracy = 0.7193\n",
            "Epoch 498 : for modelA 3 ==> Training loss= 0.0671 , Validation loss= 0.0645 , Average loss= 0.0641 , Accuracy = 0.7193\n",
            "Epoch 498 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0645 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 499 : for modelA 0 ==> Training loss= 0.0672 , Validation loss= 0.0642 , Average loss= 0.0645 , Accuracy = 0.7193\n",
            "Epoch 499 : for modelA 1 ==> Training loss= 0.0668 , Validation loss= 0.0645 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 499 : for modelA 2 ==> Training loss= 0.0667 , Validation loss= 0.0649 , Average loss= 0.0649 , Accuracy = 0.7193\n",
            "Epoch 499 : for modelA 3 ==> Training loss= 0.0669 , Validation loss= 0.0653 , Average loss= 0.0642 , Accuracy = 0.7193\n",
            "Epoch 499 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0645 , Average loss= 0.0645 , Accuracy = 0.7193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt #python package to draw graphs in python\n",
        "\n",
        "plt.plot(epoch_nums, training_loss[0])\n",
        "plt.plot(epoch_nums, training_loss[1])\n",
        "plt.plot(epoch_nums, training_loss[2])\n",
        "plt.plot(epoch_nums, training_loss[3])\n",
        "plt.plot(epoch_nums, training_loss[4])\n",
        "\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('Training loss')\n",
        "plt.legend(['3 hidden layers', '9 hidden layers', '13 hidden layers', '17 hidden layers', '19 hidden layers'], loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "3QzkTmcwjQB_",
        "outputId": "677e1098-97b6-4125-b605-ec46148b9133"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(epoch_nums, validation_loss[0])\n",
        "plt.plot(epoch_nums, validation_loss[1])\n",
        "plt.plot(epoch_nums, validation_loss[2])\n",
        "plt.plot(epoch_nums, validation_loss[3])\n",
        "plt.plot(epoch_nums, validation_loss[4])\n",
        "\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('Valdiation loss')\n",
        "plt.legend(['3 hidden layers', '9 hidden layers', '13 hidden layers', '17 hidden layers', '19 hidden layers'], loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "id": "1HenRjj1klxl",
        "outputId": "f0d3e820-5612-4d16-b3ad-a21d745d92ed"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Xxw43hDyqTFV"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q9 Model B\n",
        "\n",
        "# Create Model A using active function Rellu"
      ],
      "metadata": {
        "id": "oqz-vrFZwWPf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ModelB(Input_size,Output_size,Hiden_layers,Node_Number):\n",
        "  layers =[nn.Linear(Input_size, Node_Number),nn.ReLU()]\n",
        "  for i in range (Hiden_layers):\n",
        "    layers.append(nn.Linear(Node_Number, Node_Number))\n",
        "    layers.append(nn.ReLU())\n",
        "\n",
        "  #for last layer\n",
        "  layers.append(nn.Linear(Node_Number, Output_size))\n",
        "  layers.append(nn.ReLU())\n",
        "\n",
        "  return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "model_B_3 = ModelB(30,2,3,5)\n",
        "model_B_3"
      ],
      "metadata": {
        "id": "fQCHPDrlwbcr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "353274a7-46b5-4c76-ac0b-9679630cd968"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=30, out_features=5, bias=True)\n",
              "  (1): ReLU()\n",
              "  (2): Linear(in_features=5, out_features=5, bias=True)\n",
              "  (3): ReLU()\n",
              "  (4): Linear(in_features=5, out_features=5, bias=True)\n",
              "  (5): ReLU()\n",
              "  (6): Linear(in_features=5, out_features=5, bias=True)\n",
              "  (7): ReLU()\n",
              "  (8): Linear(in_features=5, out_features=2, bias=True)\n",
              "  (9): ReLU()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "breast = load_breast_cancer()\n",
        "\n",
        "x,y=breast.data,breast.target\n",
        "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=1) # splitting for train and validation set\n",
        "\n",
        "x_test,x_val,y_test,y_val=train_test_split(x_test,y_test,test_size=0.5,random_state=1) # further validation set is splitted for test set\n",
        "\n",
        "batch_size=10,\n",
        "\n",
        "# Create a dataset and loader for the training data and labels\n",
        "train_x = torch.Tensor(x_train).float()\n",
        "train_y = torch.Tensor(y_train).long()\n",
        "train_ds = utils.TensorDataset(train_x,train_y)\n",
        "train_loader = td.DataLoader(train_ds, batch_size=10,shuffle=True)\n",
        "\n",
        "# Create a dataset and loader for the test data and labels\n",
        "test_x = torch.Tensor(x_test).float()\n",
        "test_y = torch.Tensor(y_test).long()\n",
        "test_ds = utils.TensorDataset(test_x,test_y)\n",
        "test_loader = td.DataLoader(test_ds, batch_size=10, shuffle=True)\n",
        "\n",
        "# Create a dataset and loader for the test data and labels\n",
        "val_x = torch.Tensor(x_val).float()\n",
        "val_y = torch.Tensor(y_val).long()\n",
        "val_ds = utils.TensorDataset(val_x,val_y)\n",
        "val_loader = td.DataLoader(val_ds, batch_size=10, shuffle=True)"
      ],
      "metadata": {
        "id": "k2XYSHMj99JS"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HiddenLayer=[3,9,13,17,19]\n",
        "\n",
        "model_B_3 =  ModelB(30,2,HiddenLayer[0],5)\n",
        "model_B_9 =  ModelB(30,2,HiddenLayer[1],5)\n",
        "model_B_13 = ModelB(30,2,HiddenLayer[2],5)\n",
        "model_B_17 = ModelB(30,2,HiddenLayer[3],5)\n",
        "model_B_19 = ModelB(30,2,HiddenLayer[4],5)\n",
        "MODELSb= [model_B_3 , model_B_9 ,model_B_13 ,model_B_17 ,model_B_19]\n",
        "model_B_3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbOTFa1c2Wii",
        "outputId": "9c1303da-1fa4-42e7-8c51-6cfb1d836b02"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=30, out_features=5, bias=True)\n",
              "  (1): ReLU()\n",
              "  (2): Linear(in_features=5, out_features=5, bias=True)\n",
              "  (3): ReLU()\n",
              "  (4): Linear(in_features=5, out_features=5, bias=True)\n",
              "  (5): ReLU()\n",
              "  (6): Linear(in_features=5, out_features=5, bias=True)\n",
              "  (7): ReLU()\n",
              "  (8): Linear(in_features=5, out_features=2, bias=True)\n",
              "  (9): ReLU()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "OPTIMIZERS=[]\n",
        "for m in MODELS:\n",
        "  optimizer = torch.optim.Adam(m.parameters(), lr=0.0001)\n",
        "  OPTIMIZERS.append(optimizer)\n",
        "\n"
      ],
      "metadata": {
        "id": "WHkX1Qiy3oUE"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epoch_nums = []\n",
        "epochs=500\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "training_loss=[]\n",
        "validation_loss=[]\n",
        "acc_avg = []\n",
        "loss_avg = []\n",
        "for i in range (5):\n",
        "  tr_loss=[]\n",
        "  val_loss=[]\n",
        "  acc=[]\n",
        "  ave_loss = []\n",
        "  training_loss.append(tr_loss)\n",
        "  validation_loss.append(val_loss)\n",
        "  acc_avg.append(acc_avg)\n",
        "  loss_avg.append(ave_loss)\n",
        "\n",
        "\n",
        "\n",
        "for epoch in range(1, epochs):\n",
        "  epoch_nums.append(epoch)\n",
        "  for i in range(5):\n",
        "    # Feed the training data into the model to optimize the weights\n",
        "    train_loss,valid_loss = train(MODELSb[i], train_loader,val_loader, criterion, OPTIMIZERS[i])\n",
        "    avg_loss, avg_accuracy = test (MODELSb[i], val_loader,criterion)\n",
        "\n",
        "\n",
        "    # Log the metrcs for this epoch and models\n",
        "\n",
        "    training_loss[i].append(train_loss)\n",
        "    validation_loss[i].append(valid_loss)\n",
        "    acc_avg[i].append(avg_accuracy)\n",
        "    loss_avg[i].append(avg_loss)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Print stats for every 10th epoch so we can see training progress\n",
        "    if (epoch) % 1 == 0:\n",
        "        print('Epoch {:d} : for modelA {:} ==> Training loss= {:.4f} , Validation loss= {:.4f} , Average loss= {:.4f} , Accuracy = {:.4f}'.format(epoch,i, train_loss, valid_loss ,avg_loss, avg_accuracy))\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8MvEW3MQ9Wkb",
        "outputId": "469e0bdc-6cf6-436f-e41f-b18d0f1549b7"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 1 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0846 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 1 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0843 , Average loss= 0.0837 , Accuracy = 0.2807\n",
            "Epoch 1 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 1 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0654 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 2 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 2 : for modelA 1 ==> Training loss= 0.0774 , Validation loss= 0.0843 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 2 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0843 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 2 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 2 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0650 , Average loss= 0.0657 , Accuracy = 0.7193\n",
            "Epoch 3 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 3 : for modelA 1 ==> Training loss= 0.0774 , Validation loss= 0.0837 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 3 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0850 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 3 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 3 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0647 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 4 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 4 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0843 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 4 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0850 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 4 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 4 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0654 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 5 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 5 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0837 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 5 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0850 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 5 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 5 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 6 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 6 : for modelA 1 ==> Training loss= 0.0774 , Validation loss= 0.0846 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 6 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0847 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 6 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 6 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0654 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 7 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 7 : for modelA 1 ==> Training loss= 0.0770 , Validation loss= 0.0843 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 7 : for modelA 2 ==> Training loss= 0.0776 , Validation loss= 0.0847 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 7 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 7 : for modelA 4 ==> Training loss= 0.0666 , Validation loss= 0.0647 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 8 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 8 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0837 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 8 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0853 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 8 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 8 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0650 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 9 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 9 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0846 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 9 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0840 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 9 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 9 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0647 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 10 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 10 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0849 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 10 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0850 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 10 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 10 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0657 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 11 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 11 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0843 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 11 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0850 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 11 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 11 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 12 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 12 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0843 , Average loss= 0.0849 , Accuracy = 0.2807\n",
            "Epoch 12 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0847 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 12 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0712 , Accuracy = 0.7193\n",
            "Epoch 12 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0657 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 13 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 13 : for modelA 1 ==> Training loss= 0.0770 , Validation loss= 0.0846 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 13 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0850 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 13 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0712 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 13 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0643 , Average loss= 0.0657 , Accuracy = 0.7193\n",
            "Epoch 14 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 14 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0846 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 14 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0853 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 14 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0715 , Accuracy = 0.7193\n",
            "Epoch 14 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0650 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 15 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 15 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0840 , Average loss= 0.0849 , Accuracy = 0.2807\n",
            "Epoch 15 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0843 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 15 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0712 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 15 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 16 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 16 : for modelA 1 ==> Training loss= 0.0770 , Validation loss= 0.0846 , Average loss= 0.0849 , Accuracy = 0.2807\n",
            "Epoch 16 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0850 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 16 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 16 : for modelA 4 ==> Training loss= 0.0666 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 17 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 17 : for modelA 1 ==> Training loss= 0.0770 , Validation loss= 0.0843 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 17 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0840 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 17 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 17 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0650 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 18 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 18 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0846 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 18 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0847 , Average loss= 0.0853 , Accuracy = 0.2807\n",
            "Epoch 18 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 18 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0657 , Accuracy = 0.7193\n",
            "Epoch 19 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 19 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0846 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 19 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0850 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 19 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 19 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 20 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 20 : for modelA 1 ==> Training loss= 0.0774 , Validation loss= 0.0846 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 20 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0847 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 20 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0712 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 20 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0654 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 21 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 21 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0840 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 21 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0847 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 21 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 21 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0657 , Accuracy = 0.7193\n",
            "Epoch 22 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 22 : for modelA 1 ==> Training loss= 0.0770 , Validation loss= 0.0849 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 22 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0843 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 22 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 22 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0654 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 23 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 23 : for modelA 1 ==> Training loss= 0.0774 , Validation loss= 0.0837 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 23 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0843 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 23 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 23 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0654 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 24 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 24 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0840 , Average loss= 0.0849 , Accuracy = 0.2807\n",
            "Epoch 24 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0847 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 24 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 24 : for modelA 4 ==> Training loss= 0.0666 , Validation loss= 0.0647 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 25 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 25 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0840 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 25 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0847 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 25 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 25 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0647 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 26 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 26 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0846 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 26 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0847 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 26 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 26 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0647 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 27 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 27 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0843 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 27 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0843 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 27 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 27 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0657 , Accuracy = 0.7193\n",
            "Epoch 28 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 28 : for modelA 1 ==> Training loss= 0.0774 , Validation loss= 0.0849 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 28 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0847 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 28 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 28 : for modelA 4 ==> Training loss= 0.0671 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 29 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 29 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0846 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 29 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0853 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 29 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 29 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 30 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 30 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0846 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 30 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0840 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 30 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 30 : for modelA 4 ==> Training loss= 0.0666 , Validation loss= 0.0654 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 31 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 31 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0843 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 31 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0843 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 31 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0712 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 31 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0654 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 32 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 32 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0846 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 32 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0850 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 32 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 32 : for modelA 4 ==> Training loss= 0.0672 , Validation loss= 0.0650 , Average loss= 0.0657 , Accuracy = 0.7193\n",
            "Epoch 33 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 33 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0846 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 33 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0843 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 33 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 33 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0657 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 34 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 34 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0840 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 34 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0847 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 34 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 34 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0643 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 35 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 35 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0846 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 35 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0847 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 35 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 35 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0657 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 36 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 36 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0846 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 36 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0840 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 36 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 36 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0647 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 37 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 37 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0843 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 37 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0847 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 37 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0712 , Accuracy = 0.7193\n",
            "Epoch 37 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0654 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 38 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 38 : for modelA 1 ==> Training loss= 0.0774 , Validation loss= 0.0843 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 38 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0850 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 38 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 38 : for modelA 4 ==> Training loss= 0.0666 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 39 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 39 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0849 , Average loss= 0.0837 , Accuracy = 0.2807\n",
            "Epoch 39 : for modelA 2 ==> Training loss= 0.0776 , Validation loss= 0.0847 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 39 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 39 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0657 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 40 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 40 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0843 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 40 : for modelA 2 ==> Training loss= 0.0776 , Validation loss= 0.0850 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 40 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0712 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 40 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0643 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 41 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 41 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0840 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 41 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0850 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 41 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 41 : for modelA 4 ==> Training loss= 0.0666 , Validation loss= 0.0654 , Average loss= 0.0657 , Accuracy = 0.7193\n",
            "Epoch 42 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 42 : for modelA 1 ==> Training loss= 0.0770 , Validation loss= 0.0843 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 42 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0843 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 42 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 42 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0647 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 43 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 43 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0843 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 43 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0847 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 43 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 43 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 44 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 44 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0843 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 44 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0840 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 44 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0712 , Accuracy = 0.7193\n",
            "Epoch 44 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0654 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 45 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 45 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0837 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 45 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0853 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 45 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 45 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 46 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 46 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0843 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 46 : for modelA 2 ==> Training loss= 0.0776 , Validation loss= 0.0847 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 46 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 46 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0650 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 47 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 47 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0843 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 47 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0847 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 47 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 47 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0654 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 48 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 48 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0846 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 48 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0843 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 48 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 48 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 49 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 49 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0846 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 49 : for modelA 2 ==> Training loss= 0.0776 , Validation loss= 0.0843 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 49 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 49 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0647 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 50 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 50 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0843 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 50 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0847 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 50 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 50 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0643 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 51 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 51 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0846 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 51 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0840 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 51 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0712 , Accuracy = 0.7193\n",
            "Epoch 51 : for modelA 4 ==> Training loss= 0.0671 , Validation loss= 0.0654 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 52 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 52 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0843 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 52 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0850 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 52 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 52 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0647 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 53 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 53 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0843 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 53 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0850 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 53 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 53 : for modelA 4 ==> Training loss= 0.0666 , Validation loss= 0.0647 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 54 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 54 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0840 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 54 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0850 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 54 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 54 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0647 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 55 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 55 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0837 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 55 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0843 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 55 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 55 : for modelA 4 ==> Training loss= 0.0666 , Validation loss= 0.0654 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 56 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 56 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0843 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 56 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0850 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 56 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0712 , Accuracy = 0.7193\n",
            "Epoch 56 : for modelA 4 ==> Training loss= 0.0671 , Validation loss= 0.0650 , Average loss= 0.0657 , Accuracy = 0.7193\n",
            "Epoch 57 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 57 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0846 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 57 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0847 , Average loss= 0.0853 , Accuracy = 0.2807\n",
            "Epoch 57 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 57 : for modelA 4 ==> Training loss= 0.0672 , Validation loss= 0.0650 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 58 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 58 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0846 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 58 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0847 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 58 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0715 , Accuracy = 0.7193\n",
            "Epoch 58 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0647 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 59 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 59 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0843 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 59 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0847 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 59 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 59 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0654 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 60 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 60 : for modelA 1 ==> Training loss= 0.0774 , Validation loss= 0.0846 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 60 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0843 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 60 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 60 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0661 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 61 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 61 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0840 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 61 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0840 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 61 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 61 : for modelA 4 ==> Training loss= 0.0666 , Validation loss= 0.0657 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 62 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 62 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0840 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 62 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0853 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 62 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0712 , Accuracy = 0.7193\n",
            "Epoch 62 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0650 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 63 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 63 : for modelA 1 ==> Training loss= 0.0774 , Validation loss= 0.0843 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 63 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0853 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 63 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 63 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0647 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 64 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 64 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0843 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 64 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0847 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 64 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 64 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0654 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 65 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 65 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0840 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 65 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0850 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 65 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 65 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0647 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 66 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 66 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0846 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 66 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0850 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 66 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 66 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0647 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 67 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 67 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0846 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 67 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0853 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 67 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 67 : for modelA 4 ==> Training loss= 0.0666 , Validation loss= 0.0643 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 68 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 68 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0843 , Average loss= 0.0837 , Accuracy = 0.2807\n",
            "Epoch 68 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0850 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 68 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 68 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0647 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 69 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 69 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0843 , Average loss= 0.0837 , Accuracy = 0.2807\n",
            "Epoch 69 : for modelA 2 ==> Training loss= 0.0776 , Validation loss= 0.0850 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 69 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0712 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 69 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0654 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 70 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 70 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0846 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 70 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0847 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 70 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0712 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 70 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0657 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 71 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 71 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0849 , Average loss= 0.0849 , Accuracy = 0.2807\n",
            "Epoch 71 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0843 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 71 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 71 : for modelA 4 ==> Training loss= 0.0671 , Validation loss= 0.0654 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 72 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 72 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0843 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 72 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0850 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 72 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 72 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 73 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 73 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0843 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 73 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0847 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 73 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0712 , Accuracy = 0.7193\n",
            "Epoch 73 : for modelA 4 ==> Training loss= 0.0666 , Validation loss= 0.0647 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 74 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 74 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0840 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 74 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0847 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 74 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 74 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0654 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 75 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 75 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0849 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 75 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0847 , Average loss= 0.0853 , Accuracy = 0.2807\n",
            "Epoch 75 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 75 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0647 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 76 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 76 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0846 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 76 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0843 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 76 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0712 , Accuracy = 0.7193\n",
            "Epoch 76 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0654 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 77 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 77 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0846 , Average loss= 0.0849 , Accuracy = 0.2807\n",
            "Epoch 77 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0850 , Average loss= 0.0853 , Accuracy = 0.2807\n",
            "Epoch 77 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 77 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0647 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 78 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 78 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0846 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 78 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0840 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 78 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0712 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 78 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0657 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 79 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 79 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0843 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 79 : for modelA 2 ==> Training loss= 0.0776 , Validation loss= 0.0847 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 79 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 79 : for modelA 4 ==> Training loss= 0.0666 , Validation loss= 0.0647 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 80 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 80 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0846 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 80 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0847 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 80 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 80 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 81 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 81 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0846 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 81 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0850 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 81 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 81 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0650 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 82 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 82 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0843 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 82 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0853 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 82 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 82 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0647 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 83 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 83 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0843 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 83 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0850 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 83 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0715 , Accuracy = 0.7193\n",
            "Epoch 83 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0647 , Average loss= 0.0661 , Accuracy = 0.7193\n",
            "Epoch 84 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 84 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0843 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 84 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0847 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 84 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 84 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0647 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 85 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 85 : for modelA 1 ==> Training loss= 0.0770 , Validation loss= 0.0840 , Average loss= 0.0837 , Accuracy = 0.2807\n",
            "Epoch 85 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0840 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 85 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 85 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0657 , Average loss= 0.0657 , Accuracy = 0.7193\n",
            "Epoch 86 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 86 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0846 , Average loss= 0.0837 , Accuracy = 0.2807\n",
            "Epoch 86 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0847 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 86 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 86 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0647 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 87 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 87 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0846 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 87 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0843 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 87 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 87 : for modelA 4 ==> Training loss= 0.0666 , Validation loss= 0.0654 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 88 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 88 : for modelA 1 ==> Training loss= 0.0774 , Validation loss= 0.0834 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 88 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0850 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 88 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 88 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 89 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 89 : for modelA 1 ==> Training loss= 0.0774 , Validation loss= 0.0849 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 89 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0843 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 89 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 89 : for modelA 4 ==> Training loss= 0.0671 , Validation loss= 0.0654 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 90 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 90 : for modelA 1 ==> Training loss= 0.0769 , Validation loss= 0.0843 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 90 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0847 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 90 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0712 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 90 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0647 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 91 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 91 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0843 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 91 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0853 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 91 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 91 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 92 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 92 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0846 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 92 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0847 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 92 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 92 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 93 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 93 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0846 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 93 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0847 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 93 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 93 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0647 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 94 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 94 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0843 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 94 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0843 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 94 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 94 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 95 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 95 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0846 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 95 : for modelA 2 ==> Training loss= 0.0776 , Validation loss= 0.0847 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 95 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0712 , Accuracy = 0.7193\n",
            "Epoch 95 : for modelA 4 ==> Training loss= 0.0666 , Validation loss= 0.0647 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 96 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 96 : for modelA 1 ==> Training loss= 0.0774 , Validation loss= 0.0840 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 96 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0843 , Average loss= 0.0853 , Accuracy = 0.2807\n",
            "Epoch 96 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0712 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 96 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0650 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 97 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 97 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0843 , Average loss= 0.0849 , Accuracy = 0.2807\n",
            "Epoch 97 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0843 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 97 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 97 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 98 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 98 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0843 , Average loss= 0.0849 , Accuracy = 0.2807\n",
            "Epoch 98 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0843 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 98 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 98 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0643 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 99 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 99 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0843 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 99 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0850 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 99 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 99 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 100 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 100 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0849 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 100 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0850 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 100 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 100 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0647 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 101 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 101 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0840 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 101 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0850 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 101 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 101 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0647 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 102 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 102 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0843 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 102 : for modelA 2 ==> Training loss= 0.0776 , Validation loss= 0.0840 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 102 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 102 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 103 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 103 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0843 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 103 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0850 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 103 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 103 : for modelA 4 ==> Training loss= 0.0666 , Validation loss= 0.0657 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 104 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 104 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0846 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 104 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0834 , Average loss= 0.0853 , Accuracy = 0.2807\n",
            "Epoch 104 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 104 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0657 , Average loss= 0.0657 , Accuracy = 0.7193\n",
            "Epoch 105 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 105 : for modelA 1 ==> Training loss= 0.0770 , Validation loss= 0.0846 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 105 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0843 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 105 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 105 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0643 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 106 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 106 : for modelA 1 ==> Training loss= 0.0774 , Validation loss= 0.0843 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 106 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0847 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 106 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 106 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0657 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 107 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 107 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0843 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 107 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0843 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 107 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 107 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0643 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 108 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 108 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0843 , Average loss= 0.0849 , Accuracy = 0.2807\n",
            "Epoch 108 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0843 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 108 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 108 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0650 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 109 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 109 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0846 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 109 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0847 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 109 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0715 , Average loss= 0.0712 , Accuracy = 0.7193\n",
            "Epoch 109 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0647 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 110 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 110 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0843 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 110 : for modelA 2 ==> Training loss= 0.0776 , Validation loss= 0.0847 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 110 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 110 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 111 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 111 : for modelA 1 ==> Training loss= 0.0774 , Validation loss= 0.0834 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 111 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0847 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 111 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 111 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0654 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 112 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 112 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0849 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 112 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0847 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 112 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 112 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0643 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 113 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 113 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0849 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 113 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0843 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 113 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 113 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0643 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 114 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 114 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0843 , Average loss= 0.0849 , Accuracy = 0.2807\n",
            "Epoch 114 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0850 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 114 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 114 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 115 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 115 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0849 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 115 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0847 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 115 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 115 : for modelA 4 ==> Training loss= 0.0671 , Validation loss= 0.0650 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 116 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 116 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0849 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 116 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0840 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 116 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 116 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0647 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 117 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 117 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0843 , Average loss= 0.0849 , Accuracy = 0.2807\n",
            "Epoch 117 : for modelA 2 ==> Training loss= 0.0776 , Validation loss= 0.0847 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 117 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 117 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 118 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 118 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0843 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 118 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0850 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 118 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 118 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0643 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 119 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 119 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0843 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 119 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0847 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 119 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 119 : for modelA 4 ==> Training loss= 0.0671 , Validation loss= 0.0650 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 120 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 120 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0843 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 120 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0843 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 120 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 120 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0654 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 121 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 121 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0843 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 121 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0847 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 121 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 121 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 122 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 122 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0846 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 122 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0843 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 122 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0712 , Average loss= 0.0712 , Accuracy = 0.7193\n",
            "Epoch 122 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0647 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 123 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 123 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0849 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 123 : for modelA 2 ==> Training loss= 0.0776 , Validation loss= 0.0843 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 123 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 123 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 124 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 124 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0843 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 124 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0847 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 124 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 124 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0643 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 125 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 125 : for modelA 1 ==> Training loss= 0.0770 , Validation loss= 0.0843 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 125 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0847 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 125 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 125 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0647 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 126 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 126 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0846 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 126 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0843 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 126 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 126 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 127 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 127 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0846 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 127 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0853 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 127 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 127 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 128 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 128 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0846 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 128 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0850 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 128 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 128 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0647 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 129 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 129 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0843 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 129 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0843 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 129 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 129 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 130 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 130 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0843 , Average loss= 0.0837 , Accuracy = 0.2807\n",
            "Epoch 130 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0850 , Average loss= 0.0853 , Accuracy = 0.2807\n",
            "Epoch 130 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 130 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0643 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 131 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 131 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0843 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 131 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0843 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 131 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 131 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0657 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 132 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 132 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0843 , Average loss= 0.0849 , Accuracy = 0.2807\n",
            "Epoch 132 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0847 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 132 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 132 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0650 , Average loss= 0.0657 , Accuracy = 0.7193\n",
            "Epoch 133 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 133 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0840 , Average loss= 0.0834 , Accuracy = 0.2807\n",
            "Epoch 133 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0847 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 133 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 133 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0650 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 134 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 134 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0849 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 134 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0850 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 134 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 134 : for modelA 4 ==> Training loss= 0.0671 , Validation loss= 0.0654 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 135 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 135 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0846 , Average loss= 0.0849 , Accuracy = 0.2807\n",
            "Epoch 135 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0847 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 135 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0712 , Accuracy = 0.7193\n",
            "Epoch 135 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0647 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 136 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 136 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0846 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 136 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0840 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 136 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 136 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0647 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 137 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 137 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0843 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 137 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0847 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 137 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0712 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 137 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 138 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 138 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0846 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 138 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0843 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 138 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 138 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 139 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 139 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0840 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 139 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0853 , Average loss= 0.0834 , Accuracy = 0.2807\n",
            "Epoch 139 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 139 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 140 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 140 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0840 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 140 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0847 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 140 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0712 , Accuracy = 0.7193\n",
            "Epoch 140 : for modelA 4 ==> Training loss= 0.0671 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 141 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 141 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0846 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 141 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0847 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 141 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0712 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 141 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0647 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 142 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 142 : for modelA 1 ==> Training loss= 0.0774 , Validation loss= 0.0840 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 142 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0850 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 142 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0712 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 142 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0657 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 143 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 143 : for modelA 1 ==> Training loss= 0.0770 , Validation loss= 0.0840 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 143 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0840 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 143 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 143 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 144 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 144 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0843 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 144 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0843 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 144 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 144 : for modelA 4 ==> Training loss= 0.0666 , Validation loss= 0.0657 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 145 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 145 : for modelA 1 ==> Training loss= 0.0774 , Validation loss= 0.0843 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 145 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0843 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 145 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0712 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 145 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0647 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 146 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 146 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0843 , Average loss= 0.0837 , Accuracy = 0.2807\n",
            "Epoch 146 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0850 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 146 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 146 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 147 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 147 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0846 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 147 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0850 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 147 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0712 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 147 : for modelA 4 ==> Training loss= 0.0666 , Validation loss= 0.0647 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 148 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 148 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0840 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 148 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0850 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 148 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 148 : for modelA 4 ==> Training loss= 0.0666 , Validation loss= 0.0654 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 149 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 149 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0849 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 149 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0840 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 149 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0712 , Accuracy = 0.7193\n",
            "Epoch 149 : for modelA 4 ==> Training loss= 0.0671 , Validation loss= 0.0647 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 150 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 150 : for modelA 1 ==> Training loss= 0.0774 , Validation loss= 0.0843 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 150 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0847 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 150 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 150 : for modelA 4 ==> Training loss= 0.0671 , Validation loss= 0.0654 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 151 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 151 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0840 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 151 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0850 , Average loss= 0.0853 , Accuracy = 0.2807\n",
            "Epoch 151 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0712 , Accuracy = 0.7193\n",
            "Epoch 151 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0650 , Average loss= 0.0657 , Accuracy = 0.7193\n",
            "Epoch 152 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 152 : for modelA 1 ==> Training loss= 0.0774 , Validation loss= 0.0840 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 152 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0847 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 152 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 152 : for modelA 4 ==> Training loss= 0.0666 , Validation loss= 0.0647 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 153 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 153 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0840 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 153 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0847 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 153 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 153 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0654 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 154 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 154 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0840 , Average loss= 0.0834 , Accuracy = 0.2807\n",
            "Epoch 154 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0840 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 154 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 154 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0657 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 155 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 155 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0843 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 155 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0843 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 155 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 155 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0654 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 156 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 156 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0843 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 156 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0850 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 156 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 156 : for modelA 4 ==> Training loss= 0.0666 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 157 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 157 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0843 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 157 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0840 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 157 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 157 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 158 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 158 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0843 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 158 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0843 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 158 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0712 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 158 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0650 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 159 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 159 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0846 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 159 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0850 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 159 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0712 , Average loss= 0.0712 , Accuracy = 0.7193\n",
            "Epoch 159 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 160 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 160 : for modelA 1 ==> Training loss= 0.0770 , Validation loss= 0.0840 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 160 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0850 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 160 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 160 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0643 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 161 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 161 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0843 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 161 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0843 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 161 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 161 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0647 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 162 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 162 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0843 , Average loss= 0.0837 , Accuracy = 0.2807\n",
            "Epoch 162 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0840 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 162 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 162 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0643 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 163 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 163 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0840 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 163 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0840 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 163 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 163 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0657 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 164 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 164 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0840 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 164 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0850 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 164 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0712 , Accuracy = 0.7193\n",
            "Epoch 164 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0654 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 165 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 165 : for modelA 1 ==> Training loss= 0.0774 , Validation loss= 0.0849 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 165 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0850 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 165 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 165 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0654 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 166 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 166 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0846 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 166 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0847 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 166 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 166 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0657 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 167 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 167 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0843 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 167 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0847 , Average loss= 0.0837 , Accuracy = 0.2807\n",
            "Epoch 167 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 167 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0647 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 168 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 168 : for modelA 1 ==> Training loss= 0.0770 , Validation loss= 0.0840 , Average loss= 0.0849 , Accuracy = 0.2807\n",
            "Epoch 168 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0850 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 168 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 168 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0643 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 169 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 169 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0840 , Average loss= 0.0834 , Accuracy = 0.2807\n",
            "Epoch 169 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0847 , Average loss= 0.0853 , Accuracy = 0.2807\n",
            "Epoch 169 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 169 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0661 , Average loss= 0.0657 , Accuracy = 0.7193\n",
            "Epoch 170 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 170 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0849 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 170 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0840 , Average loss= 0.0853 , Accuracy = 0.2807\n",
            "Epoch 170 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 170 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0654 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 171 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 171 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0840 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 171 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0847 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 171 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 171 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0647 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 172 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 172 : for modelA 1 ==> Training loss= 0.0774 , Validation loss= 0.0840 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 172 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0847 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 172 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 172 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0650 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 173 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 173 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0843 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 173 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0847 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 173 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 173 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0654 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 174 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 174 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0843 , Average loss= 0.0837 , Accuracy = 0.2807\n",
            "Epoch 174 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0850 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 174 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 174 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0654 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 175 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 175 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0846 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 175 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0850 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 175 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 175 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0661 , Accuracy = 0.7193\n",
            "Epoch 176 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 176 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0840 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 176 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0847 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 176 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 176 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0643 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 177 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 177 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0843 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 177 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0847 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 177 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 177 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0647 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 178 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 178 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0840 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 178 : for modelA 2 ==> Training loss= 0.0776 , Validation loss= 0.0847 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 178 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0712 , Accuracy = 0.7193\n",
            "Epoch 178 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0657 , Accuracy = 0.7193\n",
            "Epoch 179 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 179 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0843 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 179 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0843 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 179 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 179 : for modelA 4 ==> Training loss= 0.0671 , Validation loss= 0.0647 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 180 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 180 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0840 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 180 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0847 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 180 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0712 , Accuracy = 0.7193\n",
            "Epoch 180 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 181 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 181 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0843 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 181 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0843 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 181 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 181 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0654 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 182 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 182 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0843 , Average loss= 0.0849 , Accuracy = 0.2807\n",
            "Epoch 182 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0843 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 182 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 182 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 183 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 183 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0843 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 183 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0853 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 183 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 183 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0647 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 184 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 184 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0846 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 184 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0853 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 184 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 184 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 185 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 185 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0843 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 185 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0843 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 185 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 185 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0654 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 186 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 186 : for modelA 1 ==> Training loss= 0.0774 , Validation loss= 0.0840 , Average loss= 0.0831 , Accuracy = 0.2807\n",
            "Epoch 186 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0847 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 186 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 186 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0647 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 187 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 187 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0846 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 187 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0847 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 187 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0712 , Accuracy = 0.7193\n",
            "Epoch 187 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0654 , Average loss= 0.0657 , Accuracy = 0.7193\n",
            "Epoch 188 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 188 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0840 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 188 : for modelA 2 ==> Training loss= 0.0776 , Validation loss= 0.0843 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 188 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0712 , Accuracy = 0.7193\n",
            "Epoch 188 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0647 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 189 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 189 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0846 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 189 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0850 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 189 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 189 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 190 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 190 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0846 , Average loss= 0.0849 , Accuracy = 0.2807\n",
            "Epoch 190 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0847 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 190 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0712 , Accuracy = 0.7193\n",
            "Epoch 190 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0647 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 191 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 191 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0843 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 191 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0850 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 191 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 191 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0654 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 192 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 192 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0843 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 192 : for modelA 2 ==> Training loss= 0.0776 , Validation loss= 0.0850 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 192 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 192 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0654 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 193 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 193 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0840 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 193 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0843 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 193 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 193 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0654 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 194 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 194 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0846 , Average loss= 0.0834 , Accuracy = 0.2807\n",
            "Epoch 194 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0850 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 194 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 194 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0654 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 195 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 195 : for modelA 1 ==> Training loss= 0.0774 , Validation loss= 0.0846 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 195 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0840 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 195 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 195 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0654 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 196 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 196 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0843 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 196 : for modelA 2 ==> Training loss= 0.0776 , Validation loss= 0.0843 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 196 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 196 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0647 , Average loss= 0.0661 , Accuracy = 0.7193\n",
            "Epoch 197 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 197 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0840 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 197 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0853 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 197 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 197 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0654 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 198 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 198 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0849 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 198 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0850 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 198 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 198 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0643 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 199 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 199 : for modelA 1 ==> Training loss= 0.0770 , Validation loss= 0.0846 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 199 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0853 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 199 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 199 : for modelA 4 ==> Training loss= 0.0666 , Validation loss= 0.0650 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 200 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 200 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0840 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 200 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0850 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 200 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 200 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0647 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 201 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 201 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0846 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 201 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0843 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 201 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 201 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0647 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 202 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 202 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0837 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 202 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0843 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 202 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 202 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0654 , Average loss= 0.0661 , Accuracy = 0.7193\n",
            "Epoch 203 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 203 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0840 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 203 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0847 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 203 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0712 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 203 : for modelA 4 ==> Training loss= 0.0671 , Validation loss= 0.0650 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 204 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 204 : for modelA 1 ==> Training loss= 0.0774 , Validation loss= 0.0840 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 204 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0840 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 204 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 204 : for modelA 4 ==> Training loss= 0.0666 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 205 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 205 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0840 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 205 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0847 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 205 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 205 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0647 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 206 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 206 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0837 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 206 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0847 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 206 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 206 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 207 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 207 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0843 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 207 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0850 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 207 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 207 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 208 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 208 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0840 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 208 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0850 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 208 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 208 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 209 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 209 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0843 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 209 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0850 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 209 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 209 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 210 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 210 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0846 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 210 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0847 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 210 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 210 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0647 , Average loss= 0.0661 , Accuracy = 0.7193\n",
            "Epoch 211 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 211 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0846 , Average loss= 0.0837 , Accuracy = 0.2807\n",
            "Epoch 211 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0850 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 211 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 211 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0654 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 212 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 212 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0843 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 212 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0843 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 212 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 212 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0657 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 213 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 213 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0846 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 213 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0847 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 213 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 213 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0654 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 214 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 214 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0843 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 214 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0850 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 214 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 214 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0647 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 215 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 215 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0843 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 215 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0847 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 215 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 215 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0654 , Average loss= 0.0657 , Accuracy = 0.7193\n",
            "Epoch 216 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 216 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0849 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 216 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0850 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 216 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 216 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 217 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 217 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0840 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 217 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0847 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 217 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 217 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0647 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 218 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 218 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0840 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 218 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0853 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 218 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 218 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0647 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 219 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 219 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0843 , Average loss= 0.0834 , Accuracy = 0.2807\n",
            "Epoch 219 : for modelA 2 ==> Training loss= 0.0776 , Validation loss= 0.0853 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 219 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 219 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 220 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 220 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0843 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 220 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0850 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 220 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 220 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0647 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 221 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 221 : for modelA 1 ==> Training loss= 0.0770 , Validation loss= 0.0846 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 221 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0847 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 221 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 221 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0647 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 222 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 222 : for modelA 1 ==> Training loss= 0.0774 , Validation loss= 0.0843 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 222 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0843 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 222 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 222 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 223 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 223 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0834 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 223 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0843 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 223 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 223 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0647 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 224 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 224 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0840 , Average loss= 0.0834 , Accuracy = 0.2807\n",
            "Epoch 224 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0847 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 224 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 224 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0650 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 225 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 225 : for modelA 1 ==> Training loss= 0.0774 , Validation loss= 0.0843 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 225 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0850 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 225 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0712 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 225 : for modelA 4 ==> Training loss= 0.0666 , Validation loss= 0.0647 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 226 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 226 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0840 , Average loss= 0.0834 , Accuracy = 0.2807\n",
            "Epoch 226 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0850 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 226 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0715 , Accuracy = 0.7193\n",
            "Epoch 226 : for modelA 4 ==> Training loss= 0.0666 , Validation loss= 0.0647 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 227 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 227 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0843 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 227 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0837 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 227 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 227 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0654 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 228 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 228 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0840 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 228 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0847 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 228 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 228 : for modelA 4 ==> Training loss= 0.0666 , Validation loss= 0.0654 , Average loss= 0.0657 , Accuracy = 0.7193\n",
            "Epoch 229 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 229 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0846 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 229 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0850 , Average loss= 0.0837 , Accuracy = 0.2807\n",
            "Epoch 229 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 229 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 230 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 230 : for modelA 1 ==> Training loss= 0.0774 , Validation loss= 0.0843 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 230 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0853 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 230 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 230 : for modelA 4 ==> Training loss= 0.0671 , Validation loss= 0.0657 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 231 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 231 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0837 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 231 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0843 , Average loss= 0.0853 , Accuracy = 0.2807\n",
            "Epoch 231 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 231 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0643 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 232 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 232 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0843 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 232 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0847 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 232 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 232 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0647 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 233 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 233 : for modelA 1 ==> Training loss= 0.0774 , Validation loss= 0.0846 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 233 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0847 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 233 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 233 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0643 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 234 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 234 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0840 , Average loss= 0.0837 , Accuracy = 0.2807\n",
            "Epoch 234 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0850 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 234 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 234 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0654 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 235 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 235 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0846 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 235 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0853 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 235 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0715 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 235 : for modelA 4 ==> Training loss= 0.0666 , Validation loss= 0.0643 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 236 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 236 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0843 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 236 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0850 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 236 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 236 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0657 , Accuracy = 0.7193\n",
            "Epoch 237 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 237 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0840 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 237 : for modelA 2 ==> Training loss= 0.0776 , Validation loss= 0.0847 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 237 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0716 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 237 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0643 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 238 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 238 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0840 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 238 : for modelA 2 ==> Training loss= 0.0776 , Validation loss= 0.0840 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 238 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 238 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0647 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 239 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 239 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0849 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 239 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0853 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 239 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 239 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0647 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 240 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 240 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0846 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 240 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0843 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 240 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 240 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0647 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 241 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 241 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0840 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 241 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0847 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 241 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 241 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0647 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 242 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 242 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0840 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 242 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0843 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 242 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 242 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0647 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 243 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 243 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0843 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 243 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0847 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 243 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 243 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0647 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 244 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 244 : for modelA 1 ==> Training loss= 0.0774 , Validation loss= 0.0846 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 244 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0850 , Average loss= 0.0853 , Accuracy = 0.2807\n",
            "Epoch 244 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 244 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 245 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 245 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0846 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 245 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0853 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 245 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 245 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0657 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 246 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 246 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0843 , Average loss= 0.0837 , Accuracy = 0.2807\n",
            "Epoch 246 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0847 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 246 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0712 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 246 : for modelA 4 ==> Training loss= 0.0666 , Validation loss= 0.0650 , Average loss= 0.0657 , Accuracy = 0.7193\n",
            "Epoch 247 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 247 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0846 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 247 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0853 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 247 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 247 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0647 , Average loss= 0.0657 , Accuracy = 0.7193\n",
            "Epoch 248 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 248 : for modelA 1 ==> Training loss= 0.0774 , Validation loss= 0.0840 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 248 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0850 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 248 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 248 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0647 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 249 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 249 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0843 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 249 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0850 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 249 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 249 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0647 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 250 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 250 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0840 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 250 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0847 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 250 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0712 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 250 : for modelA 4 ==> Training loss= 0.0671 , Validation loss= 0.0647 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 251 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 251 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0843 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 251 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0847 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 251 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0715 , Accuracy = 0.7193\n",
            "Epoch 251 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0654 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 252 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 252 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0846 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 252 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0847 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 252 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0712 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 252 : for modelA 4 ==> Training loss= 0.0671 , Validation loss= 0.0647 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 253 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 253 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0840 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 253 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0843 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 253 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0712 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 253 : for modelA 4 ==> Training loss= 0.0666 , Validation loss= 0.0647 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 254 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 254 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0834 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 254 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0850 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 254 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 254 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0650 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 255 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 255 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0840 , Average loss= 0.0837 , Accuracy = 0.2807\n",
            "Epoch 255 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0847 , Average loss= 0.0853 , Accuracy = 0.2807\n",
            "Epoch 255 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 255 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 256 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 256 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0840 , Average loss= 0.0837 , Accuracy = 0.2807\n",
            "Epoch 256 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0847 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 256 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 256 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0650 , Average loss= 0.0657 , Accuracy = 0.7193\n",
            "Epoch 257 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 257 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0843 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 257 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0847 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 257 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 257 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0643 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 258 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 258 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0843 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 258 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0843 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 258 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 258 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 259 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 259 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0846 , Average loss= 0.0849 , Accuracy = 0.2807\n",
            "Epoch 259 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0850 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 259 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 259 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0654 , Average loss= 0.0657 , Accuracy = 0.7193\n",
            "Epoch 260 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 260 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0849 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 260 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0843 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 260 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 260 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 261 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 261 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0840 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 261 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0843 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 261 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 261 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0657 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 262 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 262 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0846 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 262 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0847 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 262 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 262 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 263 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 263 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0846 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 263 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0847 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 263 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 263 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0654 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 264 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 264 : for modelA 1 ==> Training loss= 0.0770 , Validation loss= 0.0843 , Average loss= 0.0849 , Accuracy = 0.2807\n",
            "Epoch 264 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0847 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 264 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0712 , Accuracy = 0.7193\n",
            "Epoch 264 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0647 , Average loss= 0.0657 , Accuracy = 0.7193\n",
            "Epoch 265 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 265 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0840 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 265 : for modelA 2 ==> Training loss= 0.0776 , Validation loss= 0.0843 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 265 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 265 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0643 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 266 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 266 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0843 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 266 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0850 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 266 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 266 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0654 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 267 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 267 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0846 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 267 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0853 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 267 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 267 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0647 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 268 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 268 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0840 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 268 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0840 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 268 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 268 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0654 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 269 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 269 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0846 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 269 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0850 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 269 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 269 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0654 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 270 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 270 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0840 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 270 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0847 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 270 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 270 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 271 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 271 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0843 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 271 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0840 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 271 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 271 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 272 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 272 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0846 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 272 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0850 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 272 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 272 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0647 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 273 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 273 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0834 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 273 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0843 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 273 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0715 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 273 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0647 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 274 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 274 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0846 , Average loss= 0.0837 , Accuracy = 0.2807\n",
            "Epoch 274 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0847 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 274 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0712 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 274 : for modelA 4 ==> Training loss= 0.0666 , Validation loss= 0.0650 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 275 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 275 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0846 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 275 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0843 , Average loss= 0.0853 , Accuracy = 0.2807\n",
            "Epoch 275 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0712 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 275 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0647 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 276 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 276 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0843 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 276 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0847 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 276 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 276 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0650 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 277 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 277 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0843 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 277 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0840 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 277 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 277 : for modelA 4 ==> Training loss= 0.0666 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 278 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 278 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0843 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 278 : for modelA 2 ==> Training loss= 0.0776 , Validation loss= 0.0847 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 278 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 278 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0661 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 279 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 279 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0840 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 279 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0847 , Average loss= 0.0853 , Accuracy = 0.2807\n",
            "Epoch 279 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 279 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 280 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 280 : for modelA 1 ==> Training loss= 0.0774 , Validation loss= 0.0837 , Average loss= 0.0837 , Accuracy = 0.2807\n",
            "Epoch 280 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0843 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 280 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 280 : for modelA 4 ==> Training loss= 0.0666 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 281 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 281 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0843 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 281 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0843 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 281 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 281 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0654 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 282 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 282 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0843 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 282 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0850 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 282 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 282 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0654 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 283 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 283 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0843 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 283 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0847 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 283 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 283 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0647 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 284 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 284 : for modelA 1 ==> Training loss= 0.0770 , Validation loss= 0.0843 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 284 : for modelA 2 ==> Training loss= 0.0776 , Validation loss= 0.0843 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 284 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 284 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0647 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 285 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 285 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0846 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 285 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0847 , Average loss= 0.0837 , Accuracy = 0.2807\n",
            "Epoch 285 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 285 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0643 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 286 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 286 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0849 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 286 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0847 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 286 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 286 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0654 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 287 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 287 : for modelA 1 ==> Training loss= 0.0774 , Validation loss= 0.0843 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 287 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0840 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 287 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 287 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 288 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 288 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0843 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 288 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0847 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 288 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 288 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0647 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 289 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 289 : for modelA 1 ==> Training loss= 0.0770 , Validation loss= 0.0840 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 289 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0847 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 289 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0712 , Accuracy = 0.7193\n",
            "Epoch 289 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0643 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 290 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 290 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0846 , Average loss= 0.0834 , Accuracy = 0.2807\n",
            "Epoch 290 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0850 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 290 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 290 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0647 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 291 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 291 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0834 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 291 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0843 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 291 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 291 : for modelA 4 ==> Training loss= 0.0671 , Validation loss= 0.0650 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 292 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 292 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0840 , Average loss= 0.0837 , Accuracy = 0.2807\n",
            "Epoch 292 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0853 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 292 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 292 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0647 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 293 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 293 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0843 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 293 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0843 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 293 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 293 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0647 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 294 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 294 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0840 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 294 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0850 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 294 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0712 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 294 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 295 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 295 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0846 , Average loss= 0.0849 , Accuracy = 0.2807\n",
            "Epoch 295 : for modelA 2 ==> Training loss= 0.0776 , Validation loss= 0.0847 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 295 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 295 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0647 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 296 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 296 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0840 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 296 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0843 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 296 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 296 : for modelA 4 ==> Training loss= 0.0672 , Validation loss= 0.0650 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 297 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 297 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0843 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 297 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0847 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 297 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 297 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0643 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 298 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 298 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0846 , Average loss= 0.0837 , Accuracy = 0.2807\n",
            "Epoch 298 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0847 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 298 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 298 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0650 , Average loss= 0.0657 , Accuracy = 0.7193\n",
            "Epoch 299 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 299 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0840 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 299 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0843 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 299 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0712 , Average loss= 0.0712 , Accuracy = 0.7193\n",
            "Epoch 299 : for modelA 4 ==> Training loss= 0.0666 , Validation loss= 0.0647 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 300 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 300 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0843 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 300 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0847 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 300 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 300 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 301 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 301 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0846 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 301 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0843 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 301 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 301 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0650 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 302 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 302 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0843 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 302 : for modelA 2 ==> Training loss= 0.0776 , Validation loss= 0.0850 , Average loss= 0.0853 , Accuracy = 0.2807\n",
            "Epoch 302 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0712 , Accuracy = 0.7193\n",
            "Epoch 302 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0657 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 303 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 303 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0843 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 303 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0847 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 303 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 303 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0654 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 304 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 304 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0849 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 304 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0843 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 304 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 304 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 305 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 305 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0846 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 305 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0843 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 305 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 305 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0647 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 306 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 306 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0846 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 306 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0847 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 306 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 306 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 307 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 307 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0843 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 307 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0840 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 307 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 307 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 308 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 308 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0843 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 308 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0847 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 308 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 308 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0650 , Average loss= 0.0657 , Accuracy = 0.7193\n",
            "Epoch 309 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 309 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0846 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 309 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0843 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 309 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 309 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0657 , Average loss= 0.0657 , Accuracy = 0.7193\n",
            "Epoch 310 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 310 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0837 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 310 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0847 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 310 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 310 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0654 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 311 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 311 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0843 , Average loss= 0.0849 , Accuracy = 0.2807\n",
            "Epoch 311 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0853 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 311 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 311 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 312 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 312 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0840 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 312 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0843 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 312 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 312 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 313 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 313 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0843 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 313 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0847 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 313 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 313 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0654 , Average loss= 0.0657 , Accuracy = 0.7193\n",
            "Epoch 314 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 314 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0849 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 314 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0850 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 314 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 314 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0647 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 315 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 315 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0846 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 315 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0853 , Average loss= 0.0853 , Accuracy = 0.2807\n",
            "Epoch 315 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0712 , Accuracy = 0.7193\n",
            "Epoch 315 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 316 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 316 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0840 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 316 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0847 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 316 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 316 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0661 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 317 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 317 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0840 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 317 : for modelA 2 ==> Training loss= 0.0776 , Validation loss= 0.0850 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 317 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 317 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0643 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 318 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 318 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0843 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 318 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0843 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 318 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 318 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0650 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 319 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 319 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0843 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 319 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0850 , Average loss= 0.0837 , Accuracy = 0.2807\n",
            "Epoch 319 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 319 : for modelA 4 ==> Training loss= 0.0672 , Validation loss= 0.0643 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 320 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 320 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0840 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 320 : for modelA 2 ==> Training loss= 0.0776 , Validation loss= 0.0850 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 320 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 320 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0647 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 321 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 321 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0846 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 321 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0843 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 321 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 321 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0657 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 322 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 322 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0840 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 322 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0847 , Average loss= 0.0853 , Accuracy = 0.2807\n",
            "Epoch 322 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0712 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 322 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0657 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 323 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 323 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0840 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 323 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0850 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 323 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 323 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0654 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 324 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 324 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0846 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 324 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0847 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 324 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 324 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0647 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 325 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 325 : for modelA 1 ==> Training loss= 0.0774 , Validation loss= 0.0843 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 325 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0847 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 325 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0712 , Accuracy = 0.7193\n",
            "Epoch 325 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0647 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 326 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 326 : for modelA 1 ==> Training loss= 0.0774 , Validation loss= 0.0843 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 326 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0847 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 326 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 326 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0647 , Average loss= 0.0661 , Accuracy = 0.7193\n",
            "Epoch 327 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 327 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0849 , Average loss= 0.0849 , Accuracy = 0.2807\n",
            "Epoch 327 : for modelA 2 ==> Training loss= 0.0776 , Validation loss= 0.0840 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 327 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 327 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0654 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 328 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 328 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0846 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 328 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0847 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 328 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 328 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0643 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 329 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 329 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0846 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 329 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0847 , Average loss= 0.0853 , Accuracy = 0.2807\n",
            "Epoch 329 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0712 , Average loss= 0.0716 , Accuracy = 0.7193\n",
            "Epoch 329 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0643 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 330 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 330 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0840 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 330 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0847 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 330 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0712 , Accuracy = 0.7193\n",
            "Epoch 330 : for modelA 4 ==> Training loss= 0.0666 , Validation loss= 0.0647 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 331 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 331 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0843 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 331 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0847 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 331 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 331 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0647 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 332 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 332 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0846 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 332 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0847 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 332 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 332 : for modelA 4 ==> Training loss= 0.0666 , Validation loss= 0.0654 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 333 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 333 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0846 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 333 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0853 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 333 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 333 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 334 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 334 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0849 , Average loss= 0.0837 , Accuracy = 0.2807\n",
            "Epoch 334 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0847 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 334 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 334 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0654 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 335 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 335 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0846 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 335 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0850 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 335 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0715 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 335 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0647 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 336 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 336 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0843 , Average loss= 0.0837 , Accuracy = 0.2807\n",
            "Epoch 336 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0847 , Average loss= 0.0853 , Accuracy = 0.2807\n",
            "Epoch 336 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 336 : for modelA 4 ==> Training loss= 0.0671 , Validation loss= 0.0654 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 337 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 337 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0843 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 337 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0847 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 337 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 337 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0647 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 338 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 338 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0840 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 338 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0843 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 338 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 338 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 339 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 339 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0843 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 339 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0853 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 339 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 339 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 340 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 340 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0843 , Average loss= 0.0849 , Accuracy = 0.2807\n",
            "Epoch 340 : for modelA 2 ==> Training loss= 0.0776 , Validation loss= 0.0843 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 340 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 340 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0654 , Average loss= 0.0657 , Accuracy = 0.7193\n",
            "Epoch 341 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 341 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0846 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 341 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0840 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 341 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 341 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 342 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 342 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0846 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 342 : for modelA 2 ==> Training loss= 0.0776 , Validation loss= 0.0840 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 342 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 342 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0647 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 343 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 343 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0843 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 343 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0843 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 343 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 343 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0654 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 344 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 344 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0843 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 344 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0850 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 344 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0712 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 344 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0647 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 345 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 345 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0846 , Average loss= 0.0837 , Accuracy = 0.2807\n",
            "Epoch 345 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0853 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 345 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0712 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 345 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0654 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 346 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 346 : for modelA 1 ==> Training loss= 0.0774 , Validation loss= 0.0846 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 346 : for modelA 2 ==> Training loss= 0.0776 , Validation loss= 0.0853 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 346 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 346 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 347 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 347 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0843 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 347 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0850 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 347 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 347 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0654 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 348 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 348 : for modelA 1 ==> Training loss= 0.0770 , Validation loss= 0.0843 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 348 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0850 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 348 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 348 : for modelA 4 ==> Training loss= 0.0666 , Validation loss= 0.0647 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 349 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 349 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0843 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 349 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0843 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 349 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 349 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0654 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 350 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 350 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0843 , Average loss= 0.0837 , Accuracy = 0.2807\n",
            "Epoch 350 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0847 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 350 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0712 , Accuracy = 0.7193\n",
            "Epoch 350 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0647 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 351 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 351 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0843 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 351 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0850 , Average loss= 0.0853 , Accuracy = 0.2807\n",
            "Epoch 351 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0712 , Accuracy = 0.7193\n",
            "Epoch 351 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0647 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 352 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 352 : for modelA 1 ==> Training loss= 0.0774 , Validation loss= 0.0846 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 352 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0847 , Average loss= 0.0853 , Accuracy = 0.2807\n",
            "Epoch 352 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 352 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0647 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 353 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 353 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0843 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 353 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0847 , Average loss= 0.0853 , Accuracy = 0.2807\n",
            "Epoch 353 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 353 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0650 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 354 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 354 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0843 , Average loss= 0.0849 , Accuracy = 0.2807\n",
            "Epoch 354 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0847 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 354 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 354 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0650 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 355 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 355 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0849 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 355 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0850 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 355 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 355 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0647 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 356 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 356 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0849 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 356 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0850 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 356 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 356 : for modelA 4 ==> Training loss= 0.0671 , Validation loss= 0.0650 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 357 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 357 : for modelA 1 ==> Training loss= 0.0770 , Validation loss= 0.0843 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 357 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0847 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 357 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 357 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0647 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 358 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 358 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0843 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 358 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0853 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 358 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 358 : for modelA 4 ==> Training loss= 0.0666 , Validation loss= 0.0650 , Average loss= 0.0657 , Accuracy = 0.7193\n",
            "Epoch 359 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 359 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0843 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 359 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0843 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 359 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 359 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0647 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 360 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 360 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0843 , Average loss= 0.0849 , Accuracy = 0.2807\n",
            "Epoch 360 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0843 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 360 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 360 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0650 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 361 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 361 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0846 , Average loss= 0.0849 , Accuracy = 0.2807\n",
            "Epoch 361 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0850 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 361 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 361 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 362 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 362 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0840 , Average loss= 0.0849 , Accuracy = 0.2807\n",
            "Epoch 362 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0850 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 362 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 362 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0647 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 363 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 363 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0843 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 363 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0850 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 363 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 363 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0654 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 364 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 364 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0840 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 364 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0847 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 364 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0712 , Accuracy = 0.7193\n",
            "Epoch 364 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0654 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 365 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 365 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0843 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 365 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0840 , Average loss= 0.0853 , Accuracy = 0.2807\n",
            "Epoch 365 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 365 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0657 , Accuracy = 0.7193\n",
            "Epoch 366 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 366 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0837 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 366 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0853 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 366 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 366 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 367 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 367 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0846 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 367 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0847 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 367 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0712 , Accuracy = 0.7193\n",
            "Epoch 367 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 368 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 368 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0846 , Average loss= 0.0849 , Accuracy = 0.2807\n",
            "Epoch 368 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0853 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 368 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 368 : for modelA 4 ==> Training loss= 0.0671 , Validation loss= 0.0657 , Average loss= 0.0661 , Accuracy = 0.7193\n",
            "Epoch 369 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 369 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0840 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 369 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0847 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 369 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 369 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0657 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 370 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 370 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0846 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 370 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0847 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 370 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0716 , Accuracy = 0.7193\n",
            "Epoch 370 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 371 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 371 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0849 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 371 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0847 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 371 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0712 , Accuracy = 0.7193\n",
            "Epoch 371 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0657 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 372 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 372 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0849 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 372 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0847 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 372 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 372 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0654 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 373 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 373 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0846 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 373 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0847 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 373 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0712 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 373 : for modelA 4 ==> Training loss= 0.0666 , Validation loss= 0.0647 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 374 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 374 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0846 , Average loss= 0.0837 , Accuracy = 0.2807\n",
            "Epoch 374 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0850 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 374 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 374 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0650 , Average loss= 0.0661 , Accuracy = 0.7193\n",
            "Epoch 375 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 375 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0846 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 375 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0847 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 375 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 375 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0654 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 376 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 376 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0843 , Average loss= 0.0849 , Accuracy = 0.2807\n",
            "Epoch 376 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0853 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 376 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 376 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0650 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 377 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 377 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0843 , Average loss= 0.0849 , Accuracy = 0.2807\n",
            "Epoch 377 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0850 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 377 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 377 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0647 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 378 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 378 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0846 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 378 : for modelA 2 ==> Training loss= 0.0776 , Validation loss= 0.0850 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 378 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 378 : for modelA 4 ==> Training loss= 0.0671 , Validation loss= 0.0650 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 379 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 379 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0840 , Average loss= 0.0849 , Accuracy = 0.2807\n",
            "Epoch 379 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0853 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 379 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0712 , Accuracy = 0.7193\n",
            "Epoch 379 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0647 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 380 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 380 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0843 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 380 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0843 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 380 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 380 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 381 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 381 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0840 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 381 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0837 , Average loss= 0.0853 , Accuracy = 0.2807\n",
            "Epoch 381 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 381 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 382 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 382 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0843 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 382 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0850 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 382 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 382 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 383 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 383 : for modelA 1 ==> Training loss= 0.0774 , Validation loss= 0.0843 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 383 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0850 , Average loss= 0.0853 , Accuracy = 0.2807\n",
            "Epoch 383 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 383 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0647 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 384 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 384 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0843 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 384 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0850 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 384 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 384 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0647 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 385 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 385 : for modelA 1 ==> Training loss= 0.0774 , Validation loss= 0.0843 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 385 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0850 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 385 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 385 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0647 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 386 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 386 : for modelA 1 ==> Training loss= 0.0774 , Validation loss= 0.0846 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 386 : for modelA 2 ==> Training loss= 0.0776 , Validation loss= 0.0847 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 386 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 386 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0661 , Accuracy = 0.7193\n",
            "Epoch 387 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 387 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0843 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 387 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0850 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 387 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 387 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0654 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 388 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 388 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0843 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 388 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0850 , Average loss= 0.0853 , Accuracy = 0.2807\n",
            "Epoch 388 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 388 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0661 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 389 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 389 : for modelA 1 ==> Training loss= 0.0774 , Validation loss= 0.0849 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 389 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0843 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 389 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 389 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0654 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 390 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 390 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0849 , Average loss= 0.0849 , Accuracy = 0.2807\n",
            "Epoch 390 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0847 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 390 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 390 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0654 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 391 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 391 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0843 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 391 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0847 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 391 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0712 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 391 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0654 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 392 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 392 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0840 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 392 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0840 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 392 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 392 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0647 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 393 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 393 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0843 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 393 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0847 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 393 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 393 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0654 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 394 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 394 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0846 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 394 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0853 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 394 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 394 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0657 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 395 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 395 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0846 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 395 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0847 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 395 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 395 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0647 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 396 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 396 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0846 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 396 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0847 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 396 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 396 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0647 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 397 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 397 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0849 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 397 : for modelA 2 ==> Training loss= 0.0776 , Validation loss= 0.0847 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 397 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 397 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 398 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 398 : for modelA 1 ==> Training loss= 0.0774 , Validation loss= 0.0846 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 398 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0850 , Average loss= 0.0837 , Accuracy = 0.2807\n",
            "Epoch 398 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0712 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 398 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 399 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 399 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0849 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 399 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0850 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 399 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 399 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0654 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 400 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 400 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0846 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 400 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0850 , Average loss= 0.0853 , Accuracy = 0.2807\n",
            "Epoch 400 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 400 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0647 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 401 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 401 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0849 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 401 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0843 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 401 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 401 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0650 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 402 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 402 : for modelA 1 ==> Training loss= 0.0774 , Validation loss= 0.0843 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 402 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0850 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 402 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 402 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 403 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 403 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0837 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 403 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0853 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 403 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 403 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 404 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 404 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0840 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 404 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0850 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 404 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 404 : for modelA 4 ==> Training loss= 0.0672 , Validation loss= 0.0657 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 405 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 405 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0849 , Average loss= 0.0837 , Accuracy = 0.2807\n",
            "Epoch 405 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0850 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 405 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 405 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0647 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 406 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 406 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0840 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 406 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0850 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 406 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0712 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 406 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0657 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 407 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 407 : for modelA 1 ==> Training loss= 0.0770 , Validation loss= 0.0846 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 407 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0853 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 407 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 407 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0650 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 408 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 408 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0840 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 408 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0850 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 408 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 408 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0650 , Average loss= 0.0657 , Accuracy = 0.7193\n",
            "Epoch 409 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 409 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0843 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 409 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0850 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 409 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 409 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0647 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 410 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 410 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0843 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 410 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0850 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 410 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 410 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0654 , Average loss= 0.0657 , Accuracy = 0.7193\n",
            "Epoch 411 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 411 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0840 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 411 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0847 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 411 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 411 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0643 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 412 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 412 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0840 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 412 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0847 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 412 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 412 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0654 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 413 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 413 : for modelA 1 ==> Training loss= 0.0774 , Validation loss= 0.0843 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 413 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0853 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 413 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 413 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 414 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 414 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0849 , Average loss= 0.0849 , Accuracy = 0.2807\n",
            "Epoch 414 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0847 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 414 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 414 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 415 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 415 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0846 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 415 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0843 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 415 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 415 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0647 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 416 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 416 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0849 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 416 : for modelA 2 ==> Training loss= 0.0776 , Validation loss= 0.0840 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 416 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0715 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 416 : for modelA 4 ==> Training loss= 0.0671 , Validation loss= 0.0647 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 417 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 417 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0849 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 417 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0850 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 417 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 417 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0654 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 418 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 418 : for modelA 1 ==> Training loss= 0.0774 , Validation loss= 0.0846 , Average loss= 0.0837 , Accuracy = 0.2807\n",
            "Epoch 418 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0847 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 418 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 418 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0654 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 419 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 419 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0840 , Average loss= 0.0849 , Accuracy = 0.2807\n",
            "Epoch 419 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0853 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 419 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 419 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0643 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 420 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 420 : for modelA 1 ==> Training loss= 0.0774 , Validation loss= 0.0846 , Average loss= 0.0837 , Accuracy = 0.2807\n",
            "Epoch 420 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0840 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 420 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 420 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 421 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 421 : for modelA 1 ==> Training loss= 0.0774 , Validation loss= 0.0843 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 421 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0850 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 421 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 421 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0647 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 422 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 422 : for modelA 1 ==> Training loss= 0.0770 , Validation loss= 0.0846 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 422 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0847 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 422 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0712 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 422 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0661 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 423 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 423 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0846 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 423 : for modelA 2 ==> Training loss= 0.0776 , Validation loss= 0.0850 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 423 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 423 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0654 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 424 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 424 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0840 , Average loss= 0.0849 , Accuracy = 0.2807\n",
            "Epoch 424 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0837 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 424 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 424 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0654 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 425 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 425 : for modelA 1 ==> Training loss= 0.0770 , Validation loss= 0.0846 , Average loss= 0.0849 , Accuracy = 0.2807\n",
            "Epoch 425 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0850 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 425 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 425 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0643 , Average loss= 0.0657 , Accuracy = 0.7193\n",
            "Epoch 426 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 426 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0840 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 426 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0847 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 426 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 426 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0647 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 427 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 427 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0849 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 427 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0847 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 427 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 427 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0654 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 428 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 428 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0846 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 428 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0847 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 428 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 428 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0647 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 429 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 429 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0843 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 429 : for modelA 2 ==> Training loss= 0.0776 , Validation loss= 0.0847 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 429 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 429 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0650 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 430 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 430 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0840 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 430 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0853 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 430 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 430 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0650 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 431 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 431 : for modelA 1 ==> Training loss= 0.0774 , Validation loss= 0.0846 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 431 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0847 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 431 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 431 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0661 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 432 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 432 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0840 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 432 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0837 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 432 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 432 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0643 , Average loss= 0.0657 , Accuracy = 0.7193\n",
            "Epoch 433 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 433 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0849 , Average loss= 0.0849 , Accuracy = 0.2807\n",
            "Epoch 433 : for modelA 2 ==> Training loss= 0.0776 , Validation loss= 0.0847 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 433 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0715 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 433 : for modelA 4 ==> Training loss= 0.0672 , Validation loss= 0.0643 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 434 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 434 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0843 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 434 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0847 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 434 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 434 : for modelA 4 ==> Training loss= 0.0666 , Validation loss= 0.0647 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 435 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 435 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0843 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 435 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0847 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 435 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0715 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 435 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0650 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 436 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 436 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0840 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 436 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0843 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 436 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 436 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 437 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 437 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0846 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 437 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0847 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 437 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 437 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0654 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 438 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 438 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0846 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 438 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0853 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 438 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 438 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0654 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 439 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 439 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0843 , Average loss= 0.0849 , Accuracy = 0.2807\n",
            "Epoch 439 : for modelA 2 ==> Training loss= 0.0776 , Validation loss= 0.0847 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 439 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0715 , Accuracy = 0.7193\n",
            "Epoch 439 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0647 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 440 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 440 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0846 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 440 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0847 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 440 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 440 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0647 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 441 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 441 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0840 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 441 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0850 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 441 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 441 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0657 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 442 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 442 : for modelA 1 ==> Training loss= 0.0770 , Validation loss= 0.0843 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 442 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0850 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 442 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 442 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0650 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 443 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 443 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0843 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 443 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0853 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 443 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 443 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0643 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 444 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 444 : for modelA 1 ==> Training loss= 0.0774 , Validation loss= 0.0843 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 444 : for modelA 2 ==> Training loss= 0.0776 , Validation loss= 0.0850 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 444 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0715 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 444 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 445 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 445 : for modelA 1 ==> Training loss= 0.0774 , Validation loss= 0.0843 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 445 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0843 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 445 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 445 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 446 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 446 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0846 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 446 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0853 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 446 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 446 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 447 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 447 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0846 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 447 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0843 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 447 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0712 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 447 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0650 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 448 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 448 : for modelA 1 ==> Training loss= 0.0774 , Validation loss= 0.0840 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 448 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0850 , Average loss= 0.0853 , Accuracy = 0.2807\n",
            "Epoch 448 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 448 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0647 , Average loss= 0.0661 , Accuracy = 0.7193\n",
            "Epoch 449 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 449 : for modelA 1 ==> Training loss= 0.0774 , Validation loss= 0.0840 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 449 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0847 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 449 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 449 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0657 , Average loss= 0.0657 , Accuracy = 0.7193\n",
            "Epoch 450 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 450 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0843 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 450 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0847 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 450 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 450 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 451 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 451 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0843 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 451 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0847 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 451 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 451 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0643 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 452 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 452 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0846 , Average loss= 0.0837 , Accuracy = 0.2807\n",
            "Epoch 452 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0853 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 452 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 452 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0654 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 453 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 453 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0849 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 453 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0840 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 453 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 453 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0643 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 454 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 454 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0840 , Average loss= 0.0849 , Accuracy = 0.2807\n",
            "Epoch 454 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0847 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 454 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0712 , Accuracy = 0.7193\n",
            "Epoch 454 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0650 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 455 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 455 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0843 , Average loss= 0.0837 , Accuracy = 0.2807\n",
            "Epoch 455 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0843 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 455 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 455 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0643 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 456 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 456 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0846 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 456 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0847 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 456 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 456 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0647 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 457 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 457 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0840 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 457 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0843 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 457 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 457 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0647 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 458 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 458 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0849 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 458 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0847 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 458 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 458 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0661 , Average loss= 0.0643 , Accuracy = 0.7193\n",
            "Epoch 459 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 459 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0843 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 459 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0850 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 459 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 459 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 460 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 460 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0846 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 460 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0843 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 460 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 460 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0654 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 461 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 461 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0840 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 461 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0847 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 461 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 461 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0647 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 462 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 462 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0843 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 462 : for modelA 2 ==> Training loss= 0.0776 , Validation loss= 0.0847 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 462 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 462 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0654 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 463 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 463 : for modelA 1 ==> Training loss= 0.0770 , Validation loss= 0.0840 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 463 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0853 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 463 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 463 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0647 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 464 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 464 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0843 , Average loss= 0.0849 , Accuracy = 0.2807\n",
            "Epoch 464 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0850 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 464 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0712 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 464 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0650 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 465 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 465 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0843 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 465 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0843 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 465 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 465 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0654 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 466 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 466 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0840 , Average loss= 0.0849 , Accuracy = 0.2807\n",
            "Epoch 466 : for modelA 2 ==> Training loss= 0.0776 , Validation loss= 0.0850 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 466 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 466 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0657 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 467 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 467 : for modelA 1 ==> Training loss= 0.0774 , Validation loss= 0.0837 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 467 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0847 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 467 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 467 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0654 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 468 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 468 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0849 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 468 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0843 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 468 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 468 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0657 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 469 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 469 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0846 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 469 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0850 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 469 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 469 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0647 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 470 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 470 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0849 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 470 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0853 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 470 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 470 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0654 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 471 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 471 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0849 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 471 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0840 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 471 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 471 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0643 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 472 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 472 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0840 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 472 : for modelA 2 ==> Training loss= 0.0776 , Validation loss= 0.0847 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 472 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 472 : for modelA 4 ==> Training loss= 0.0666 , Validation loss= 0.0647 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 473 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 473 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0840 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 473 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0840 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 473 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 473 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0657 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 474 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 474 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0843 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 474 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0847 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 474 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 474 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0654 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 475 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 475 : for modelA 1 ==> Training loss= 0.0770 , Validation loss= 0.0846 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 475 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0847 , Average loss= 0.0837 , Accuracy = 0.2807\n",
            "Epoch 475 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0712 , Average loss= 0.0712 , Accuracy = 0.7193\n",
            "Epoch 475 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0647 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 476 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 476 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0843 , Average loss= 0.0837 , Accuracy = 0.2807\n",
            "Epoch 476 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0850 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 476 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 476 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0650 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 477 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 477 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0843 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 477 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0847 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 477 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 477 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0661 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 478 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 478 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0846 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 478 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0847 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 478 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0712 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 478 : for modelA 4 ==> Training loss= 0.0666 , Validation loss= 0.0650 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 479 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 479 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0843 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 479 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0847 , Average loss= 0.0853 , Accuracy = 0.2807\n",
            "Epoch 479 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 479 : for modelA 4 ==> Training loss= 0.0671 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 480 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 480 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0843 , Average loss= 0.0849 , Accuracy = 0.2807\n",
            "Epoch 480 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0847 , Average loss= 0.0853 , Accuracy = 0.2807\n",
            "Epoch 480 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0712 , Accuracy = 0.7193\n",
            "Epoch 480 : for modelA 4 ==> Training loss= 0.0666 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 481 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 481 : for modelA 1 ==> Training loss= 0.0774 , Validation loss= 0.0840 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 481 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0847 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 481 : for modelA 3 ==> Training loss= 0.0693 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 481 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0647 , Average loss= 0.0657 , Accuracy = 0.7193\n",
            "Epoch 482 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 482 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0843 , Average loss= 0.0849 , Accuracy = 0.2807\n",
            "Epoch 482 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0850 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 482 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 482 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0647 , Average loss= 0.0657 , Accuracy = 0.7193\n",
            "Epoch 483 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 483 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0846 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 483 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0847 , Average loss= 0.0853 , Accuracy = 0.2807\n",
            "Epoch 483 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 483 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0643 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 484 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 484 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0840 , Average loss= 0.0849 , Accuracy = 0.2807\n",
            "Epoch 484 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0847 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 484 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 484 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0647 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 485 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 485 : for modelA 1 ==> Training loss= 0.0770 , Validation loss= 0.0846 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 485 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0843 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 485 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 485 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 486 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 486 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0837 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 486 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0850 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 486 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0712 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 486 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0647 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 487 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 487 : for modelA 1 ==> Training loss= 0.0774 , Validation loss= 0.0843 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 487 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0853 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 487 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 487 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0647 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 488 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 488 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0840 , Average loss= 0.0849 , Accuracy = 0.2807\n",
            "Epoch 488 : for modelA 2 ==> Training loss= 0.0776 , Validation loss= 0.0853 , Average loss= 0.0853 , Accuracy = 0.2807\n",
            "Epoch 488 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 488 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 489 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 489 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0846 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 489 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0840 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 489 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 489 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0654 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 490 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 490 : for modelA 1 ==> Training loss= 0.0774 , Validation loss= 0.0843 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 490 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0850 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 490 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 490 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0647 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 491 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 491 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0846 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 491 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0847 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 491 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 491 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0647 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 492 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 492 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0843 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 492 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0843 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 492 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 492 : for modelA 4 ==> Training loss= 0.0670 , Validation loss= 0.0647 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 493 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 493 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0849 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 493 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0847 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 493 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 493 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 494 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 494 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0840 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 494 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0843 , Average loss= 0.0850 , Accuracy = 0.2807\n",
            "Epoch 494 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 494 : for modelA 4 ==> Training loss= 0.0667 , Validation loss= 0.0643 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 495 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 495 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0840 , Average loss= 0.0846 , Accuracy = 0.2807\n",
            "Epoch 495 : for modelA 2 ==> Training loss= 0.0772 , Validation loss= 0.0850 , Average loss= 0.0853 , Accuracy = 0.2807\n",
            "Epoch 495 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 495 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0643 , Average loss= 0.0654 , Accuracy = 0.7193\n",
            "Epoch 496 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 496 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0843 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 496 : for modelA 2 ==> Training loss= 0.0775 , Validation loss= 0.0853 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 496 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 496 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0654 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 497 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 497 : for modelA 1 ==> Training loss= 0.0772 , Validation loss= 0.0840 , Average loss= 0.0849 , Accuracy = 0.2807\n",
            "Epoch 497 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0843 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 497 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0714 , Average loss= 0.0714 , Accuracy = 0.7193\n",
            "Epoch 497 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0647 , Average loss= 0.0647 , Accuracy = 0.7193\n",
            "Epoch 498 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 498 : for modelA 1 ==> Training loss= 0.0773 , Validation loss= 0.0837 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 498 : for modelA 2 ==> Training loss= 0.0773 , Validation loss= 0.0847 , Average loss= 0.0847 , Accuracy = 0.2807\n",
            "Epoch 498 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 498 : for modelA 4 ==> Training loss= 0.0669 , Validation loss= 0.0650 , Average loss= 0.0650 , Accuracy = 0.7193\n",
            "Epoch 499 : for modelA 0 ==> Training loss= 0.0701 , Validation loss= 0.0730 , Average loss= 0.0730 , Accuracy = 0.2807\n",
            "Epoch 499 : for modelA 1 ==> Training loss= 0.0771 , Validation loss= 0.0846 , Average loss= 0.0840 , Accuracy = 0.2807\n",
            "Epoch 499 : for modelA 2 ==> Training loss= 0.0774 , Validation loss= 0.0837 , Average loss= 0.0843 , Accuracy = 0.2807\n",
            "Epoch 499 : for modelA 3 ==> Training loss= 0.0692 , Validation loss= 0.0713 , Average loss= 0.0713 , Accuracy = 0.7193\n",
            "Epoch 499 : for modelA 4 ==> Training loss= 0.0671 , Validation loss= 0.0647 , Average loss= 0.0657 , Accuracy = 0.7193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt #python package to draw graphs in python\n",
        "\n",
        "plt.plot(epoch_nums, training_loss[0])\n",
        "plt.plot(epoch_nums, training_loss[1])\n",
        "plt.plot(epoch_nums, training_loss[2])\n",
        "plt.plot(epoch_nums, training_loss[3])\n",
        "plt.plot(epoch_nums, training_loss[4])\n",
        "\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('Training loss')\n",
        "plt.legend(['3 hidden layers', '9 hidden layers', '13 hidden layers', '17 hidden layers', '19 hidden layers'], loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "wXDhIhWD9aLi",
        "outputId": "83f3088b-1377-4290-c47e-9115e8e69b45"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(epoch_nums, validation_loss[0])\n",
        "plt.plot(epoch_nums, validation_loss[1])\n",
        "plt.plot(epoch_nums, validation_loss[2])\n",
        "plt.plot(epoch_nums, validation_loss[3])\n",
        "plt.plot(epoch_nums, validation_loss[4])\n",
        "\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('Valdiation loss')\n",
        "plt.legend(['3 hidden layers', '9 hidden layers', '13 hidden layers', '17 hidden layers', '19 hidden layers'], loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "xSRnB6LR9dih",
        "outputId": "0e74b5fa-6e2a-4190-82c2-6eb1854d230d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}