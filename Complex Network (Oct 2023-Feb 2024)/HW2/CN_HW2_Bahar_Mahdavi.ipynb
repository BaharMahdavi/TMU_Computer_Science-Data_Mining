{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#The Complex Network Homework **No. 2**\n",
        "Bahar Mahdavi - SN: 40152521337"
      ],
      "metadata": {
        "id": "qJ1ZhKMeUVZQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A. call the MUTAG dataset in torch_geometric.datasets and print its features including the number of available graphs, the number of classes, and the length of the feature vector."
      ],
      "metadata": {
        "id": "vgzoIcEHk8zf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###The installation of PyTorch Geometric (PyG) on Colab\n",
        "PyTorch Geometric is an extension library to the popular deep learning framework PyTorch, and consists of various methods and utilities to ease the implementation of Graph Neural Networks.\n",
        "https://pytorch-geometric.readthedocs.io/en/latest/notes/installation.html"
      ],
      "metadata": {
        "id": "f8DVjYVQmnBG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install torch geometric\n",
        "!pip install torch-geometric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WzmoZNQIIOiI",
        "outputId": "36cef577-5062-40aa-9bcf-99d65dc4e66f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.10/dist-packages (2.4.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.11.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2023.11.17)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Loading dataset"
      ],
      "metadata": {
        "id": "Mu_OGRufMxxj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **MUTAG dataset** consists of 188 chemical compounds divided into two classes according to their mutagenic effect on a bacterium.\n",
        "\n",
        "MUTAG is a dataset contains 188 graphs representing molecules where the nodes are atoms (C, N, O, F, I, Cl, or Br) and the undirected edges are bonds (aromatic, single, double, or triple). Each graph comes with a target value of either 1 indicating mutagenicity (causing genetic mutation) or 0, meaning no mutagenicity. (DOI: 10.1021/jm00106a046)\n",
        "\n",
        "The chemical data was obtained form http://cdb.ics.uci.edu and converted to graphs, where vertices represent atoms and edges represent chemical bonds. Explicit hydrogen atoms have been removed and vertices are labeled by atom type and edges by bond type (single, double, triple or aromatic). Chemical data was processed using the Chemistry Development Kit (v1.4)."
      ],
      "metadata": {
        "id": "YYY3vNvSrjPY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch_geometric.datasets import TUDataset\n",
        "\n",
        "# Load the MUTAG dataset\n",
        "dataset = TUDataset(root='data/TUDataset', name='MUTAG')\n",
        "dataset = dataset.shuffle()\n",
        "data = dataset[0]  # Get the first graph object.\n",
        "\n",
        "dataset.download()"
      ],
      "metadata": {
        "id": "_5_xJe_PIaDc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82949936-07c7-41c3-a1d7-6c3ac8ac594f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://www.chrsmrrs.com/graphkerneldatasets/MUTAG.zip\n",
            "Extracting data/TUDataset/MUTAG/MUTAG.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print dataset features.\n",
        "print(f'Dataset: {dataset}:')\n",
        "print('=====================')\n",
        "print(f'Number of graphs: {len(dataset)}')\n",
        "print(f'Number of classes: {dataset.num_classes}')\n",
        "print(f'Number of features: {dataset.num_features}')\n",
        "print()\n",
        "print(f'Number of node features: {dataset.num_node_features}')\n",
        "print(f'Number of edge features: {dataset.num_edge_features}')\n",
        "print(f'Number of node labels: {dataset.num_node_labels}')\n",
        "print(f'Number of edge labels: {dataset.num_edge_labels}')\n",
        "print()\n",
        "print(data)\n",
        "print('=============================================================')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H265LE7Dr2_1",
        "outputId": "8c95b0a6-f084-4129-fa46-a43203d81e1e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset: MUTAG(188):\n",
            "=====================\n",
            "Number of graphs: 188\n",
            "Number of classes: 2\n",
            "Number of features: 7\n",
            "\n",
            "Number of node features: 7\n",
            "Number of edge features: 4\n",
            "Number of node labels: 7\n",
            "Number of edge labels: 4\n",
            "\n",
            "Data(edge_index=[2, 44], x=[21, 7], edge_attr=[44, 4], y=[1])\n",
            "=============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Gather some statistics about the first graph.\n",
        "print(f'Number of nodes: {data.num_nodes}')\n",
        "print(f'Number of edges: {data.num_edges}')\n",
        "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
        "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
        "print(f'Has self-loops: {data.has_self_loops()}')\n",
        "print(f'Is undirected: {data.is_undirected()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BlZAs4HauG3V",
        "outputId": "1f55ebcc-1da6-406c-a58e-72845488d397"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of nodes: 21\n",
            "Number of edges: 44\n",
            "Average node degree: 2.10\n",
            "Has isolated nodes: False\n",
            "Has self-loops: False\n",
            "Is undirected: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Graph visualization\n"
      ],
      "metadata": {
        "id": "rLoQ6R8_Fya0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "from torch_geometric.utils import to_networkx\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def visualize_graph(G, color):\n",
        "    plt.figure(figsize=(7,7))\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    nx.draw_networkx(G,\n",
        "                     pos=nx.spring_layout(G, seed=10),\n",
        "                     with_labels=True,\n",
        "                     node_color='orchid',\n",
        "                     cmap=\"Set2\")\n",
        "    plt.show()\n",
        "\n",
        "G = to_networkx(data, to_undirected=True)\n",
        "visualize_graph(G, color=data.y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "I_dshTf9vpCL",
        "outputId": "18ab1171-8479-4d53-f1f7-8776b161e51c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/networkx/drawing/nx_pylab.py:437: UserWarning: No data for colormapping provided via 'c'. Parameters 'cmap' will be ignored\n",
            "  node_collection = ax.scatter(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 700x700 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "G.nodes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0uLDx28GUIi",
        "outputId": "f6048a23-0471-4f68-aff6-8ff773afebcd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NodeView((0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "G.edges"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15bFXO5uG8GX",
        "outputId": "5507abe1-b0e6-4bf9-d9fb-63a2dfff45cf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "EdgeView([(0, 1), (0, 5), (1, 2), (2, 3), (2, 18), (3, 4), (4, 5), (5, 6), (6, 7), (6, 11), (7, 8), (8, 9), (9, 10), (9, 15), (10, 11), (11, 12), (12, 13), (12, 14), (15, 16), (15, 17), (18, 19), (18, 20)])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## B. Design and train a graph neural network model to classify the given dataset and valuate the accuracy of your model and"
      ],
      "metadata": {
        "id": "jIGJ1J0x3ihV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split dataset into train and test"
      ],
      "metadata": {
        "id": "QQE8spBSNGTs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(10)\n",
        "dataset = dataset.shuffle()\n",
        "\n",
        "train_dataset = dataset[:150]\n",
        "test_dataset = dataset[150:]\n",
        "\n",
        "print(f'Number of training graphs: {len(train_dataset)}')\n",
        "print(f'Number of test graphs: {len(test_dataset)}')"
      ],
      "metadata": {
        "id": "1BRIx_Hv28aM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eab80463-1d1e-422e-f192-81eed5f25eab"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training graphs: 150\n",
            "Number of test graphs: 38\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "for step, data in enumerate(train_loader):\n",
        "    print(f'Step {step + 1}:')\n",
        "    print('=======')\n",
        "    print(f'Number of graphs in the current batch: {data.num_graphs}')\n",
        "    print(data)\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ecCqULXxQ_D",
        "outputId": "3762198f-8745-4519-cb3d-99e201dd9054"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1:\n",
            "=======\n",
            "Number of graphs in the current batch: 64\n",
            "DataBatch(edge_index=[2, 2572], x=[1163, 7], edge_attr=[2572, 4], y=[64], batch=[1163], ptr=[65])\n",
            "\n",
            "Step 2:\n",
            "=======\n",
            "Number of graphs in the current batch: 64\n",
            "DataBatch(edge_index=[2, 2396], x=[1094, 7], edge_attr=[2396, 4], y=[64], batch=[1094], ptr=[65])\n",
            "\n",
            "Step 3:\n",
            "=======\n",
            "Number of graphs in the current batch: 22\n",
            "DataBatch(edge_index=[2, 918], x=[416, 7], edge_attr=[918, 4], y=[22], batch=[416], ptr=[23])\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementing the model"
      ],
      "metadata": {
        "id": "oKoeP_Rl8J-p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import Linear\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GraphConv\n",
        "from torch_geometric.nn import global_mean_pool\n",
        "\n",
        "class GNN(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels):\n",
        "        super(GNN, self).__init__()\n",
        "        torch.manual_seed(10)\n",
        "        self.conv1 = GraphConv(dataset.num_node_features, hidden_channels)\n",
        "        self.conv2 = GraphConv(hidden_channels, hidden_channels)\n",
        "        self.conv3 = GraphConv(hidden_channels, hidden_channels)\n",
        "        self.lin = Linear(hidden_channels, dataset.num_classes)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        # 1. Obtain node embeddings\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv3(x, edge_index)\n",
        "\n",
        "        # 2. Readout layer\n",
        "        x = global_mean_pool(x, batch) # Pool node features to graph-level [batch_size, hidden_channels]\n",
        "\n",
        "        # 3. Apply a final classifier\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.lin(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "model = GNN(hidden_channels=64)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBawgGwn4uzR",
        "outputId": "d36ff624-13d7-4f3f-e430-084c6f817c76"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GNN(\n",
            "  (conv1): GraphConv(7, 64)\n",
            "  (conv2): GraphConv(64, 64)\n",
            "  (conv3): GraphConv(64, 64)\n",
            "  (lin): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training and test"
      ],
      "metadata": {
        "id": "pmwQCD808Xr5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Javascript\n",
        "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "\n",
        "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
        "         out = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
        "         loss = criterion(out, data.y)  # Compute the loss.\n",
        "         loss.backward()  # Derive gradients.\n",
        "         optimizer.step()  # Update parameters based on gradients.\n",
        "         optimizer.zero_grad()  # Clear gradients.\n",
        "\n",
        "def test(loader):\n",
        "     model.eval()\n",
        "\n",
        "     correct = 0\n",
        "     loss_ = 0\n",
        "     for data in loader:  # Iterate in batches over the training/test dataset.\n",
        "         out = model(data.x, data.edge_index, data.batch)\n",
        "         loss = criterion(out, data.y)\n",
        "         loss_ += loss.item()\n",
        "         pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
        "         correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
        "     return correct / len(loader.dataset), loss_ / len(loader.dataset)  # Derive ratio of correct predictions.\n",
        "\n",
        "for epoch in range(1, 301):\n",
        "    train()\n",
        "    train_acc, train_loss = test(train_loader)\n",
        "    test_acc, test_loss = test(test_loader)\n",
        "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "nME3b4zXJhgR",
        "outputId": "9e9a187d-870b-4eb3-f83d-5ce14c94f4ff"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "google.colab.output.setIframeHeight(0, true, {maxHeight: 300})"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Train Acc: 0.6533, Test Acc: 0.7105, Train Loss: 0.0145, Test Loss: 0.0168\n",
            "Epoch: 002, Train Acc: 0.6533, Test Acc: 0.7105, Train Loss: 0.0122, Test Loss: 0.0146\n",
            "Epoch: 003, Train Acc: 0.6533, Test Acc: 0.7105, Train Loss: 0.0130, Test Loss: 0.0169\n",
            "Epoch: 004, Train Acc: 0.6533, Test Acc: 0.7105, Train Loss: 0.0126, Test Loss: 0.0163\n",
            "Epoch: 005, Train Acc: 0.6533, Test Acc: 0.7105, Train Loss: 0.0121, Test Loss: 0.0148\n",
            "Epoch: 006, Train Acc: 0.6533, Test Acc: 0.7105, Train Loss: 0.0117, Test Loss: 0.0135\n",
            "Epoch: 007, Train Acc: 0.6533, Test Acc: 0.7105, Train Loss: 0.0104, Test Loss: 0.0131\n",
            "Epoch: 008, Train Acc: 0.6533, Test Acc: 0.7105, Train Loss: 0.0105, Test Loss: 0.0129\n",
            "Epoch: 009, Train Acc: 0.7267, Test Acc: 0.6842, Train Loss: 0.0103, Test Loss: 0.0122\n",
            "Epoch: 010, Train Acc: 0.7267, Test Acc: 0.7368, Train Loss: 0.0097, Test Loss: 0.0114\n",
            "Epoch: 011, Train Acc: 0.7400, Test Acc: 0.7632, Train Loss: 0.0099, Test Loss: 0.0109\n",
            "Epoch: 012, Train Acc: 0.8000, Test Acc: 0.7895, Train Loss: 0.0093, Test Loss: 0.0109\n",
            "Epoch: 013, Train Acc: 0.8067, Test Acc: 0.7895, Train Loss: 0.0088, Test Loss: 0.0103\n",
            "Epoch: 014, Train Acc: 0.8067, Test Acc: 0.7895, Train Loss: 0.0088, Test Loss: 0.0101\n",
            "Epoch: 015, Train Acc: 0.8000, Test Acc: 0.7895, Train Loss: 0.0092, Test Loss: 0.0100\n",
            "Epoch: 016, Train Acc: 0.7800, Test Acc: 0.8158, Train Loss: 0.0092, Test Loss: 0.0108\n",
            "Epoch: 017, Train Acc: 0.7800, Test Acc: 0.7895, Train Loss: 0.0084, Test Loss: 0.0100\n",
            "Epoch: 018, Train Acc: 0.7800, Test Acc: 0.7632, Train Loss: 0.0081, Test Loss: 0.0099\n",
            "Epoch: 019, Train Acc: 0.7800, Test Acc: 0.7895, Train Loss: 0.0081, Test Loss: 0.0100\n",
            "Epoch: 020, Train Acc: 0.7867, Test Acc: 0.7632, Train Loss: 0.0086, Test Loss: 0.0113\n",
            "Epoch: 021, Train Acc: 0.8000, Test Acc: 0.7895, Train Loss: 0.0076, Test Loss: 0.0105\n",
            "Epoch: 022, Train Acc: 0.7733, Test Acc: 0.7632, Train Loss: 0.0076, Test Loss: 0.0102\n",
            "Epoch: 023, Train Acc: 0.7867, Test Acc: 0.7895, Train Loss: 0.0076, Test Loss: 0.0103\n",
            "Epoch: 024, Train Acc: 0.7733, Test Acc: 0.7632, Train Loss: 0.0072, Test Loss: 0.0102\n",
            "Epoch: 025, Train Acc: 0.7867, Test Acc: 0.7895, Train Loss: 0.0076, Test Loss: 0.0102\n",
            "Epoch: 026, Train Acc: 0.7933, Test Acc: 0.7368, Train Loss: 0.0076, Test Loss: 0.0101\n",
            "Epoch: 027, Train Acc: 0.7933, Test Acc: 0.7632, Train Loss: 0.0082, Test Loss: 0.0106\n",
            "Epoch: 028, Train Acc: 0.7933, Test Acc: 0.7368, Train Loss: 0.0074, Test Loss: 0.0100\n",
            "Epoch: 029, Train Acc: 0.7933, Test Acc: 0.7632, Train Loss: 0.0077, Test Loss: 0.0100\n",
            "Epoch: 030, Train Acc: 0.7933, Test Acc: 0.7368, Train Loss: 0.0068, Test Loss: 0.0100\n",
            "Epoch: 031, Train Acc: 0.8067, Test Acc: 0.7632, Train Loss: 0.0069, Test Loss: 0.0102\n",
            "Epoch: 032, Train Acc: 0.7867, Test Acc: 0.7368, Train Loss: 0.0071, Test Loss: 0.0102\n",
            "Epoch: 033, Train Acc: 0.7867, Test Acc: 0.7368, Train Loss: 0.0074, Test Loss: 0.0103\n",
            "Epoch: 034, Train Acc: 0.7867, Test Acc: 0.7895, Train Loss: 0.0071, Test Loss: 0.0103\n",
            "Epoch: 035, Train Acc: 0.7800, Test Acc: 0.7632, Train Loss: 0.0073, Test Loss: 0.0102\n",
            "Epoch: 036, Train Acc: 0.7867, Test Acc: 0.7105, Train Loss: 0.0076, Test Loss: 0.0103\n",
            "Epoch: 037, Train Acc: 0.8000, Test Acc: 0.7632, Train Loss: 0.0086, Test Loss: 0.0109\n",
            "Epoch: 038, Train Acc: 0.8000, Test Acc: 0.7632, Train Loss: 0.0089, Test Loss: 0.0112\n",
            "Epoch: 039, Train Acc: 0.7867, Test Acc: 0.7632, Train Loss: 0.0077, Test Loss: 0.0102\n",
            "Epoch: 040, Train Acc: 0.7933, Test Acc: 0.7632, Train Loss: 0.0084, Test Loss: 0.0110\n",
            "Epoch: 041, Train Acc: 0.7867, Test Acc: 0.7632, Train Loss: 0.0092, Test Loss: 0.0121\n",
            "Epoch: 042, Train Acc: 0.7933, Test Acc: 0.7632, Train Loss: 0.0076, Test Loss: 0.0100\n",
            "Epoch: 043, Train Acc: 0.7867, Test Acc: 0.7895, Train Loss: 0.0081, Test Loss: 0.0099\n",
            "Epoch: 044, Train Acc: 0.8200, Test Acc: 0.7632, Train Loss: 0.0083, Test Loss: 0.0103\n",
            "Epoch: 045, Train Acc: 0.8200, Test Acc: 0.7895, Train Loss: 0.0072, Test Loss: 0.0100\n",
            "Epoch: 046, Train Acc: 0.8000, Test Acc: 0.7632, Train Loss: 0.0080, Test Loss: 0.0101\n",
            "Epoch: 047, Train Acc: 0.7733, Test Acc: 0.7632, Train Loss: 0.0073, Test Loss: 0.0102\n",
            "Epoch: 048, Train Acc: 0.8067, Test Acc: 0.7632, Train Loss: 0.0079, Test Loss: 0.0110\n",
            "Epoch: 049, Train Acc: 0.7800, Test Acc: 0.7895, Train Loss: 0.0070, Test Loss: 0.0103\n",
            "Epoch: 050, Train Acc: 0.8133, Test Acc: 0.7632, Train Loss: 0.0075, Test Loss: 0.0104\n",
            "Epoch: 051, Train Acc: 0.7867, Test Acc: 0.7105, Train Loss: 0.0079, Test Loss: 0.0107\n",
            "Epoch: 052, Train Acc: 0.7933, Test Acc: 0.7895, Train Loss: 0.0075, Test Loss: 0.0105\n",
            "Epoch: 053, Train Acc: 0.7867, Test Acc: 0.7632, Train Loss: 0.0074, Test Loss: 0.0105\n",
            "Epoch: 054, Train Acc: 0.7800, Test Acc: 0.7368, Train Loss: 0.0075, Test Loss: 0.0105\n",
            "Epoch: 055, Train Acc: 0.8133, Test Acc: 0.7895, Train Loss: 0.0075, Test Loss: 0.0108\n",
            "Epoch: 056, Train Acc: 0.7933, Test Acc: 0.7632, Train Loss: 0.0069, Test Loss: 0.0105\n",
            "Epoch: 057, Train Acc: 0.8000, Test Acc: 0.7632, Train Loss: 0.0076, Test Loss: 0.0107\n",
            "Epoch: 058, Train Acc: 0.8133, Test Acc: 0.7632, Train Loss: 0.0075, Test Loss: 0.0115\n",
            "Epoch: 059, Train Acc: 0.7933, Test Acc: 0.7632, Train Loss: 0.0076, Test Loss: 0.0103\n",
            "Epoch: 060, Train Acc: 0.7933, Test Acc: 0.7632, Train Loss: 0.0076, Test Loss: 0.0104\n",
            "Epoch: 061, Train Acc: 0.8133, Test Acc: 0.7895, Train Loss: 0.0068, Test Loss: 0.0106\n",
            "Epoch: 062, Train Acc: 0.7800, Test Acc: 0.7368, Train Loss: 0.0061, Test Loss: 0.0103\n",
            "Epoch: 063, Train Acc: 0.7933, Test Acc: 0.7632, Train Loss: 0.0081, Test Loss: 0.0104\n",
            "Epoch: 064, Train Acc: 0.8133, Test Acc: 0.7632, Train Loss: 0.0079, Test Loss: 0.0110\n",
            "Epoch: 065, Train Acc: 0.7933, Test Acc: 0.7632, Train Loss: 0.0074, Test Loss: 0.0104\n",
            "Epoch: 066, Train Acc: 0.8000, Test Acc: 0.7632, Train Loss: 0.0071, Test Loss: 0.0105\n",
            "Epoch: 067, Train Acc: 0.7800, Test Acc: 0.7368, Train Loss: 0.0077, Test Loss: 0.0104\n",
            "Epoch: 068, Train Acc: 0.7800, Test Acc: 0.7368, Train Loss: 0.0076, Test Loss: 0.0104\n",
            "Epoch: 069, Train Acc: 0.8133, Test Acc: 0.7632, Train Loss: 0.0067, Test Loss: 0.0107\n",
            "Epoch: 070, Train Acc: 0.8000, Test Acc: 0.7368, Train Loss: 0.0068, Test Loss: 0.0105\n",
            "Epoch: 071, Train Acc: 0.7933, Test Acc: 0.7632, Train Loss: 0.0078, Test Loss: 0.0108\n",
            "Epoch: 072, Train Acc: 0.8133, Test Acc: 0.7632, Train Loss: 0.0072, Test Loss: 0.0113\n",
            "Epoch: 073, Train Acc: 0.7867, Test Acc: 0.7632, Train Loss: 0.0075, Test Loss: 0.0102\n",
            "Epoch: 074, Train Acc: 0.8067, Test Acc: 0.7368, Train Loss: 0.0066, Test Loss: 0.0101\n",
            "Epoch: 075, Train Acc: 0.8133, Test Acc: 0.7632, Train Loss: 0.0067, Test Loss: 0.0104\n",
            "Epoch: 076, Train Acc: 0.7867, Test Acc: 0.7105, Train Loss: 0.0060, Test Loss: 0.0102\n",
            "Epoch: 077, Train Acc: 0.8133, Test Acc: 0.7632, Train Loss: 0.0085, Test Loss: 0.0115\n",
            "Epoch: 078, Train Acc: 0.8067, Test Acc: 0.7632, Train Loss: 0.0072, Test Loss: 0.0099\n",
            "Epoch: 079, Train Acc: 0.8067, Test Acc: 0.7632, Train Loss: 0.0077, Test Loss: 0.0101\n",
            "Epoch: 080, Train Acc: 0.8133, Test Acc: 0.7632, Train Loss: 0.0067, Test Loss: 0.0107\n",
            "Epoch: 081, Train Acc: 0.8133, Test Acc: 0.7632, Train Loss: 0.0070, Test Loss: 0.0102\n",
            "Epoch: 082, Train Acc: 0.8133, Test Acc: 0.7632, Train Loss: 0.0073, Test Loss: 0.0101\n",
            "Epoch: 083, Train Acc: 0.8200, Test Acc: 0.7632, Train Loss: 0.0065, Test Loss: 0.0103\n",
            "Epoch: 084, Train Acc: 0.8000, Test Acc: 0.7632, Train Loss: 0.0063, Test Loss: 0.0096\n",
            "Epoch: 085, Train Acc: 0.8067, Test Acc: 0.7895, Train Loss: 0.0079, Test Loss: 0.0098\n",
            "Epoch: 086, Train Acc: 0.8200, Test Acc: 0.7632, Train Loss: 0.0077, Test Loss: 0.0101\n",
            "Epoch: 087, Train Acc: 0.8200, Test Acc: 0.7632, Train Loss: 0.0070, Test Loss: 0.0094\n",
            "Epoch: 088, Train Acc: 0.8267, Test Acc: 0.7895, Train Loss: 0.0068, Test Loss: 0.0090\n",
            "Epoch: 089, Train Acc: 0.8400, Test Acc: 0.7632, Train Loss: 0.0064, Test Loss: 0.0090\n",
            "Epoch: 090, Train Acc: 0.8400, Test Acc: 0.7895, Train Loss: 0.0058, Test Loss: 0.0100\n",
            "Epoch: 091, Train Acc: 0.8000, Test Acc: 0.8158, Train Loss: 0.0092, Test Loss: 0.0126\n",
            "Epoch: 092, Train Acc: 0.8267, Test Acc: 0.7895, Train Loss: 0.0066, Test Loss: 0.0094\n",
            "Epoch: 093, Train Acc: 0.8067, Test Acc: 0.7895, Train Loss: 0.0073, Test Loss: 0.0099\n",
            "Epoch: 094, Train Acc: 0.8200, Test Acc: 0.7368, Train Loss: 0.0067, Test Loss: 0.0094\n",
            "Epoch: 095, Train Acc: 0.8333, Test Acc: 0.7895, Train Loss: 0.0075, Test Loss: 0.0097\n",
            "Epoch: 096, Train Acc: 0.7933, Test Acc: 0.7632, Train Loss: 0.0068, Test Loss: 0.0093\n",
            "Epoch: 097, Train Acc: 0.7867, Test Acc: 0.7632, Train Loss: 0.0069, Test Loss: 0.0092\n",
            "Epoch: 098, Train Acc: 0.8200, Test Acc: 0.7632, Train Loss: 0.0066, Test Loss: 0.0094\n",
            "Epoch: 099, Train Acc: 0.8200, Test Acc: 0.7368, Train Loss: 0.0069, Test Loss: 0.0093\n",
            "Epoch: 100, Train Acc: 0.8067, Test Acc: 0.7368, Train Loss: 0.0062, Test Loss: 0.0091\n",
            "Epoch: 101, Train Acc: 0.8267, Test Acc: 0.7368, Train Loss: 0.0065, Test Loss: 0.0091\n",
            "Epoch: 102, Train Acc: 0.8133, Test Acc: 0.7368, Train Loss: 0.0066, Test Loss: 0.0089\n",
            "Epoch: 103, Train Acc: 0.8467, Test Acc: 0.7895, Train Loss: 0.0058, Test Loss: 0.0091\n",
            "Epoch: 104, Train Acc: 0.8400, Test Acc: 0.7895, Train Loss: 0.0065, Test Loss: 0.0089\n",
            "Epoch: 105, Train Acc: 0.8067, Test Acc: 0.7632, Train Loss: 0.0060, Test Loss: 0.0086\n",
            "Epoch: 106, Train Acc: 0.8200, Test Acc: 0.7632, Train Loss: 0.0061, Test Loss: 0.0085\n",
            "Epoch: 107, Train Acc: 0.8333, Test Acc: 0.7895, Train Loss: 0.0053, Test Loss: 0.0083\n",
            "Epoch: 108, Train Acc: 0.8333, Test Acc: 0.7895, Train Loss: 0.0060, Test Loss: 0.0083\n",
            "Epoch: 109, Train Acc: 0.8133, Test Acc: 0.8158, Train Loss: 0.0060, Test Loss: 0.0081\n",
            "Epoch: 110, Train Acc: 0.8533, Test Acc: 0.8158, Train Loss: 0.0061, Test Loss: 0.0081\n",
            "Epoch: 111, Train Acc: 0.8533, Test Acc: 0.7895, Train Loss: 0.0066, Test Loss: 0.0084\n",
            "Epoch: 112, Train Acc: 0.8267, Test Acc: 0.7632, Train Loss: 0.0057, Test Loss: 0.0080\n",
            "Epoch: 113, Train Acc: 0.8133, Test Acc: 0.7632, Train Loss: 0.0060, Test Loss: 0.0079\n",
            "Epoch: 114, Train Acc: 0.8533, Test Acc: 0.7895, Train Loss: 0.0052, Test Loss: 0.0085\n",
            "Epoch: 115, Train Acc: 0.8800, Test Acc: 0.8158, Train Loss: 0.0049, Test Loss: 0.0076\n",
            "Epoch: 116, Train Acc: 0.8800, Test Acc: 0.8158, Train Loss: 0.0050, Test Loss: 0.0075\n",
            "Epoch: 117, Train Acc: 0.8800, Test Acc: 0.8158, Train Loss: 0.0055, Test Loss: 0.0075\n",
            "Epoch: 118, Train Acc: 0.8733, Test Acc: 0.7895, Train Loss: 0.0049, Test Loss: 0.0079\n",
            "Epoch: 119, Train Acc: 0.8733, Test Acc: 0.7895, Train Loss: 0.0051, Test Loss: 0.0075\n",
            "Epoch: 120, Train Acc: 0.8933, Test Acc: 0.8421, Train Loss: 0.0052, Test Loss: 0.0076\n",
            "Epoch: 121, Train Acc: 0.8933, Test Acc: 0.8421, Train Loss: 0.0047, Test Loss: 0.0075\n",
            "Epoch: 122, Train Acc: 0.8800, Test Acc: 0.8158, Train Loss: 0.0048, Test Loss: 0.0073\n",
            "Epoch: 123, Train Acc: 0.8800, Test Acc: 0.8158, Train Loss: 0.0045, Test Loss: 0.0072\n",
            "Epoch: 124, Train Acc: 0.8800, Test Acc: 0.8421, Train Loss: 0.0050, Test Loss: 0.0075\n",
            "Epoch: 125, Train Acc: 0.8867, Test Acc: 0.8158, Train Loss: 0.0049, Test Loss: 0.0078\n",
            "Epoch: 126, Train Acc: 0.8800, Test Acc: 0.7895, Train Loss: 0.0052, Test Loss: 0.0076\n",
            "Epoch: 127, Train Acc: 0.8867, Test Acc: 0.8158, Train Loss: 0.0051, Test Loss: 0.0075\n",
            "Epoch: 128, Train Acc: 0.8800, Test Acc: 0.8158, Train Loss: 0.0051, Test Loss: 0.0074\n",
            "Epoch: 129, Train Acc: 0.8800, Test Acc: 0.8158, Train Loss: 0.0046, Test Loss: 0.0074\n",
            "Epoch: 130, Train Acc: 0.8867, Test Acc: 0.8158, Train Loss: 0.0042, Test Loss: 0.0075\n",
            "Epoch: 131, Train Acc: 0.8867, Test Acc: 0.8158, Train Loss: 0.0044, Test Loss: 0.0076\n",
            "Epoch: 132, Train Acc: 0.8867, Test Acc: 0.8158, Train Loss: 0.0043, Test Loss: 0.0077\n",
            "Epoch: 133, Train Acc: 0.8800, Test Acc: 0.8158, Train Loss: 0.0043, Test Loss: 0.0076\n",
            "Epoch: 134, Train Acc: 0.8867, Test Acc: 0.8158, Train Loss: 0.0049, Test Loss: 0.0075\n",
            "Epoch: 135, Train Acc: 0.8867, Test Acc: 0.8158, Train Loss: 0.0051, Test Loss: 0.0073\n",
            "Epoch: 136, Train Acc: 0.8733, Test Acc: 0.8158, Train Loss: 0.0054, Test Loss: 0.0071\n",
            "Epoch: 137, Train Acc: 0.8800, Test Acc: 0.8158, Train Loss: 0.0049, Test Loss: 0.0074\n",
            "Epoch: 138, Train Acc: 0.8867, Test Acc: 0.8158, Train Loss: 0.0046, Test Loss: 0.0074\n",
            "Epoch: 139, Train Acc: 0.8867, Test Acc: 0.8158, Train Loss: 0.0054, Test Loss: 0.0072\n",
            "Epoch: 140, Train Acc: 0.8867, Test Acc: 0.8158, Train Loss: 0.0048, Test Loss: 0.0072\n",
            "Epoch: 141, Train Acc: 0.8800, Test Acc: 0.8158, Train Loss: 0.0045, Test Loss: 0.0070\n",
            "Epoch: 142, Train Acc: 0.8800, Test Acc: 0.8158, Train Loss: 0.0047, Test Loss: 0.0072\n",
            "Epoch: 143, Train Acc: 0.8867, Test Acc: 0.7895, Train Loss: 0.0054, Test Loss: 0.0075\n",
            "Epoch: 144, Train Acc: 0.8867, Test Acc: 0.8158, Train Loss: 0.0052, Test Loss: 0.0073\n",
            "Epoch: 145, Train Acc: 0.8867, Test Acc: 0.8158, Train Loss: 0.0046, Test Loss: 0.0070\n",
            "Epoch: 146, Train Acc: 0.8733, Test Acc: 0.7895, Train Loss: 0.0051, Test Loss: 0.0085\n",
            "Epoch: 147, Train Acc: 0.8800, Test Acc: 0.8158, Train Loss: 0.0050, Test Loss: 0.0068\n",
            "Epoch: 148, Train Acc: 0.8867, Test Acc: 0.8158, Train Loss: 0.0043, Test Loss: 0.0067\n",
            "Epoch: 149, Train Acc: 0.8467, Test Acc: 0.8158, Train Loss: 0.0047, Test Loss: 0.0074\n",
            "Epoch: 150, Train Acc: 0.8867, Test Acc: 0.8158, Train Loss: 0.0048, Test Loss: 0.0071\n",
            "Epoch: 151, Train Acc: 0.8800, Test Acc: 0.8158, Train Loss: 0.0048, Test Loss: 0.0067\n",
            "Epoch: 152, Train Acc: 0.8867, Test Acc: 0.8421, Train Loss: 0.0055, Test Loss: 0.0067\n",
            "Epoch: 153, Train Acc: 0.8867, Test Acc: 0.8158, Train Loss: 0.0046, Test Loss: 0.0068\n",
            "Epoch: 154, Train Acc: 0.8800, Test Acc: 0.8158, Train Loss: 0.0046, Test Loss: 0.0068\n",
            "Epoch: 155, Train Acc: 0.8800, Test Acc: 0.8158, Train Loss: 0.0050, Test Loss: 0.0067\n",
            "Epoch: 156, Train Acc: 0.8800, Test Acc: 0.8158, Train Loss: 0.0049, Test Loss: 0.0069\n",
            "Epoch: 157, Train Acc: 0.8800, Test Acc: 0.8158, Train Loss: 0.0048, Test Loss: 0.0070\n",
            "Epoch: 158, Train Acc: 0.8800, Test Acc: 0.8158, Train Loss: 0.0047, Test Loss: 0.0069\n",
            "Epoch: 159, Train Acc: 0.8867, Test Acc: 0.8158, Train Loss: 0.0042, Test Loss: 0.0068\n",
            "Epoch: 160, Train Acc: 0.8867, Test Acc: 0.8158, Train Loss: 0.0048, Test Loss: 0.0067\n",
            "Epoch: 161, Train Acc: 0.8933, Test Acc: 0.8421, Train Loss: 0.0051, Test Loss: 0.0066\n",
            "Epoch: 162, Train Acc: 0.8867, Test Acc: 0.8421, Train Loss: 0.0050, Test Loss: 0.0068\n",
            "Epoch: 163, Train Acc: 0.8800, Test Acc: 0.8421, Train Loss: 0.0046, Test Loss: 0.0068\n",
            "Epoch: 164, Train Acc: 0.8800, Test Acc: 0.8421, Train Loss: 0.0044, Test Loss: 0.0067\n",
            "Epoch: 165, Train Acc: 0.8800, Test Acc: 0.8421, Train Loss: 0.0042, Test Loss: 0.0065\n",
            "Epoch: 166, Train Acc: 0.8800, Test Acc: 0.8421, Train Loss: 0.0049, Test Loss: 0.0066\n",
            "Epoch: 167, Train Acc: 0.8867, Test Acc: 0.8421, Train Loss: 0.0045, Test Loss: 0.0068\n",
            "Epoch: 168, Train Acc: 0.8800, Test Acc: 0.8158, Train Loss: 0.0046, Test Loss: 0.0070\n",
            "Epoch: 169, Train Acc: 0.8867, Test Acc: 0.8421, Train Loss: 0.0044, Test Loss: 0.0069\n",
            "Epoch: 170, Train Acc: 0.8867, Test Acc: 0.8158, Train Loss: 0.0044, Test Loss: 0.0067\n",
            "Epoch: 171, Train Acc: 0.8800, Test Acc: 0.8158, Train Loss: 0.0050, Test Loss: 0.0072\n",
            "Epoch: 172, Train Acc: 0.8800, Test Acc: 0.8158, Train Loss: 0.0044, Test Loss: 0.0074\n",
            "Epoch: 173, Train Acc: 0.8800, Test Acc: 0.8421, Train Loss: 0.0052, Test Loss: 0.0074\n",
            "Epoch: 174, Train Acc: 0.8667, Test Acc: 0.7895, Train Loss: 0.0045, Test Loss: 0.0074\n",
            "Epoch: 175, Train Acc: 0.8800, Test Acc: 0.8158, Train Loss: 0.0047, Test Loss: 0.0068\n",
            "Epoch: 176, Train Acc: 0.8867, Test Acc: 0.8421, Train Loss: 0.0047, Test Loss: 0.0069\n",
            "Epoch: 177, Train Acc: 0.8133, Test Acc: 0.8158, Train Loss: 0.0056, Test Loss: 0.0082\n",
            "Epoch: 178, Train Acc: 0.8867, Test Acc: 0.8158, Train Loss: 0.0053, Test Loss: 0.0070\n",
            "Epoch: 179, Train Acc: 0.8400, Test Acc: 0.8421, Train Loss: 0.0059, Test Loss: 0.0078\n",
            "Epoch: 180, Train Acc: 0.8800, Test Acc: 0.8158, Train Loss: 0.0050, Test Loss: 0.0067\n",
            "Epoch: 181, Train Acc: 0.8733, Test Acc: 0.8158, Train Loss: 0.0053, Test Loss: 0.0067\n",
            "Epoch: 182, Train Acc: 0.8800, Test Acc: 0.8158, Train Loss: 0.0046, Test Loss: 0.0066\n",
            "Epoch: 183, Train Acc: 0.8667, Test Acc: 0.8158, Train Loss: 0.0050, Test Loss: 0.0068\n",
            "Epoch: 184, Train Acc: 0.8800, Test Acc: 0.8158, Train Loss: 0.0050, Test Loss: 0.0066\n",
            "Epoch: 185, Train Acc: 0.8800, Test Acc: 0.8421, Train Loss: 0.0047, Test Loss: 0.0068\n",
            "Epoch: 186, Train Acc: 0.8867, Test Acc: 0.8421, Train Loss: 0.0045, Test Loss: 0.0068\n",
            "Epoch: 187, Train Acc: 0.8667, Test Acc: 0.8158, Train Loss: 0.0046, Test Loss: 0.0072\n",
            "Epoch: 188, Train Acc: 0.8800, Test Acc: 0.8421, Train Loss: 0.0045, Test Loss: 0.0070\n",
            "Epoch: 189, Train Acc: 0.8867, Test Acc: 0.8421, Train Loss: 0.0045, Test Loss: 0.0065\n",
            "Epoch: 190, Train Acc: 0.8933, Test Acc: 0.8421, Train Loss: 0.0054, Test Loss: 0.0064\n",
            "Epoch: 191, Train Acc: 0.8800, Test Acc: 0.8158, Train Loss: 0.0055, Test Loss: 0.0064\n",
            "Epoch: 192, Train Acc: 0.8867, Test Acc: 0.8421, Train Loss: 0.0053, Test Loss: 0.0064\n",
            "Epoch: 193, Train Acc: 0.8867, Test Acc: 0.8421, Train Loss: 0.0045, Test Loss: 0.0065\n",
            "Epoch: 194, Train Acc: 0.9000, Test Acc: 0.8421, Train Loss: 0.0047, Test Loss: 0.0066\n",
            "Epoch: 195, Train Acc: 0.8867, Test Acc: 0.8158, Train Loss: 0.0048, Test Loss: 0.0068\n",
            "Epoch: 196, Train Acc: 0.8667, Test Acc: 0.7632, Train Loss: 0.0050, Test Loss: 0.0076\n",
            "Epoch: 197, Train Acc: 0.8600, Test Acc: 0.7632, Train Loss: 0.0047, Test Loss: 0.0075\n",
            "Epoch: 198, Train Acc: 0.8733, Test Acc: 0.8158, Train Loss: 0.0041, Test Loss: 0.0076\n",
            "Epoch: 199, Train Acc: 0.8533, Test Acc: 0.8158, Train Loss: 0.0053, Test Loss: 0.0074\n",
            "Epoch: 200, Train Acc: 0.8733, Test Acc: 0.8421, Train Loss: 0.0051, Test Loss: 0.0070\n",
            "Epoch: 201, Train Acc: 0.8800, Test Acc: 0.8421, Train Loss: 0.0052, Test Loss: 0.0068\n",
            "Epoch: 202, Train Acc: 0.8800, Test Acc: 0.8421, Train Loss: 0.0047, Test Loss: 0.0070\n",
            "Epoch: 203, Train Acc: 0.8733, Test Acc: 0.8421, Train Loss: 0.0058, Test Loss: 0.0069\n",
            "Epoch: 204, Train Acc: 0.8733, Test Acc: 0.8421, Train Loss: 0.0047, Test Loss: 0.0070\n",
            "Epoch: 205, Train Acc: 0.8867, Test Acc: 0.8421, Train Loss: 0.0051, Test Loss: 0.0071\n",
            "Epoch: 206, Train Acc: 0.8733, Test Acc: 0.8421, Train Loss: 0.0052, Test Loss: 0.0075\n",
            "Epoch: 207, Train Acc: 0.8667, Test Acc: 0.7895, Train Loss: 0.0045, Test Loss: 0.0075\n",
            "Epoch: 208, Train Acc: 0.8800, Test Acc: 0.8421, Train Loss: 0.0051, Test Loss: 0.0073\n",
            "Epoch: 209, Train Acc: 0.8800, Test Acc: 0.8158, Train Loss: 0.0044, Test Loss: 0.0074\n",
            "Epoch: 210, Train Acc: 0.8800, Test Acc: 0.7895, Train Loss: 0.0049, Test Loss: 0.0078\n",
            "Epoch: 211, Train Acc: 0.8667, Test Acc: 0.7632, Train Loss: 0.0048, Test Loss: 0.0080\n",
            "Epoch: 212, Train Acc: 0.8867, Test Acc: 0.8421, Train Loss: 0.0049, Test Loss: 0.0075\n",
            "Epoch: 213, Train Acc: 0.8800, Test Acc: 0.8158, Train Loss: 0.0043, Test Loss: 0.0068\n",
            "Epoch: 214, Train Acc: 0.8867, Test Acc: 0.8158, Train Loss: 0.0043, Test Loss: 0.0066\n",
            "Epoch: 215, Train Acc: 0.8800, Test Acc: 0.8421, Train Loss: 0.0050, Test Loss: 0.0068\n",
            "Epoch: 216, Train Acc: 0.8800, Test Acc: 0.8158, Train Loss: 0.0042, Test Loss: 0.0065\n",
            "Epoch: 217, Train Acc: 0.8800, Test Acc: 0.8158, Train Loss: 0.0051, Test Loss: 0.0064\n",
            "Epoch: 218, Train Acc: 0.8667, Test Acc: 0.8684, Train Loss: 0.0053, Test Loss: 0.0063\n",
            "Epoch: 219, Train Acc: 0.8800, Test Acc: 0.8158, Train Loss: 0.0046, Test Loss: 0.0065\n",
            "Epoch: 220, Train Acc: 0.8667, Test Acc: 0.8684, Train Loss: 0.0051, Test Loss: 0.0067\n",
            "Epoch: 221, Train Acc: 0.8667, Test Acc: 0.8684, Train Loss: 0.0045, Test Loss: 0.0068\n",
            "Epoch: 222, Train Acc: 0.8667, Test Acc: 0.8684, Train Loss: 0.0046, Test Loss: 0.0069\n",
            "Epoch: 223, Train Acc: 0.8667, Test Acc: 0.8684, Train Loss: 0.0045, Test Loss: 0.0070\n",
            "Epoch: 224, Train Acc: 0.8533, Test Acc: 0.8158, Train Loss: 0.0053, Test Loss: 0.0076\n",
            "Epoch: 225, Train Acc: 0.8667, Test Acc: 0.7632, Train Loss: 0.0046, Test Loss: 0.0074\n",
            "Epoch: 226, Train Acc: 0.8867, Test Acc: 0.8421, Train Loss: 0.0053, Test Loss: 0.0068\n",
            "Epoch: 227, Train Acc: 0.8867, Test Acc: 0.8158, Train Loss: 0.0048, Test Loss: 0.0065\n",
            "Epoch: 228, Train Acc: 0.8800, Test Acc: 0.8158, Train Loss: 0.0044, Test Loss: 0.0065\n",
            "Epoch: 229, Train Acc: 0.8800, Test Acc: 0.8158, Train Loss: 0.0047, Test Loss: 0.0068\n",
            "Epoch: 230, Train Acc: 0.8600, Test Acc: 0.8421, Train Loss: 0.0045, Test Loss: 0.0077\n",
            "Epoch: 231, Train Acc: 0.8800, Test Acc: 0.8158, Train Loss: 0.0047, Test Loss: 0.0069\n",
            "Epoch: 232, Train Acc: 0.8800, Test Acc: 0.8158, Train Loss: 0.0046, Test Loss: 0.0068\n",
            "Epoch: 233, Train Acc: 0.8800, Test Acc: 0.8158, Train Loss: 0.0049, Test Loss: 0.0067\n",
            "Epoch: 234, Train Acc: 0.8800, Test Acc: 0.8158, Train Loss: 0.0047, Test Loss: 0.0069\n",
            "Epoch: 235, Train Acc: 0.8800, Test Acc: 0.8158, Train Loss: 0.0045, Test Loss: 0.0068\n",
            "Epoch: 236, Train Acc: 0.8667, Test Acc: 0.8421, Train Loss: 0.0049, Test Loss: 0.0066\n",
            "Epoch: 237, Train Acc: 0.8800, Test Acc: 0.8158, Train Loss: 0.0050, Test Loss: 0.0069\n",
            "Epoch: 238, Train Acc: 0.8800, Test Acc: 0.8158, Train Loss: 0.0051, Test Loss: 0.0069\n",
            "Epoch: 239, Train Acc: 0.8800, Test Acc: 0.8158, Train Loss: 0.0051, Test Loss: 0.0066\n",
            "Epoch: 240, Train Acc: 0.8667, Test Acc: 0.8158, Train Loss: 0.0048, Test Loss: 0.0064\n",
            "Epoch: 241, Train Acc: 0.8667, Test Acc: 0.8421, Train Loss: 0.0042, Test Loss: 0.0063\n",
            "Epoch: 242, Train Acc: 0.8733, Test Acc: 0.8421, Train Loss: 0.0065, Test Loss: 0.0068\n",
            "Epoch: 243, Train Acc: 0.8800, Test Acc: 0.8421, Train Loss: 0.0048, Test Loss: 0.0067\n",
            "Epoch: 244, Train Acc: 0.8600, Test Acc: 0.8158, Train Loss: 0.0046, Test Loss: 0.0066\n",
            "Epoch: 245, Train Acc: 0.8800, Test Acc: 0.8158, Train Loss: 0.0049, Test Loss: 0.0066\n",
            "Epoch: 246, Train Acc: 0.8733, Test Acc: 0.8421, Train Loss: 0.0047, Test Loss: 0.0070\n",
            "Epoch: 247, Train Acc: 0.8867, Test Acc: 0.8421, Train Loss: 0.0045, Test Loss: 0.0069\n",
            "Epoch: 248, Train Acc: 0.8867, Test Acc: 0.8158, Train Loss: 0.0043, Test Loss: 0.0066\n",
            "Epoch: 249, Train Acc: 0.8800, Test Acc: 0.8158, Train Loss: 0.0050, Test Loss: 0.0068\n",
            "Epoch: 250, Train Acc: 0.8533, Test Acc: 0.7895, Train Loss: 0.0057, Test Loss: 0.0074\n",
            "Epoch: 251, Train Acc: 0.8933, Test Acc: 0.8421, Train Loss: 0.0046, Test Loss: 0.0070\n",
            "Epoch: 252, Train Acc: 0.8867, Test Acc: 0.8158, Train Loss: 0.0047, Test Loss: 0.0070\n",
            "Epoch: 253, Train Acc: 0.8867, Test Acc: 0.8158, Train Loss: 0.0049, Test Loss: 0.0069\n",
            "Epoch: 254, Train Acc: 0.8800, Test Acc: 0.8421, Train Loss: 0.0044, Test Loss: 0.0070\n",
            "Epoch: 255, Train Acc: 0.8733, Test Acc: 0.8158, Train Loss: 0.0047, Test Loss: 0.0073\n",
            "Epoch: 256, Train Acc: 0.8800, Test Acc: 0.8158, Train Loss: 0.0050, Test Loss: 0.0074\n",
            "Epoch: 257, Train Acc: 0.8667, Test Acc: 0.7632, Train Loss: 0.0042, Test Loss: 0.0080\n",
            "Epoch: 258, Train Acc: 0.8600, Test Acc: 0.7632, Train Loss: 0.0045, Test Loss: 0.0080\n",
            "Epoch: 259, Train Acc: 0.8867, Test Acc: 0.8158, Train Loss: 0.0049, Test Loss: 0.0073\n",
            "Epoch: 260, Train Acc: 0.8800, Test Acc: 0.7895, Train Loss: 0.0053, Test Loss: 0.0071\n",
            "Epoch: 261, Train Acc: 0.8800, Test Acc: 0.8158, Train Loss: 0.0047, Test Loss: 0.0068\n",
            "Epoch: 262, Train Acc: 0.8600, Test Acc: 0.8158, Train Loss: 0.0052, Test Loss: 0.0072\n",
            "Epoch: 263, Train Acc: 0.8867, Test Acc: 0.8421, Train Loss: 0.0051, Test Loss: 0.0064\n",
            "Epoch: 264, Train Acc: 0.8533, Test Acc: 0.8421, Train Loss: 0.0055, Test Loss: 0.0075\n",
            "Epoch: 265, Train Acc: 0.8800, Test Acc: 0.8158, Train Loss: 0.0048, Test Loss: 0.0071\n",
            "Epoch: 266, Train Acc: 0.8667, Test Acc: 0.8158, Train Loss: 0.0051, Test Loss: 0.0078\n",
            "Epoch: 267, Train Acc: 0.8533, Test Acc: 0.8158, Train Loss: 0.0049, Test Loss: 0.0077\n",
            "Epoch: 268, Train Acc: 0.8667, Test Acc: 0.8421, Train Loss: 0.0044, Test Loss: 0.0072\n",
            "Epoch: 269, Train Acc: 0.8800, Test Acc: 0.8158, Train Loss: 0.0050, Test Loss: 0.0065\n",
            "Epoch: 270, Train Acc: 0.8800, Test Acc: 0.8158, Train Loss: 0.0045, Test Loss: 0.0066\n",
            "Epoch: 271, Train Acc: 0.8800, Test Acc: 0.8158, Train Loss: 0.0049, Test Loss: 0.0065\n",
            "Epoch: 272, Train Acc: 0.8800, Test Acc: 0.8158, Train Loss: 0.0055, Test Loss: 0.0066\n",
            "Epoch: 273, Train Acc: 0.8800, Test Acc: 0.8158, Train Loss: 0.0054, Test Loss: 0.0067\n",
            "Epoch: 274, Train Acc: 0.8667, Test Acc: 0.8421, Train Loss: 0.0051, Test Loss: 0.0063\n",
            "Epoch: 275, Train Acc: 0.8800, Test Acc: 0.8158, Train Loss: 0.0043, Test Loss: 0.0066\n",
            "Epoch: 276, Train Acc: 0.8800, Test Acc: 0.8158, Train Loss: 0.0047, Test Loss: 0.0067\n",
            "Epoch: 277, Train Acc: 0.8800, Test Acc: 0.8158, Train Loss: 0.0046, Test Loss: 0.0067\n",
            "Epoch: 278, Train Acc: 0.8667, Test Acc: 0.8684, Train Loss: 0.0040, Test Loss: 0.0066\n",
            "Epoch: 279, Train Acc: 0.8667, Test Acc: 0.8684, Train Loss: 0.0043, Test Loss: 0.0066\n",
            "Epoch: 280, Train Acc: 0.8800, Test Acc: 0.8158, Train Loss: 0.0051, Test Loss: 0.0066\n",
            "Epoch: 281, Train Acc: 0.8800, Test Acc: 0.8158, Train Loss: 0.0049, Test Loss: 0.0067\n",
            "Epoch: 282, Train Acc: 0.8800, Test Acc: 0.8158, Train Loss: 0.0040, Test Loss: 0.0070\n",
            "Epoch: 283, Train Acc: 0.8800, Test Acc: 0.8158, Train Loss: 0.0042, Test Loss: 0.0067\n",
            "Epoch: 284, Train Acc: 0.8800, Test Acc: 0.8158, Train Loss: 0.0056, Test Loss: 0.0066\n",
            "Epoch: 285, Train Acc: 0.8867, Test Acc: 0.8158, Train Loss: 0.0047, Test Loss: 0.0066\n",
            "Epoch: 286, Train Acc: 0.8867, Test Acc: 0.8158, Train Loss: 0.0045, Test Loss: 0.0066\n",
            "Epoch: 287, Train Acc: 0.8800, Test Acc: 0.8421, Train Loss: 0.0046, Test Loss: 0.0066\n",
            "Epoch: 288, Train Acc: 0.8867, Test Acc: 0.8421, Train Loss: 0.0041, Test Loss: 0.0066\n",
            "Epoch: 289, Train Acc: 0.8800, Test Acc: 0.8158, Train Loss: 0.0043, Test Loss: 0.0066\n",
            "Epoch: 290, Train Acc: 0.8800, Test Acc: 0.8158, Train Loss: 0.0043, Test Loss: 0.0071\n",
            "Epoch: 291, Train Acc: 0.8667, Test Acc: 0.7632, Train Loss: 0.0044, Test Loss: 0.0074\n",
            "Epoch: 292, Train Acc: 0.8800, Test Acc: 0.8158, Train Loss: 0.0047, Test Loss: 0.0070\n",
            "Epoch: 293, Train Acc: 0.8800, Test Acc: 0.8158, Train Loss: 0.0048, Test Loss: 0.0073\n",
            "Epoch: 294, Train Acc: 0.8800, Test Acc: 0.8158, Train Loss: 0.0047, Test Loss: 0.0074\n",
            "Epoch: 295, Train Acc: 0.8800, Test Acc: 0.8158, Train Loss: 0.0047, Test Loss: 0.0072\n",
            "Epoch: 296, Train Acc: 0.8800, Test Acc: 0.8158, Train Loss: 0.0046, Test Loss: 0.0066\n",
            "Epoch: 297, Train Acc: 0.8867, Test Acc: 0.8158, Train Loss: 0.0050, Test Loss: 0.0064\n",
            "Epoch: 298, Train Acc: 0.8800, Test Acc: 0.8421, Train Loss: 0.0046, Test Loss: 0.0065\n",
            "Epoch: 299, Train Acc: 0.8400, Test Acc: 0.8684, Train Loss: 0.0047, Test Loss: 0.0065\n",
            "Epoch: 300, Train Acc: 0.8400, Test Acc: 0.8684, Train Loss: 0.0049, Test Loss: 0.0064\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## C. Observe how the accuracy changes by varying the type of graph convolution (GCNConv, GraphConv, GAT, e.g.)."
      ],
      "metadata": {
        "id": "Gf9GAVLDMtq2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define GCNConv model"
      ],
      "metadata": {
        "id": "P7X251gkaKsK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.nn import GCNConv\n",
        "\n",
        "class GCNConvModel(torch.nn.Module):\n",
        "    def __init__(self, num_node_features, num_classes, hidden_channels):\n",
        "        super(GCNConvModel, self).__init__()\n",
        "        torch.manual_seed(10)\n",
        "        self.conv1 = GCNConv(dataset.num_node_features, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.lin = Linear(hidden_channels, dataset.num_classes)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv3(x, edge_index)\n",
        "\n",
        "        x = global_mean_pool(x, batch)\n",
        "\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.lin(x)\n",
        "\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "model = GCNConvModel(num_node_features=7, num_classes=2, hidden_channels=64)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R59V8pmSAvUM",
        "outputId": "be0a7fbe-e07f-45aa-a157-c34ac24314db"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GCNConvModel(\n",
            "  (conv1): GCNConv(7, 64)\n",
            "  (conv2): GCNConv(64, 64)\n",
            "  (conv3): GCNConv(64, 64)\n",
            "  (lin): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define GATConv model"
      ],
      "metadata": {
        "id": "9zpUYRUeabEB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.nn import GATConv\n",
        "\n",
        "class GATConvModel(torch.nn.Module):\n",
        "    def __init__(self, num_node_features, num_classes, hidden_channels):\n",
        "        super(GATConvModel, self).__init__()\n",
        "        torch.manual_seed(10)\n",
        "        self.conv1 = GATConv(dataset.num_node_features, hidden_channels)\n",
        "        self.conv2 = GATConv(hidden_channels, hidden_channels)\n",
        "        self.conv3 = GATConv(hidden_channels, hidden_channels)\n",
        "        self.lin = Linear(hidden_channels, dataset.num_classes)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv3(x, edge_index)\n",
        "\n",
        "        x = global_mean_pool(x, batch)\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.lin(x)\n",
        "\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "model = GATConvModel(num_node_features=7, num_classes=2, hidden_channels=64)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e816cuP__3nH",
        "outputId": "33e0db81-6bea-4c0d-f90d-dffbdcb75fb6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GATConvModel(\n",
            "  (conv1): GATConv(7, 64, heads=1)\n",
            "  (conv2): GATConv(64, 64, heads=1)\n",
            "  (conv3): GATConv(64, 64, heads=1)\n",
            "  (lin): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define GraphConv model"
      ],
      "metadata": {
        "id": "w9riwD54e39Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.nn import GraphConv\n",
        "\n",
        "class GraphConvModel(torch.nn.Module):\n",
        "    def __init__(self, num_node_features, num_classes, hidden_channels):\n",
        "        super(GraphConvModel, self).__init__()\n",
        "        torch.manual_seed(10)\n",
        "        self.conv1 = GraphConv(dataset.num_node_features, hidden_channels)\n",
        "        self.conv2 = GraphConv(hidden_channels, hidden_channels)\n",
        "        self.conv3 = GraphConv(hidden_channels, hidden_channels)\n",
        "        self.lin = Linear(hidden_channels, dataset.num_classes)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv3(x, edge_index)\n",
        "\n",
        "        x = global_mean_pool(x, batch)\n",
        "\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.lin(x)\n",
        "\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "model = GraphConvModel(num_node_features=7, num_classes=2, hidden_channels=64)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCN9iZVaXL8o",
        "outputId": "c1f1fd0f-6355-4b6a-ec2a-4cba33d391ae"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GraphConvModel(\n",
            "  (conv1): GraphConv(7, 64)\n",
            "  (conv2): GraphConv(64, 64)\n",
            "  (conv3): GraphConv(64, 64)\n",
            "  (lin): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define SAGEConv model"
      ],
      "metadata": {
        "id": "LBotjlIT7WV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.nn import SAGEConv\n",
        "\n",
        "class SAGEConvModel(torch.nn.Module):\n",
        "    def __init__(self, num_node_features, num_classes, hidden_channels):\n",
        "        super(SAGEConvModel, self).__init__()\n",
        "        torch.manual_seed(10)\n",
        "        self.conv1 = SAGEConv(dataset.num_node_features, hidden_channels)\n",
        "        self.conv2 = SAGEConv(hidden_channels, hidden_channels)\n",
        "        self.conv3 = SAGEConv(hidden_channels, hidden_channels)\n",
        "        self.lin = Linear(hidden_channels, dataset.num_classes)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv3(x, edge_index)\n",
        "\n",
        "        x = global_mean_pool(x, batch)\n",
        "\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.lin(x)\n",
        "\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "model = SAGEConvModel(num_node_features=7, num_classes=2, hidden_channels=64)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgJt2C-e-4nu",
        "outputId": "0fd6b423-9d85-44b5-b21a-086c4f1d9f58"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SAGEConvModel(\n",
            "  (conv1): SAGEConv(7, 64, aggr=mean)\n",
            "  (conv2): SAGEConv(64, 64, aggr=mean)\n",
            "  (conv3): SAGEConv(64, 64, aggr=mean)\n",
            "  (lin): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define TransformerConv model"
      ],
      "metadata": {
        "id": "n7SRCGkcAfE9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.nn import TransformerConv\n",
        "\n",
        "class TransformerConvModel(torch.nn.Module):\n",
        "    def __init__(self, num_node_features, num_classes, hidden_channels):\n",
        "        super(TransformerConvModel, self).__init__()\n",
        "        torch.manual_seed(10)\n",
        "        self.conv1 = TransformerConv(dataset.num_node_features, hidden_channels)\n",
        "        self.conv2 = TransformerConv(hidden_channels, hidden_channels)\n",
        "        self.conv3 = TransformerConv(hidden_channels, hidden_channels)\n",
        "        self.lin = Linear(hidden_channels, dataset.num_classes)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv3(x, edge_index)\n",
        "\n",
        "        x = global_mean_pool(x, batch)\n",
        "\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.lin(x)\n",
        "\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "model = TransformerConvModel(num_node_features=7, num_classes=2, hidden_channels=64)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9fObPr9AobA",
        "outputId": "055e2931-5843-4cf9-feba-50aaa532d081"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TransformerConvModel(\n",
            "  (conv1): TransformerConv(7, 64, heads=1)\n",
            "  (conv2): TransformerConv(64, 64, heads=1)\n",
            "  (conv3): TransformerConv(64, 64, heads=1)\n",
            "  (lin): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define train and evaluate model"
      ],
      "metadata": {
        "id": "Fom42mEoBcJN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate(model, train_loader, test_loader):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    # Train the model\n",
        "    model.train()\n",
        "    for epoch in range(500):\n",
        "        for data in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            out = model(data)\n",
        "            loss = criterion(out, data.y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # Evaluate the model\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    loss_ = 0\n",
        "    for data in test_loader:\n",
        "        out = model(data)\n",
        "        pred = out.argmax(dim=1)\n",
        "        loss = criterion(out, data.y)\n",
        "        loss_ += loss.item()\n",
        "        correct += pred.eq(data.y).sum().item()\n",
        "    accuracy = correct / len(test_loader.dataset)\n",
        "\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "xFJSDKs1CwqK"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " hidden_channels=64\n",
        "\n",
        " models = {\n",
        "    'GCN': GCNConvModel(dataset.num_node_features, dataset.num_classes, hidden_channels),\n",
        "    'GAT': GATConvModel(dataset.num_node_features, dataset.num_classes, hidden_channels),\n",
        "    'GraphConv': GraphConvModel(dataset.num_node_features, dataset.num_classes, hidden_channels),\n",
        "    'SAGE': SAGEConvModel(dataset.num_node_features, dataset.num_classes, hidden_channels),\n",
        "    'Transformer': TransformerConvModel(dataset.num_node_features, dataset.num_classes, hidden_channels)\n",
        "}"
      ],
      "metadata": {
        "id": "-D9zZAVNhPBw"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for model_name, model in models.items():\n",
        "    accuracy = train_and_evaluate(model, train_loader, test_loader)\n",
        "    print(f'Accuracy of {model_name}: {accuracy:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYPY3sisBtCS",
        "outputId": "b21e08ee-4df6-45d1-fe30-f718ea240f4f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of GCN: 0.8684\n",
            "Accuracy of GAT: 0.7895\n",
            "Accuracy of GraphConv: 0.9211\n",
            "Accuracy of SAGE: 0.8421\n",
            "Accuracy of Transformer: 0.7368\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## D. Explain the mechanism of dividing graphic data into smaller batches (mini-batch) during the training process."
      ],
      "metadata": {
        "id": "76JTKaDogtKM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recently, the training of GNNs has been widening to very large graphs. The number of nodes and the edges of these graphs reach millions to billions and the graphs with such scales make the ordinary naive software/hardware approaches ineffective. The earlier implementations of GNN were mostly focusing on a small scale graphs and assumed the whole graph fits into a single GPU memory. However, for large graphs whose node/edge feature data cannot fit into the GPU memory, at least part of the graph needs to be placed into the CPU memory. One common practice to train GNNs in such scenario is to create a smaller set of problem by performing a mini-batched training. With the mini-batch training, only a subset of nodes are randomly picked along with their neighboring nodes and sent to GPU. This method is very effective when training GNNs on large graphs as it practically reduces the memory footprint of the application. Overall, the use of mini-batches in the training process helps to manage memory usage, improve computational efficiency, and facilitate the training of graph-based machine learning models on large-scale graph data (1).\n",
        "\n",
        "A mini-batch training process that places the all or part of the input graph feature data in the CPU memory needs to frequently transfer mini-batch data from CPU to GPU through a slow PCIe interconnect. Furthermore, the minibatch method amplifies the total amount of data access because the different minibatches can have overlapping nodes. Due to these reasons, training GNN is often throttled by CPU-GPU data transfer time (1).\n"
      ],
      "metadata": {
        "id": "lm7ShXx230mM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The mechanism of dividing graph data into mini-batches during the training process aims to improve training speed and enhance the efficiency of training\n",
        "in graph-based machine learning models, such as graph neural networks (GNNs). This approach optimizes the training process by using small batches of data instead of the entire dataset in each training step. By breaking down the data into mini-batches, it allows for leveraging parallel processing capabilities and optimizing memory usage, resulting in advantages in terms of speed and efficiency. Additionally, training on mini-batches can reduce the risk of overfitting as the model learns interactively on a small subset of data rather than fitting precisely to the entire dataset and preventing the model from memorizing the entire dataset. In a graph-based model, the input data is split into batches of a specified size using appropriate data structures and algorithms. During each training iteration, the network model is then trained on each mini-batch separately at a time, and after completing the training on each mini-batch, its parameters (weights) are updated based on the gradients computed on that mini-batch. This process iterates for multiple epochs until all data has been trained on or until the model converges or reaches a certain stopping criterion, such as reaching a specified number of training epochs or achieving the desired accuracy level (2)."
      ],
      "metadata": {
        "id": "rj27pN4r-r1_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The mechanism of dividing graph data into mini-batches during the training process faces particular challenges due to the connected and non-uniform structure of graphs. Below, the mechanism of batching graph data is explained in detail (2):\n",
        "\n",
        "1. **Graph Aggregation into a Batch:**\n",
        "   - **Graph Merging:** To create a batch, multiple independent graphs are aggregated into a larger graph. This process involves combining the node and edge lists of individual graphs.\n",
        "   - **Preserving Graph Structure:** During aggregation, the edges of each individual graph are preserved separately to ensure that connections between nodes in different graphs are not established erroneously.\n",
        "\n",
        "2. **Node Indexing:**\n",
        "   - **Graph Indices (Batch Indices):** Graph indices are utilized to maintain information related to each graph within the batch. These indices indicate the source graph of each node in the batch.\n",
        "   - **Adjusting Edge Indices:** Edge indices for each graph need to be adjusted to reflect the new positions of nodes in the batch. This adjustment is achieved by adding an offset to the edge indices of each graph.\n",
        "\n",
        "3. **DataLoader Process in torch_geometric:**\n",
        "   - **Automatic Aggregation:** DataLoader in torch_geometric streamlines the aggregation of graphs into batches automatically. This process entails the fusion of nodes, edges, and other graph features.\n",
        "   - **Load Balancing:** DataLoader strives to balance the computational workload in each batch, considering variations in the number of nodes and edges across different graphs.\n",
        "\n",
        "4. **Batch Processing in GNN:**\n",
        "   - **Independent Node Computations:** In Graph Neural Networks (GNNs), computations for each node, including features and neighbors, are performed independently of other nodes. This enables parallel processing of multiple graphs within a batch.\n",
        "   - **Utilizing Graph Indices:** Graph indices play a crucial role in node processing, ensuring that computations related to a specific graph in the batch are only applied to the nodes of that graph.\n",
        "\n",
        "By implementing this mechanism, torch_geometric effectively categorizes graph data, enhancing computational efficiency while preserving the original structure and features of the graphs throughout the training process (2)."
      ],
      "metadata": {
        "id": "oMDzGbUzMb_x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Current deep learning models for classification tasks in computer vision are trained using minibatches. They take advantage of the relationships between samples in a minibatch, using graph neural networks to aggregate information from similar images. This helps mitigate the adverse effects of alterations to the input images on classification performance. Diverse experiments on image-based object and scene classification show that this approach not only improves a classifier’s performance but also increases its robustness to image perturbations and adversarial attacks. Further, mini-batch graph neural networks can help to alleviate the problem of mode collapse in Generative Adversarial Networks (3).\n",
        "\n",
        "In most of these settings, the training data is divided into mini-batches to adjust to limitations in computational and memory resources. Within a particular mini-batch, the input images may have varying degrees of similarity between them. Exploiting this variability during the feature encoding stage has the potential to improve the performance of downstream computer vision tasks. Motivated by this idea, different approaches have been proposed to take advantage of the relationships between samples in a mini-batch for computer vision tasks, and in particular for image-based classification. These approaches all explicitly encourage the embeddings in the feature space to be close to one another when the underlying images are similar, by using an extra similarity based loss term (3).\n",
        "\n",
        "One of these approach is based on the construction of a graph from each mini-batch of samples, which allows information to be pooled across those with similar features, using graph convolution operations. As such, the requirement that similar images have similar embeddings is implicit, in that no additional loss term has to be optimized. This allows for the aggregation of features during training in a manner that adjusts dynamically to each particular mini-batch ensemble of images. A perturbation analysis explains how this, in turn, affords a degree of robustness to input image alterations and adversarial attacks (3)."
      ],
      "metadata": {
        "id": "F3nR-g6a80Rw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "References:\n",
        "1. Min, Seung Won, et al. \"Graph neural network training and data tiering.\" Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 2022.\n",
        "2. ChatGPT\n",
        "3. Mondal, Arnab Kumar, Vineet Jain, and Kaleem Siddiqi. \"Mini-batch graphs for robust image classification.\" arXiv preprint arXiv:2105.03237 (2021)."
      ],
      "metadata": {
        "id": "KjlWKB2_yv02"
      }
    }
  ]
}